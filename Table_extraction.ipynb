{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "V-HqtTT_kr3E"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiragelb/NCC-Statistical-Reports/blob/main/Table_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install pandoc\n",
        "!pip install pypandoc\n",
        "!pip install python-docx\n",
        "!pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMvSuUElnUQ7",
        "outputId": "15b77bc1-23c3-4eb0-84a5-3641a62b616a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc-data\n",
            "Suggested packages:\n",
            "  texlive-latex-recommended texlive-xetex texlive-luatex pandoc-citeproc\n",
            "  texlive-latex-extra context wkhtmltopdf librsvg2-bin groff ghc nodejs php\n",
            "  python ruby libjs-mathjax libjs-katex citation-style-language-styles\n",
            "The following NEW packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc\n",
            "  pandoc-data\n",
            "0 upgraded, 4 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 20.6 MB of archives.\n",
            "After this operation, 156 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [115 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm-extensions0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [25.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc-data all 2.9.2.1-3ubuntu2 [81.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc amd64 2.9.2.1-3ubuntu2 [20.3 MB]\n",
            "Fetched 20.6 MB in 1s (14.9 MB/s)\n",
            "Selecting previously unselected package libcmark-gfm0.29.0.gfm.3:amd64.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../libcmark-gfm0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcmark-gfm-extensions0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../libcmark-gfm-extensions0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package pandoc-data.\n",
            "Preparing to unpack .../pandoc-data_2.9.2.1-3ubuntu2_all.deb ...\n",
            "Unpacking pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package pandoc.\n",
            "Preparing to unpack .../pandoc_2.9.2.1-3ubuntu2_amd64.deb ...\n",
            "Unpacking pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Setting up libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Setting up pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.15\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
            "Downloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New data extraction from drive"
      ],
      "metadata": {
        "id": "V-HqtTT_kr3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger('__main__')\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "class GoogleDriveManager:\n",
        "    \"\"\"\n",
        "    Manages Google Drive operations including listing, filtering, downloading, and uploading files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, folder_id):\n",
        "        \"\"\"\n",
        "        Initialize the GoogleDriveManager with authentication and folder ID.\n",
        "\n",
        "        Args:\n",
        "            folder_id: The Google Drive folder ID to work with\n",
        "        \"\"\"\n",
        "        self.folder_id = folder_id\n",
        "        self.drive_service = None\n",
        "        self.files_df = None  # Cache for file listings\n",
        "\n",
        "        # Authenticate and build service\n",
        "        self._authenticate()\n",
        "\n",
        "    def _authenticate(self):\n",
        "        \"\"\"Authenticate with Google Drive and build the service object.\"\"\"\n",
        "        try:\n",
        "            auth.authenticate_user()\n",
        "            self.drive_service = build('drive', 'v3')\n",
        "            logger.info(\"âœ… Successfully authenticated with Google Drive\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ Authentication failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def list_all_files(self, force_refresh=False):\n",
        "        \"\"\"\n",
        "        Recursively list all files in the folder and subfolders.\n",
        "\n",
        "        Args:\n",
        "            force_refresh: If True, force a new listing even if cached data exists\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with columns [file_name, file_path, file_id, file_url]\n",
        "        \"\"\"\n",
        "        if self.files_df is not None and not force_refresh:\n",
        "            logger.info(\"ðŸ“‹ Using cached file list\")\n",
        "            return self.files_df\n",
        "\n",
        "        logger.info(\"ðŸ” Listing all files in folder...\")\n",
        "        all_files = self._list_files_recursive(self.folder_id)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        if all_files:\n",
        "            self.files_df = pd.DataFrame(all_files)\n",
        "\n",
        "            # Deduplicate by folder+name (file_path already encodes folder)\n",
        "            self.files_df = self.files_df.drop_duplicates(\n",
        "                subset=[\"file_path\", \"file_name\"], keep=\"first\"\n",
        "            )\n",
        "\n",
        "            logger.info(f\"âœ… Found {len(self.files_df)} unique files\")\n",
        "        else:\n",
        "            self.files_df = pd.DataFrame(columns=['file_name', 'file_path', 'file_id', 'file_url'])\n",
        "            logger.info(\"ðŸ“ No files found in folder\")\n",
        "\n",
        "        return self.files_df\n",
        "\n",
        "    def _list_files_recursive(self, parent_id, parent_path=\"\"):\n",
        "        \"\"\"\n",
        "        Recursively list files in a folder.\n",
        "\n",
        "        Args:\n",
        "            parent_id: Google Drive folder ID\n",
        "            parent_path: Path string for tracking folder hierarchy\n",
        "\n",
        "        Returns:\n",
        "            list: List of file dictionaries\n",
        "        \"\"\"\n",
        "        all_files = []\n",
        "        query = f\"'{parent_id}' in parents and trashed=false\"\n",
        "        page_token = None\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                response = self.drive_service.files().list(\n",
        "                    q=query,\n",
        "                    spaces='drive',\n",
        "                    fields='nextPageToken, files(id, name, mimeType)',\n",
        "                    pageToken=page_token\n",
        "                ).execute()\n",
        "\n",
        "                for item in response.get('files', []):\n",
        "                    item_path = f\"{parent_path}/{item['name']}\" if parent_path else item['name']\n",
        "\n",
        "                    if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "                        # Recurse into subfolder\n",
        "                        all_files.extend(self._list_files_recursive(item['id'], item_path))\n",
        "                    else:\n",
        "                        all_files.append({\n",
        "                            \"file_name\": item['name'],\n",
        "                            \"file_path\": item_path,\n",
        "                            \"file_id\": item['id'],\n",
        "                            \"file_url\": f\"https://drive.google.com/file/d/{item['id']}/view?usp=sharing\"\n",
        "                        })\n",
        "\n",
        "                page_token = response.get('nextPageToken', None)\n",
        "                if page_token is None:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"âŒ Error listing files in {parent_path}: {e}\")\n",
        "                break\n",
        "\n",
        "        return all_files\n",
        "\n",
        "    def filter_files(self, df=None, years=None, chapters=None):\n",
        "        \"\"\"\n",
        "        Filter files based on specified years and chapters using exact matching.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to filter (if None, uses cached files_df)\n",
        "            years: List of years to include (e.g., [2021, 2022, 2023])\n",
        "            chapters: List of chapter numbers to include (e.g., [1, 2, 5, 10])\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Filtered DataFrame containing only requested files\n",
        "        \"\"\"\n",
        "        # Use provided df or cached one\n",
        "        if df is None:\n",
        "            if self.files_df is None:\n",
        "                logger.warning(\"âš ï¸ No files listed yet. Running list_all_files() first.\")\n",
        "                self.list_all_files()\n",
        "            df = self.files_df.copy()\n",
        "        else:\n",
        "            df = df.copy()\n",
        "\n",
        "        if df.empty:\n",
        "            logger.warning(\"âš ï¸ No files to filter\")\n",
        "            return df\n",
        "\n",
        "        # Apply year filter\n",
        "        if years is not None:\n",
        "            year_strings = [str(year) for year in years]\n",
        "            # Exact match: year must be a folder in the path\n",
        "            year_mask = df['file_path'].apply(\n",
        "                lambda path: any(f\"/{year}/\" in f\"/{path}\" or path.startswith(f\"{year}/\")\n",
        "                               for year in year_strings)\n",
        "            )\n",
        "            df = df[year_mask]\n",
        "            logger.info(f\"ðŸ“… Filtered for years: {years} - {len(df)} files\")\n",
        "\n",
        "        # Apply chapter filter\n",
        "        if chapters is not None:\n",
        "            # Exact match for filename pattern: 01.docx, 02.docx, etc.\n",
        "            chapter_filenames = [f\"{ch:02d}.docx\" for ch in chapters]\n",
        "            chapter_mask = df['file_name'].apply(\n",
        "                lambda name: name in chapter_filenames\n",
        "            )\n",
        "            df = df[chapter_mask]\n",
        "            logger.info(f\"ðŸ“– Filtered for chapters: {chapters} - {len(df)} files\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def download_files(self, filtered_df, download_dir=\"/content/reports\"):\n",
        "        \"\"\"\n",
        "        Download files from a filtered DataFrame.\n",
        "\n",
        "        Args:\n",
        "            filtered_df: DataFrame containing files to download\n",
        "            download_dir: Base directory for downloads\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping file paths to local paths\n",
        "        \"\"\"\n",
        "        if filtered_df is None or filtered_df.empty:\n",
        "            logger.warning(\"âš ï¸ No files to download\")\n",
        "            return {}\n",
        "\n",
        "        downloaded_files = {}\n",
        "        total_files = len(filtered_df)\n",
        "\n",
        "        logger.info(f\"ðŸ“¥ Starting download of {total_files} files...\")\n",
        "\n",
        "        for idx, row in filtered_df.iterrows():\n",
        "            file_id = row['file_id']\n",
        "            file_name = row['file_name']\n",
        "            file_path = row['file_path']\n",
        "\n",
        "            # Extract year from path (assuming structure: year/filename)\n",
        "            path_parts = file_path.split('/')\n",
        "            if len(path_parts) >= 2:\n",
        "                year = path_parts[0]\n",
        "                local_path = os.path.join(download_dir, year, file_name)\n",
        "            else:\n",
        "                local_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "            # Ensure directory exists\n",
        "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "            try:\n",
        "                # Download file\n",
        "                request = self.drive_service.files().get_media(fileId=file_id)\n",
        "                fh = io.FileIO(local_path, \"wb\")\n",
        "                downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "                done = False\n",
        "                while not done:\n",
        "                    status, done = downloader.next_chunk()\n",
        "                    if status:\n",
        "                        progress = int(status.progress() * 100)\n",
        "                        print(f\"â¬‡ï¸  Downloading {file_name}: {progress}%\", end='\\r')\n",
        "\n",
        "                logger.info(f\"âœ… Downloaded {file_name} to {local_path}\")\n",
        "                downloaded_files[file_path] = local_path\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"âš ï¸ Failed to download {file_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        logger.info(f\"âœ… Download complete: {len(downloaded_files)}/{total_files} files\")\n",
        "        return downloaded_files\n",
        "\n",
        "    def download_selective(self, years=None, chapters=None, download_dir=\"/content/reports\"):\n",
        "        \"\"\"\n",
        "        Convenience method to list, filter, and download files in one operation.\n",
        "\n",
        "        Args:\n",
        "            years: List of years to download (e.g., [2021, 2022, 2023])\n",
        "            chapters: List of chapter numbers to download (e.g., [1, 2, 5, 10])\n",
        "            download_dir: Base directory for downloads\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping file paths to local paths\n",
        "\n",
        "        Example:\n",
        "            # Download chapters 1-5 for years 2021-2023\n",
        "            manager.download_selective(\n",
        "                years=range(2021, 2024),\n",
        "                chapters=range(1, 6),\n",
        "                download_dir=\"/content/reports\"\n",
        "            )\n",
        "        \"\"\"\n",
        "        # Step 1: List all files\n",
        "        logger.info(\"ðŸš€ Starting selective download workflow...\")\n",
        "        all_files = self.list_all_files()\n",
        "\n",
        "        # Step 2: Filter files\n",
        "        filtered_files = self.filter_files(all_files, years=years, chapters=chapters)\n",
        "\n",
        "        if filtered_files is None or filtered_files.empty:\n",
        "            logger.warning(\"âš ï¸ No files match the specified criteria\")\n",
        "            return {}\n",
        "\n",
        "        logger.info(f\"ðŸ“Š Found {len(filtered_files)} files matching criteria\")\n",
        "\n",
        "        # Step 3: Download filtered files\n",
        "        downloaded = self.download_files(filtered_files, download_dir)\n",
        "\n",
        "        return downloaded\n",
        "\n",
        "    def get_summary(self, df=None):\n",
        "        \"\"\"\n",
        "        Get summary statistics about the files.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to summarize (if None, uses cached files_df)\n",
        "\n",
        "        Returns:\n",
        "            dict: Summary statistics\n",
        "        \"\"\"\n",
        "        if df is None:\n",
        "            if self.files_df is None:\n",
        "                logger.warning(\"âš ï¸ No files listed yet. Running list_all_files() first.\")\n",
        "                self.list_all_files()\n",
        "            df = self.files_df\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            return {\"total_files\": 0, \"years\": [], \"chapters\": []}\n",
        "\n",
        "        # Extract years from paths\n",
        "        years = df['file_path'].apply(lambda x: x.split('/')[0] if '/' in x else None)\n",
        "        years = sorted(years.dropna().unique())\n",
        "\n",
        "        # Extract chapters from filenames (assuming pattern: 01.docx, 02.docx)\n",
        "        chapters = df['file_name'].apply(\n",
        "            lambda x: int(x[:2]) if x[:2].isdigit() and x.endswith('.docx') else None\n",
        "        )\n",
        "        chapters = sorted(chapters.dropna().unique())\n",
        "\n",
        "        summary = {\n",
        "            \"total_files\": len(df),\n",
        "            \"years\": years,\n",
        "            \"year_count\": len(years),\n",
        "            \"chapters\": chapters,\n",
        "            \"chapter_count\": len(chapters),\n",
        "            \"file_types\": df['file_name'].apply(lambda x: x.split('.')[-1]).value_counts().to_dict()\n",
        "        }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def preview_files(self, df=None, n=10):\n",
        "        \"\"\"\n",
        "        Preview first n files from the DataFrame.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to preview (if None, uses cached files_df)\n",
        "            n: Number of files to preview\n",
        "        \"\"\"\n",
        "        if df is None:\n",
        "            if self.files_df is None:\n",
        "                logger.warning(\"âš ï¸ No files listed yet. Running list_all_files() first.\")\n",
        "                self.list_all_files()\n",
        "            df = self.files_df\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            logger.info(\"No files to preview\")\n",
        "            return\n",
        "\n",
        "        preview = df.head(n)[['file_name', 'file_path']]\n",
        "        logger.info(f\"\\nðŸ“‹ Preview of first {min(n, len(df))} files:\")\n",
        "        for idx, row in preview.iterrows():\n",
        "            logger.info(f\"  {row['file_path']}\")\n",
        "\n",
        "    def check_missing_files(self, years, chapters):\n",
        "        \"\"\"\n",
        "        Check which year/chapter combinations are missing.\n",
        "\n",
        "        Args:\n",
        "            years: List of years to check\n",
        "            chapters: List of chapter numbers to check\n",
        "\n",
        "        Returns:\n",
        "            list: List of missing (year, chapter) tuples\n",
        "        \"\"\"\n",
        "        if self.files_df is None:\n",
        "            self.list_all_files()\n",
        "\n",
        "        missing = []\n",
        "\n",
        "        for year in years:\n",
        "            for chapter in chapters:\n",
        "                # Check if this combination exists\n",
        "                filtered = self.filter_files(\n",
        "                    self.files_df,\n",
        "                    years=[year],\n",
        "                    chapters=[chapter]\n",
        "                )\n",
        "\n",
        "                if filtered is None or filtered.empty:\n",
        "                    missing.append((year, chapter))\n",
        "                    logger.warning(f\"âš ï¸ Missing: Year {year}, Chapter {chapter:02d}\")\n",
        "\n",
        "        if missing:\n",
        "            logger.info(f\"ðŸ“Š Total missing files: {len(missing)}\")\n",
        "        else:\n",
        "            logger.info(\"âœ… All requested files are present\")\n",
        "\n",
        "        return missing"
      ],
      "metadata": {
        "id": "Km-JPMKWmgBM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "folder_id = \"1e0eA-AIsz_BSwVHOppJMXECX42hBfG4J\"\n",
        "manager = GoogleDriveManager(folder_id)\n",
        "\n",
        "# Download specific years and chapters\n",
        "downloaded = manager.download_selective(\n",
        "    years=range(2001, 2025),\n",
        "    chapters=[1]\n",
        ")\n",
        "\n",
        "print(f\"Downloaded {len(downloaded)} files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QTrNaXWpI3Q",
        "outputId": "58f2d068-0330-41f9-b06a-cf76cc7202b2",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Successfully authenticated with Google Drive\n",
            "INFO:__main__:ðŸš€ Starting selective download workflow...\n",
            "INFO:__main__:ðŸ” Listing all files in folder...\n",
            "INFO:__main__:âœ… Found 882 unique files\n",
            "INFO:__main__:ðŸ“… Filtered for years: range(2001, 2025) - 882 files\n",
            "INFO:__main__:ðŸ“– Filtered for chapters: [1] - 24 files\n",
            "INFO:__main__:ðŸ“Š Found 24 files matching criteria\n",
            "INFO:__main__:ðŸ“¥ Starting download of 24 files...\n",
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2020/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2015/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2004/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2003/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2018/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2019/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2021/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2021/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2002/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2005/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2010/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2011/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2009/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2017/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2006/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2007/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2008/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2001/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2024/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2022/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2023/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2023/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2012/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2014/01.docx\n",
            "INFO:__main__:âœ… Download complete: 24/24 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\rDownloaded 24 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data extraction class"
      ],
      "metadata": {
        "id": "OkcPb6r0r461"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "\n",
        "\n",
        "class TableExtractor:\n",
        "    \"\"\"Simple class for extracting tables from Word documents with statistics tracking.\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/reports\", out_dir=\"/content/tables\"):\n",
        "        \"\"\"Initialize the extractor with directories and statistics.\"\"\"\n",
        "        self.base_dir = base_dir\n",
        "        self.out_dir = out_dir\n",
        "\n",
        "        # Configuration constants\n",
        "        self.YEAR_RANGE = (2001, 2025)\n",
        "        self.VALID_EXTENSION = \".docx\"\n",
        "        self.TABLE_MARKER = \"×œ×•×—\"  # Hebrew for \"table\"\n",
        "        self.EXCLUDE_MARKER = \"×ª×¨×©×™×\"  # Hebrew for \"diagram\" - exclude these\n",
        "        self.ENCODING = \"utf-8-sig\"\n",
        "        self.SUMMARY_FILE = \"tables_summary.json\"\n",
        "        self.COLUMNS_FILE = \"tables_columns.json\"\n",
        "\n",
        "        # Statistics tracking\n",
        "        self.total_tables = 0\n",
        "\n",
        "        # Metadata collectors\n",
        "        self.all_summaries = {}\n",
        "        self.all_colnames = {}\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(self.out_dir, exist_ok=True)\n",
        "\n",
        "    def _is_valid_table(self, table):\n",
        "        \"\"\"\n",
        "        Check if a table is valid (contains Hebrew table marker in first row).\n",
        "\n",
        "        Args:\n",
        "            table: A docx table object\n",
        "\n",
        "        Returns:\n",
        "            tuple: (is_valid: bool, table_name: str)\n",
        "        \"\"\"\n",
        "        if len(table.rows) == 0:\n",
        "            return False, \"\"\n",
        "\n",
        "        # Check first row cells for table marker\n",
        "        for cell in table.rows[0].cells:\n",
        "            cell_text = cell.text\n",
        "            if self.TABLE_MARKER in cell_text and self.EXCLUDE_MARKER not in cell_text:\n",
        "                return True, cell_text.strip()\n",
        "\n",
        "        return False, \"\"\n",
        "\n",
        "    def _extract_table_data(self, table):\n",
        "        \"\"\"\n",
        "        Extract data from a docx table and convert to DataFrame.\n",
        "\n",
        "        Args:\n",
        "            table: A docx table object\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Table data as a DataFrame\n",
        "        \"\"\"\n",
        "        data = [[cell.text.strip() for cell in row.cells] for row in table.rows]\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def _save_table_data(self, df, identifier, year, chapter):\n",
        "        \"\"\"\n",
        "        Save DataFrame as CSV file in the appropriate directory structure.\n",
        "\n",
        "        Args:\n",
        "            df: pandas DataFrame to save\n",
        "            identifier: Unique identifier for the table\n",
        "            year: Year of the document\n",
        "            chapter: Chapter identifier\n",
        "\n",
        "        Returns:\n",
        "            str: Path where the file was saved\n",
        "        \"\"\"\n",
        "        save_dir = os.path.join(self.out_dir, str(year), chapter)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        save_path = os.path.join(save_dir, f\"{identifier}.csv\")\n",
        "        df.to_csv(save_path, index=False, encoding=self.ENCODING)\n",
        "\n",
        "        return save_path\n",
        "\n",
        "    def _process_document(self, fpath, year, chapter):\n",
        "        \"\"\"\n",
        "        Process a single Word document and extract all valid tables.\n",
        "\n",
        "        Args:\n",
        "            fpath: Full path to the document\n",
        "            year: Year of the document\n",
        "            chapter: Chapter identifier from filename\n",
        "\n",
        "        Returns:\n",
        "            int: Number of tables extracted from this document\n",
        "        \"\"\"\n",
        "        summary = {}\n",
        "        colnames_map = {}\n",
        "        tables_extracted = 0\n",
        "\n",
        "        try:\n",
        "            doc = Document(fpath)\n",
        "        except Exception as e:\n",
        "            print(f\"skip {fpath}: {e}\")\n",
        "            return 0\n",
        "\n",
        "        serial = 1\n",
        "\n",
        "        for table in doc.tables:\n",
        "            # Validate table\n",
        "            is_valid, table_name = self._is_valid_table(table)\n",
        "            if not is_valid:\n",
        "                continue\n",
        "\n",
        "            # Extract data\n",
        "            df = self._extract_table_data(table)\n",
        "\n",
        "            # Skip empty tables\n",
        "            if len(df) == 0:\n",
        "                continue\n",
        "\n",
        "            # Remove \".docx\" suffix from chapter name\n",
        "            if \".docx\" in chapter:\n",
        "              chapter = chapter.replace('.docx', '')\n",
        "\n",
        "            # Create identifier\n",
        "            identifier = f\"{serial}_{chapter}_{year}\"\n",
        "\n",
        "            # Record mapping for JSON\n",
        "            if len(df) > 0:\n",
        "                # Deduplicate consecutive repeated text in header\n",
        "                header_cells = df.iloc[0].astype(str).tolist()\n",
        "                unique_header = []\n",
        "                for cell in header_cells:\n",
        "                    if not unique_header or cell != unique_header[-1]:\n",
        "                        unique_header.append(cell)\n",
        "                summary[identifier] = \" \".join(unique_header)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            # Combine rows [1] and [2] for column names\n",
        "            if len(df) > 2:\n",
        "                row1 = df.iloc[1].astype(str).tolist()\n",
        "                row2 = df.iloc[2].astype(str).tolist()\n",
        "                colnames_map[identifier] = [f\"{r1} {r2}\".strip() for r1, r2 in zip(row1, row2)]\n",
        "            elif len(df) > 1:\n",
        "                colnames_map[identifier] = df.iloc[1].astype(str).tolist()\n",
        "            else:\n",
        "                colnames_map[identifier] = []\n",
        "\n",
        "            # Save to CSV\n",
        "            self._save_table_data(df, identifier, year, chapter)\n",
        "\n",
        "            tables_extracted += 1\n",
        "            serial += 1\n",
        "\n",
        "        # Update metadata collectors\n",
        "        self.all_summaries.update(summary)\n",
        "        self.all_colnames.update(colnames_map)\n",
        "\n",
        "        return tables_extracted\n",
        "\n",
        "    def _save_metadata(self):\n",
        "        \"\"\"Save summary and column metadata to JSON files.\"\"\"\n",
        "        with open(os.path.join(self.out_dir, self.SUMMARY_FILE), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.all_summaries, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        with open(os.path.join(self.out_dir, self.COLUMNS_FILE), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.all_colnames, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def process_files(self, years=None, chapters=None):\n",
        "        \"\"\"\n",
        "        Process Word documents filtered by years and chapters.\n",
        "\n",
        "        Args:\n",
        "            years: List/range of years to process (None = all years in YEAR_RANGE)\n",
        "            chapters: List of chapter identifiers to process (None = all chapters)\n",
        "\n",
        "        Example:\n",
        "            extractor.process_files()  # Process all files\n",
        "            extractor.process_files(years=[2023, 2024])  # Specific years\n",
        "            extractor.process_files(chapters=['1', '2', '3'])  # Specific chapters\n",
        "            extractor.process_files(years=range(2020, 2025), chapters=['1', '2'])  # Both\n",
        "        \"\"\"\n",
        "        # Reset statistics for new extraction session\n",
        "        self.total_tables = 0\n",
        "        self.tables_per_chapter_year = {}\n",
        "        self.all_summaries = {}\n",
        "        self.all_colnames = {}\n",
        "\n",
        "        # Determine which years to process\n",
        "        if years is None:\n",
        "            years_to_process = range(2001, 2025)\n",
        "        else:\n",
        "            years_to_process = years\n",
        "\n",
        "        # Convert chapters to set for faster lookup (if provided)\n",
        "        chapters_to_process = set(map(str, chapters)) if chapters else None\n",
        "\n",
        "        # Process each year\n",
        "        for year in years_to_process:\n",
        "            print(year)\n",
        "            year_path = os.path.join(self.base_dir, str(year))\n",
        "\n",
        "            if not os.path.isdir(year_path):\n",
        "                continue\n",
        "\n",
        "            # Process each document in the year directory\n",
        "            for fname in os.listdir(year_path):\n",
        "                if not fname.endswith(self.VALID_EXTENSION):\n",
        "                    continue\n",
        "\n",
        "                # Extract chapter from filename\n",
        "                chapter = fname.split(\"_\")[0]\n",
        "\n",
        "                # Skip if not in chapters to process\n",
        "                if chapters_to_process and chapter not in chapters_to_process:\n",
        "                    continue\n",
        "\n",
        "                fpath = os.path.join(year_path, fname)\n",
        "\n",
        "                # Process the document and get table count\n",
        "                tables_in_doc = self._process_document(fpath, year, chapter)\n",
        "\n",
        "\n",
        "                # Update statistics\n",
        "                self.total_tables += tables_in_doc\n",
        "                if chapter not in self.tables_per_chapter_year:\n",
        "                    self.tables_per_chapter_year[chapter] = {}\n",
        "                if year not in self.tables_per_chapter_year[chapter]:\n",
        "                    self.tables_per_chapter_year[chapter][year] = 0\n",
        "                self.tables_per_chapter_year[chapter][year] += tables_in_doc\n",
        "\n",
        "        # Save consolidated metadata\n",
        "        self._save_metadata()\n",
        "\n",
        "        print(f\"\\nExtraction complete! Total tables: {self.total_tables}\")\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"\n",
        "        Get extraction statistics.\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary with 'total' and 'per_chapter_year' statistics\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'total': self.total_tables,\n",
        "            'per_chapter_year': self.tables_per_chapter_year\n",
        "        }\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print a formatted summary of extraction statistics.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"EXTRACTION SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Total tables extracted: {self.total_tables}\")\n",
        "\n",
        "        if self.tables_per_chapter_year:\n",
        "            print(\"\\nTables per chapter per year:\")\n",
        "            for chapter in sorted(self.tables_per_chapter_year.keys()):\n",
        "                print(f\"\\nChapter {chapter}:\")\n",
        "                for year in sorted(self.tables_per_chapter_year[chapter].keys()):\n",
        "                    count = self.tables_per_chapter_year[chapter][year]\n",
        "                    if count > 0:  # Only show years with tables\n",
        "                        print(f\"  {year}: {count}\")\n",
        "        else:\n",
        "            print(\"\\nNo tables extracted.\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "    def _identify_continuation_groups(self, summaries):\n",
        "        \"\"\"\n",
        "        Identify groups of tables that should be combined (original + continuations).\n",
        "\n",
        "        Args:\n",
        "            summaries: Dictionary of table summaries\n",
        "\n",
        "        Returns:\n",
        "            dict: Groups of related tables {original_id: [original_id, continuation_ids...]}\n",
        "        \"\"\"\n",
        "        groups = {}\n",
        "        continuation_marker = \"(×”×ž×©×š)\"\n",
        "\n",
        "        # Sort identifiers to process them in order (important for maintaining sequence)\n",
        "        sorted_ids = sorted(summaries.keys(), key=lambda x: (\n",
        "            int(x.split('_')[2]),  # year\n",
        "            x.split('_')[1],        # chapter\n",
        "            int(x.split('_')[0])    # serial number\n",
        "        ))\n",
        "\n",
        "        current_group_original = None\n",
        "        current_base_header = None\n",
        "\n",
        "        for identifier in sorted_ids:\n",
        "            header = summaries[identifier]\n",
        "\n",
        "            # Check if this is a continuation\n",
        "            if continuation_marker in header:\n",
        "                # Extract base header (without continuation marker)\n",
        "                base_header = header.replace(continuation_marker, \"\").strip()\n",
        "\n",
        "                # If we have a current group and the headers are similar enough, add to group\n",
        "                if current_group_original and current_base_header:\n",
        "                    if self._headers_match(base_header, current_base_header):\n",
        "                        groups[current_group_original].append(identifier)\n",
        "                    else:\n",
        "                        print(f\"Warning: Orphaned continuation found: {identifier}\")\n",
        "                        print(f\"  Looking for base: '{base_header}'\")\n",
        "                        print(f\"  Current base: '{current_base_header}'\")\n",
        "            else:\n",
        "                # This is an original table (not a continuation)\n",
        "                # Start a new group\n",
        "                current_group_original = identifier\n",
        "                current_base_header = header  # Store the full header as the base\n",
        "                groups[identifier] = [identifier]  # Group starts with the original\n",
        "\n",
        "        # Filter out groups with only one table (no continuations)\n",
        "        groups_with_continuations = {k: v for k, v in groups.items() if len(v) > 1}\n",
        "\n",
        "        return groups_with_continuations\n",
        "\n",
        "    def _headers_match(self, header1, header2, threshold=0.85):\n",
        "        \"\"\"\n",
        "        Check if two headers are similar enough to be considered the same table.\n",
        "        Allows for small differences in whitespace, punctuation, etc.\n",
        "\n",
        "        Args:\n",
        "            header1: First header string\n",
        "            header2: Second header string\n",
        "            threshold: Similarity threshold (0.0 to 1.0)\n",
        "\n",
        "        Returns:\n",
        "            bool: True if headers match\n",
        "        \"\"\"\n",
        "        # Normalize headers for comparison\n",
        "        def normalize(text):\n",
        "            # Remove extra whitespace, normalize spaces\n",
        "            text = ' '.join(text.split())\n",
        "            # Remove common variations\n",
        "            text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
        "            text = text.replace('  ', ' ')\n",
        "            return text.strip()\n",
        "\n",
        "        h1_normalized = normalize(header1)\n",
        "        h2_normalized = normalize(header2)\n",
        "\n",
        "        # Exact match after normalization\n",
        "        if h1_normalized == h2_normalized:\n",
        "            return True\n",
        "\n",
        "        # Calculate similarity ratio\n",
        "        # Use character-level similarity\n",
        "        longer = max(len(h1_normalized), len(h2_normalized))\n",
        "        if longer == 0:\n",
        "            return True\n",
        "\n",
        "        # Simple character matching approach\n",
        "        matches = 0\n",
        "        shorter_text = h1_normalized if len(h1_normalized) <= len(h2_normalized) else h2_normalized\n",
        "        longer_text = h2_normalized if len(h1_normalized) <= len(h2_normalized) else h1_normalized\n",
        "\n",
        "        # Check if shorter text is substantially contained in longer text\n",
        "        if shorter_text in longer_text:\n",
        "            return True\n",
        "\n",
        "        # Calculate overlap by finding longest common substring ratio\n",
        "        # For Hebrew text with potential small differences\n",
        "        words1 = set(h1_normalized.split())\n",
        "        words2 = set(h2_normalized.split())\n",
        "\n",
        "        if not words1 or not words2:\n",
        "            return False\n",
        "\n",
        "        # Calculate Jaccard similarity of words\n",
        "        intersection = words1.intersection(words2)\n",
        "        union = words1.union(words2)\n",
        "\n",
        "        if not union:\n",
        "            return False\n",
        "\n",
        "        similarity = len(intersection) / len(union)\n",
        "\n",
        "        return similarity >= threshold\n",
        "\n",
        "    def _combine_csv_files(self, identifiers, summaries):\n",
        "        \"\"\"\n",
        "        Load and combine multiple CSV files into one, removing duplicate headers.\n",
        "\n",
        "        Args:\n",
        "            identifiers: List of table identifiers [original, continuation1, ...]\n",
        "            summaries: Dictionary of table summaries for header detection\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Combined dataframe\n",
        "        \"\"\"\n",
        "        if not identifiers:\n",
        "            return None\n",
        "\n",
        "        combined_df = None\n",
        "        original_id = identifiers[0]\n",
        "        original_header = summaries[original_id]\n",
        "\n",
        "        # Parse identifier to get year and chapter\n",
        "        parts = original_id.split('_')\n",
        "        year = parts[2]\n",
        "        chapter = parts[1]\n",
        "\n",
        "        for i, identifier in enumerate(identifiers):\n",
        "            # Build path to CSV file\n",
        "            csv_path = os.path.join(self.out_dir, year, chapter, f\"{identifier}.csv\")\n",
        "\n",
        "            if not os.path.exists(csv_path):\n",
        "                print(f\"Warning: CSV file not found: {csv_path}\")\n",
        "                continue\n",
        "\n",
        "            # Load the CSV\n",
        "            df = pd.read_csv(csv_path, encoding=self.ENCODING)\n",
        "\n",
        "            if i == 0:\n",
        "                # First table (original) - keep everything\n",
        "                combined_df = df\n",
        "            else:\n",
        "                # Continuation table - need to remove duplicate header rows\n",
        "                # Get the header text from the first table to compare\n",
        "                if len(combined_df) > 0:\n",
        "                    # Create a string representation of the first row for comparison\n",
        "                    original_first_row = combined_df.iloc[0].astype(str).tolist()\n",
        "\n",
        "                    # Find and skip duplicate header rows in continuation\n",
        "                    start_idx = 0\n",
        "                    for idx in range(min(3, len(df))):  # Check first 3 rows max\n",
        "                        current_row = df.iloc[idx].astype(str).tolist()\n",
        "                        # Check if this row matches the original header pattern\n",
        "                        if self._is_header_row(current_row, original_first_row):\n",
        "                            start_idx = idx + 1\n",
        "                            break\n",
        "\n",
        "                    # Append from after the header\n",
        "                    if start_idx < len(df):\n",
        "                        combined_df = pd.concat([combined_df, df.iloc[start_idx:]],\n",
        "                                               ignore_index=True)\n",
        "                    else:\n",
        "                        # If all rows were headers, just append everything\n",
        "                        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "                else:\n",
        "                    # Original was empty, just append\n",
        "                    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    def _is_header_row(self, row, original_header_row):\n",
        "        \"\"\"\n",
        "        Check if a row appears to be a duplicate header row.\n",
        "\n",
        "        Args:\n",
        "            row: Row to check (as list of strings)\n",
        "            original_header_row: Original header row to compare against\n",
        "\n",
        "        Returns:\n",
        "            bool: True if this appears to be a duplicate header\n",
        "        \"\"\"\n",
        "        # Check if the rows have significant overlap in content\n",
        "        # This handles cases where headers might have slight variations\n",
        "        if len(row) != len(original_header_row):\n",
        "            return False\n",
        "\n",
        "        # Count matching cells (allowing for continuation marker)\n",
        "        matches = 0\n",
        "        for cell1, cell2 in zip(row, original_header_row):\n",
        "            # Remove continuation marker for comparison\n",
        "            cell1_clean = cell1.replace(\"(×”×ž×©×š)\", \"\").strip()\n",
        "            cell2_clean = cell2.replace(\"(×”×ž×©×š)\", \"\").strip()\n",
        "\n",
        "            if cell1_clean == cell2_clean and cell1_clean:  # Non-empty match\n",
        "                matches += 1\n",
        "\n",
        "        # If most cells match, it's likely a header row\n",
        "        return matches >= len(row) * 0.7  # 70% threshold\n",
        "\n",
        "    def combine_continuation_tables(self):\n",
        "        \"\"\"\n",
        "        Combine continuation tables with their originals after extraction.\n",
        "        This should be called after process_files() to merge any continuation tables.\n",
        "\n",
        "        Returns:\n",
        "            dict: Information about combined tables\n",
        "        \"\"\"\n",
        "        # Load current summaries\n",
        "        summary_path = os.path.join(self.out_dir, self.SUMMARY_FILE)\n",
        "        columns_path = os.path.join(self.out_dir, self.COLUMNS_FILE)\n",
        "\n",
        "        if not os.path.exists(summary_path):\n",
        "            print(\"No summaries file found. Run process_files() first.\")\n",
        "            return {}\n",
        "\n",
        "        # Load metadata\n",
        "        with open(summary_path, 'r', encoding='utf-8') as f:\n",
        "            summaries = json.load(f)\n",
        "\n",
        "        with open(columns_path, 'r', encoding='utf-8') as f:\n",
        "            colnames = json.load(f)\n",
        "\n",
        "        # Identify continuation groups\n",
        "        groups = self._identify_continuation_groups(summaries)\n",
        "\n",
        "        if not groups:\n",
        "            print(\"No continuation tables found.\")\n",
        "            return {}\n",
        "\n",
        "        print(f\"\\nFound {len(groups)} table(s) with continuations to combine...\")\n",
        "\n",
        "        # Track what we combined\n",
        "        combined_info = {}\n",
        "\n",
        "        # Process each group\n",
        "        for original_id, identifier_list in groups.items():\n",
        "            print(f\"\\nCombining {original_id} with {len(identifier_list)-1} continuation(s)...\")\n",
        "\n",
        "            # Combine the CSV files\n",
        "            combined_df = self._combine_csv_files(identifier_list, summaries)\n",
        "\n",
        "            if combined_df is not None:\n",
        "                # Parse identifier to get year and chapter\n",
        "                parts = original_id.split('_')\n",
        "                year = parts[2]\n",
        "                chapter = parts[1]\n",
        "\n",
        "                # Save the combined CSV (overwriting the original)\n",
        "                save_path = os.path.join(self.out_dir, year, chapter, f\"{original_id}.csv\")\n",
        "                combined_df.to_csv(save_path, index=False, encoding=self.ENCODING)\n",
        "\n",
        "                # Delete continuation CSV files\n",
        "                for continuation_id in identifier_list[1:]:  # Skip the original\n",
        "                    continuation_path = os.path.join(self.out_dir, year, chapter, f\"{continuation_id}.csv\")\n",
        "                    if os.path.exists(continuation_path):\n",
        "                        os.remove(continuation_path)\n",
        "                        print(f\"  Removed: {continuation_id}.csv\")\n",
        "\n",
        "                # Track combination info\n",
        "                combined_info[original_id] = {\n",
        "                    'parts_combined': len(identifier_list),\n",
        "                    'continuation_ids': identifier_list[1:],\n",
        "                    'rows_in_combined': len(combined_df)\n",
        "                }\n",
        "\n",
        "                print(f\"  Combined table saved as: {original_id}.csv ({len(combined_df)} rows)\")\n",
        "\n",
        "        # Update metadata files (remove continuation entries)\n",
        "        updated_summaries = {k: v for k, v in summaries.items()\n",
        "                            if \"(×”×ž×©×š)\" not in v}\n",
        "        updated_colnames = {k: v for k, v in colnames.items()\n",
        "                           if \"(×”×ž×©×š)\" not in summaries.get(k, \"\")}\n",
        "\n",
        "        # Save updated metadata\n",
        "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(updated_summaries, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        with open(columns_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(updated_colnames, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # Save combination tracking info\n",
        "        tracking_path = os.path.join(self.out_dir, \"combined_tables_info.json\")\n",
        "        with open(tracking_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(combined_info, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"\\nâœ“ Combination complete! Combined {len(groups)} table(s)\")\n",
        "        print(f\"  Combination details saved to: combined_tables_info.json\")\n",
        "\n",
        "        # Update statistics to reflect combinations\n",
        "        self._update_statistics_after_combination(combined_info)\n",
        "\n",
        "        return combined_info\n",
        "\n",
        "    def _update_statistics_after_combination(self, combined_info):\n",
        "        \"\"\"\n",
        "        Update statistics after combining continuation tables.\n",
        "\n",
        "        Args:\n",
        "            combined_info: Dictionary with combination information\n",
        "        \"\"\"\n",
        "        if not combined_info:\n",
        "            return\n",
        "\n",
        "        # Recalculate statistics based on combinations\n",
        "        for original_id, info in combined_info.items():\n",
        "            parts = original_id.split('_')\n",
        "            year = int(parts[2])\n",
        "            chapter = parts[1]\n",
        "\n",
        "            # Reduce count by number of continuations removed\n",
        "            continuations_removed = info['parts_combined'] - 1\n",
        "\n",
        "            # Update total\n",
        "            self.total_tables -= continuations_removed\n",
        "\n",
        "            # Update per-chapter-year statistics\n",
        "            if chapter in self.tables_per_chapter_year:\n",
        "                if year in self.tables_per_chapter_year[chapter]:\n",
        "                    self.tables_per_chapter_year[chapter][year] -= continuations_removed\n",
        "\n",
        "                    # Clean up if count becomes 0\n",
        "                    if self.tables_per_chapter_year[chapter][year] <= 0:\n",
        "                        self.tables_per_chapter_year[chapter][year] = 1  # At least the combined table exists\n",
        "\n",
        "        print(f\"\\nðŸ“Š Statistics updated: {self.total_tables} unique tables after combination\")"
      ],
      "metadata": {
        "id": "ErtYpb9Qr8O5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "extractor = TableExtractor(base_dir=\"/content/reports\", out_dir=\"/content/tables\")\n",
        "\n",
        "# Process everything\n",
        "extractor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeHTZd6gtq-X",
        "outputId": "046cb7a1-07aa-4ede-93a8-168db392509a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "\n",
            "Extraction complete! Total tables: 266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine continuation tables\n",
        "combined = extractor.combine_continuation_tables()\n",
        "\n",
        "# Get statistics\n",
        "stats = extractor.get_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1vmHozAnCNP",
        "outputId": "cfda18d0-47b1-45e8-e132-f4cca184f30e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Orphaned continuation found: 23_01_2001\n",
            "  Looking for base: '×œ×™×“×•×ª ×—×™ \n",
            "×œ×¤×™ ×™×™×©×•×‘* \n",
            "(×ž×¡×¤×¨×™× ×ž×•×—×œ×˜×™×) 2000-1995  ×œ×•×— 1.13'\n",
            "  Current base: '×œ×™×“×•×ª ×—×™ \n",
            " ×œ×¤×™ ×™×™×©×•×‘* (×ž×¡×¤×¨×™×)\n",
            "2000-1995 ×œ×•×— 1.13'\n",
            "Warning: Orphaned continuation found: 18_01_2002\n",
            "  Looking for base: '×œ×™×“×•×ª ×—×™ \n",
            "×œ×¤×™ ×™×™×©×•×‘* \n",
            "(×ž×¡×¤×¨×™× ×ž×•×—×œ×˜×™×) 2001-1995  ×œ×•×— 1.11'\n",
            "  Current base: '×œ×™×“×•×ª ×—×™ \n",
            " ×œ×¤×™ ×™×™×©×•×‘* (×ž×¡×¤×¨×™×)\n",
            "2001-1995 ×œ×•×— 1.11'\n",
            "Warning: Orphaned continuation found: 19_01_2002\n",
            "  Looking for base: '×œ×™×“×•×ª ×—×™ \n",
            "×œ×¤×™ ×™×™×©×•×‘* \n",
            "(×ž×¡×¤×¨×™× ×ž×•×—×œ×˜×™×) 2001-1995  ×œ×•×— 1.11'\n",
            "  Current base: '×œ×™×“×•×ª ×—×™ \n",
            " ×œ×¤×™ ×™×™×©×•×‘* (×ž×¡×¤×¨×™×)\n",
            "2001-1995 ×œ×•×— 1.11'\n",
            "Warning: Orphaned continuation found: 7_01_2004\n",
            "  Looking for base: '×ž×¡×¤×¨ ×”×™×œ×“×™× ×œ×¤×™ ×’×™×œ ×•×—×œ×§× ×‘××•×›×œ×•×¡×™×™×ª ×”×™×™×©×•×‘×™×* \n",
            "(××œ×¤×™× ×•××—×•×–×™×)\n",
            "×¡×•×£×©× ×ª2003  ×œ×•×— 1.6'\n",
            "  Current base: '×ž×¡×¤×¨ ×”×™×œ×“×™× ×œ×¤×™ ×’×™×œ ×•×—×œ×§× ×‘××•×›×œ×•×¡×™×™×ª ×”×™×™×©×•×‘×™×* \n",
            "(××œ×¤×™× ×•××—×•×–×™×)\n",
            "×¡×•×£ ×©× ×ª 2003 ×œ×•×— 1.6'\n",
            "Warning: Orphaned continuation found: 9_01_2012\n",
            "  Looking for base: '×ž×¡×¤×¨ ×”×™×œ×“×™× \n",
            "×œ×¤×™ ×’×™×œ ×•×—×œ×§× ×‘××•×›×œ×•×¡×™×™×ª ×”×™×™×©×•×‘* (××œ×¤×™× ×•××—×•×–×™×)\n",
            "×“×¦×ž×‘×¨ 2011  ×œ×•×— 1.5'\n",
            "  Current base: '×ž×¡×¤×¨ ×”×™×œ×“×™×  ×œ×”×•×¡×™×£ ××ª ×“×™×™×¨ ××œ ××¡×“ ×‘10000+\n",
            "×œ×¤×™ ×’×™×œ ×•×—×œ×§× ×‘××•×›×œ×•×¡×™×™×ª ×”×™×™×©×•×‘* (×ž×¡×¤×¨×™× ×•××—×•×–×™×)\n",
            "×“×¦×ž×‘×¨ 2011 ×œ×•×— 1.5'\n",
            "Warning: Orphaned continuation found: 10_01_2012\n",
            "  Looking for base: '×ž×¡×¤×¨ ×”×™×œ×“×™× \n",
            "×œ×¤×™ ×’×™×œ ×•×—×œ×§× ×‘××•×›×œ×•×¡×™×™×ª ×”×™×™×©×•×‘* (××œ×¤×™× ×•××—×•×–×™×)\n",
            "×“×¦×ž×‘×¨ 2011  ×œ×•×— 1.5'\n",
            "  Current base: '×ž×¡×¤×¨ ×”×™×œ×“×™×  ×œ×”×•×¡×™×£ ××ª ×“×™×™×¨ ××œ ××¡×“ ×‘10000+\n",
            "×œ×¤×™ ×’×™×œ ×•×—×œ×§× ×‘××•×›×œ×•×¡×™×™×ª ×”×™×™×©×•×‘* (×ž×¡×¤×¨×™× ×•××—×•×–×™×)\n",
            "×“×¦×ž×‘×¨ 2011 ×œ×•×— 1.5'\n",
            "Warning: Orphaned continuation found: 16_01_2012\n",
            "  Looking for base: '×œ×•×—\n",
            "2.2 ×™×œ×“×™× ×¢×•×œ×™× \n",
            "×œ×¤×™ ×ª×§×•×¤×ª ×”×¢×œ×™×™×” ×œ×™×©×¨××œ, ××¨×¥ ×ž×•×¦× ×•×’×™×œ× ×”×™×•× (×ž×¡×¤×¨×™×)\n",
            "×™× ×•××¨ 2012'\n",
            "  Current base: '×œ×•×—\n",
            "2.2 ×™×œ×“×™× ×¢×•×œ×™×   ×”×•×¤×š ×œ×ª×¨×©×™× ×¢×ž×•×“×•×ª ×©×œ ×¡×š ×”×›×œ ×‘××—×•×–×™× ×©×œ ×ž×™ ×©×¢×œ×” ×¢×“ 2001 ×•××—×¨×™ ×œ×¤×™ ×ž×•×¦×\n",
            "×œ×¤×™ ×ª×§×•×¤×ª ×”×¢×œ×™×™×” ×œ×™×©×¨××œ, ××¨×¥ ×ž×•×¦× ×•×’×™×œ× ×”×™×•× (×ž×¡×¤×¨×™×)\n",
            "×™× ×•××¨ 2012'\n",
            "\n",
            "Found 30 table(s) with continuations to combine...\n",
            "\n",
            "Combining 3_01_2001 with 1 continuation(s)...\n",
            "  Removed: 4_01_2001.csv\n",
            "  Combined table saved as: 3_01_2001.csv (74 rows)\n",
            "\n",
            "Combining 7_01_2001 with 4 continuation(s)...\n",
            "  Removed: 8_01_2001.csv\n",
            "  Removed: 9_01_2001.csv\n",
            "  Removed: 10_01_2001.csv\n",
            "  Removed: 11_01_2001.csv\n",
            "  Combined table saved as: 7_01_2001.csv (172 rows)\n",
            "\n",
            "Combining 15_01_2001 with 4 continuation(s)...\n",
            "  Removed: 16_01_2001.csv\n",
            "  Removed: 17_01_2001.csv\n",
            "  Removed: 18_01_2001.csv\n",
            "  Removed: 19_01_2001.csv\n",
            "  Combined table saved as: 15_01_2001.csv (167 rows)\n",
            "\n",
            "Combining 4_01_2002 with 2 continuation(s)...\n",
            "  Removed: 5_01_2002.csv\n",
            "  Removed: 6_01_2002.csv\n",
            "  Combined table saved as: 4_01_2002.csv (83 rows)\n",
            "\n",
            "Combining 8_01_2002 with 4 continuation(s)...\n",
            "  Removed: 9_01_2002.csv\n",
            "  Removed: 10_01_2002.csv\n",
            "  Removed: 11_01_2002.csv\n",
            "  Removed: 12_01_2002.csv\n",
            "  Combined table saved as: 8_01_2002.csv (182 rows)\n",
            "\n",
            "Combining 6_01_2003 with 4 continuation(s)...\n",
            "  Removed: 7_01_2003.csv\n",
            "  Removed: 8_01_2003.csv\n",
            "  Removed: 9_01_2003.csv\n",
            "  Removed: 10_01_2003.csv\n",
            "  Combined table saved as: 6_01_2003.csv (183 rows)\n",
            "\n",
            "Combining 12_01_2003 with 2 continuation(s)...\n",
            "  Removed: 13_01_2003.csv\n",
            "  Removed: 14_01_2003.csv\n",
            "  Combined table saved as: 12_01_2003.csv (69 rows)\n",
            "\n",
            "Combining 6_01_2004 with 3 continuation(s)...\n",
            "  Removed: 8_01_2004.csv\n",
            "  Removed: 9_01_2004.csv\n",
            "  Removed: 10_01_2004.csv\n",
            "  Combined table saved as: 6_01_2004.csv (139 rows)\n",
            "\n",
            "Combining 13_01_2004 with 3 continuation(s)...\n",
            "  Removed: 14_01_2004.csv\n",
            "  Removed: 15_01_2004.csv\n",
            "  Removed: 16_01_2004.csv\n",
            "  Combined table saved as: 13_01_2004.csv (111 rows)\n",
            "\n",
            "Combining 9_01_2005 with 3 continuation(s)...\n",
            "  Removed: 10_01_2005.csv\n",
            "  Removed: 11_01_2005.csv\n",
            "  Removed: 12_01_2005.csv\n",
            "  Combined table saved as: 9_01_2005.csv (111 rows)\n",
            "\n",
            "Combining 6_01_2006 with 1 continuation(s)...\n",
            "  Removed: 7_01_2006.csv\n",
            "  Combined table saved as: 6_01_2006.csv (190 rows)\n",
            "\n",
            "Combining 10_01_2006 with 3 continuation(s)...\n",
            "  Removed: 11_01_2006.csv\n",
            "  Removed: 12_01_2006.csv\n",
            "  Removed: 13_01_2006.csv\n",
            "  Combined table saved as: 10_01_2006.csv (111 rows)\n",
            "\n",
            "Combining 8_01_2007 with 1 continuation(s)...\n",
            "  Removed: 9_01_2007.csv\n",
            "  Combined table saved as: 8_01_2007.csv (193 rows)\n",
            "\n",
            "Combining 12_01_2007 with 2 continuation(s)...\n",
            "  Removed: 13_01_2007.csv\n",
            "  Removed: 14_01_2007.csv\n",
            "  Combined table saved as: 12_01_2007.csv (111 rows)\n",
            "\n",
            "Combining 8_01_2008 with 5 continuation(s)...\n",
            "  Removed: 9_01_2008.csv\n",
            "  Removed: 10_01_2008.csv\n",
            "  Removed: 11_01_2008.csv\n",
            "  Removed: 12_01_2008.csv\n",
            "  Removed: 13_01_2008.csv\n",
            "  Combined table saved as: 8_01_2008.csv (194 rows)\n",
            "\n",
            "Combining 16_01_2008 with 3 continuation(s)...\n",
            "  Removed: 17_01_2008.csv\n",
            "  Removed: 18_01_2008.csv\n",
            "  Removed: 19_01_2008.csv\n",
            "  Combined table saved as: 16_01_2008.csv (125 rows)\n",
            "\n",
            "Combining 8_01_2009 with 3 continuation(s)...\n",
            "  Removed: 9_01_2009.csv\n",
            "  Removed: 10_01_2009.csv\n",
            "  Removed: 11_01_2009.csv\n",
            "  Combined table saved as: 8_01_2009.csv (194 rows)\n",
            "\n",
            "Combining 6_01_2010 with 1 continuation(s)...\n",
            "  Removed: 7_01_2010.csv\n",
            "  Combined table saved as: 6_01_2010.csv (34 rows)\n",
            "\n",
            "Combining 8_01_2010 with 2 continuation(s)...\n",
            "  Removed: 9_01_2010.csv\n",
            "  Removed: 10_01_2010.csv\n",
            "  Combined table saved as: 8_01_2010.csv (195 rows)\n",
            "\n",
            "Combining 6_01_2011 with 1 continuation(s)...\n",
            "  Removed: 7_01_2011.csv\n",
            "  Combined table saved as: 6_01_2011.csv (33 rows)\n",
            "\n",
            "Combining 8_01_2011 with 2 continuation(s)...\n",
            "  Removed: 9_01_2011.csv\n",
            "  Removed: 10_01_2011.csv\n",
            "  Combined table saved as: 8_01_2011.csv (196 rows)\n",
            "\n",
            "Combining 12_01_2011 with 1 continuation(s)...\n",
            "  Removed: 13_01_2011.csv\n",
            "  Combined table saved as: 12_01_2011.csv (129 rows)\n",
            "\n",
            "Combining 12_01_2012 with 1 continuation(s)...\n",
            "  Removed: 13_01_2012.csv\n",
            "  Combined table saved as: 12_01_2012.csv (132 rows)\n",
            "\n",
            "Combining 22_01_2012 with 1 continuation(s)...\n",
            "  Removed: 23_01_2012.csv\n",
            "  Combined table saved as: 22_01_2012.csv (122 rows)\n",
            "\n",
            "Combining 6_01_2014 with 1 continuation(s)...\n",
            "  Removed: 7_01_2014.csv\n",
            "  Combined table saved as: 6_01_2014.csv (32 rows)\n",
            "\n",
            "Combining 8_01_2014 with 3 continuation(s)...\n",
            "  Removed: 9_01_2014.csv\n",
            "  Removed: 10_01_2014.csv\n",
            "  Removed: 11_01_2014.csv\n",
            "  Combined table saved as: 8_01_2014.csv (200 rows)\n",
            "\n",
            "Combining 6_01_2015 with 1 continuation(s)...\n",
            "  Removed: 7_01_2015.csv\n",
            "  Combined table saved as: 6_01_2015.csv (32 rows)\n",
            "\n",
            "Combining 8_01_2015 with 2 continuation(s)...\n",
            "  Removed: 9_01_2015.csv\n",
            "  Removed: 10_01_2015.csv\n",
            "  Combined table saved as: 8_01_2015.csv (200 rows)\n",
            "\n",
            "Combining 6_01_2019 with 1 continuation(s)...\n",
            "  Removed: 7_01_2019.csv\n",
            "  Combined table saved as: 6_01_2019.csv (137 rows)\n",
            "\n",
            "Combining 4_01_2022 with 4 continuation(s)...\n",
            "  Removed: 5_01_2022.csv\n",
            "  Removed: 6_01_2022.csv\n",
            "  Removed: 7_01_2022.csv\n",
            "  Removed: 8_01_2022.csv\n",
            "  Combined table saved as: 4_01_2022.csv (214 rows)\n",
            "\n",
            "âœ“ Combination complete! Combined 30 table(s)\n",
            "  Combination details saved to: combined_tables_info.json\n",
            "\n",
            "ðŸ“Š Statistics updated: 197 unique tables after combination\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print summary\n",
        "extractor.print_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GePB3e9-v4UW",
        "outputId": "3a07ba6b-4d5b-41da-d84d-6f53cbc706c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "EXTRACTION SUMMARY\n",
            "==================================================\n",
            "Total tables extracted: 197\n",
            "\n",
            "Tables per chapter per year:\n",
            "\n",
            "Chapter 01.docx:\n",
            "  2001: 23\n",
            "  2002: 19\n",
            "  2003: 14\n",
            "  2004: 16\n",
            "  2005: 12\n",
            "  2006: 14\n",
            "  2007: 15\n",
            "  2008: 20\n",
            "  2009: 15\n",
            "  2010: 14\n",
            "  2011: 14\n",
            "  2012: 31\n",
            "  2014: 14\n",
            "  2015: 13\n",
            "  2017: 2\n",
            "  2018: 2\n",
            "  2019: 11\n",
            "  2020: 1\n",
            "  2021: 1\n",
            "  2022: 13\n",
            "  2023: 1\n",
            "  2024: 1\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}