{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiragelb/NCC-Statistical-Reports/blob/main/Table_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install pandoc\n",
        "!pip install pypandoc\n",
        "!pip install python-docx\n",
        "!pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMvSuUElnUQ7",
        "outputId": "15b77bc1-23c3-4eb0-84a5-3641a62b616a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc-data\n",
            "Suggested packages:\n",
            "  texlive-latex-recommended texlive-xetex texlive-luatex pandoc-citeproc\n",
            "  texlive-latex-extra context wkhtmltopdf librsvg2-bin groff ghc nodejs php\n",
            "  python ruby libjs-mathjax libjs-katex citation-style-language-styles\n",
            "The following NEW packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc\n",
            "  pandoc-data\n",
            "0 upgraded, 4 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 20.6 MB of archives.\n",
            "After this operation, 156 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [115 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm-extensions0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [25.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc-data all 2.9.2.1-3ubuntu2 [81.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc amd64 2.9.2.1-3ubuntu2 [20.3 MB]\n",
            "Fetched 20.6 MB in 1s (14.9 MB/s)\n",
            "Selecting previously unselected package libcmark-gfm0.29.0.gfm.3:amd64.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../libcmark-gfm0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcmark-gfm-extensions0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../libcmark-gfm-extensions0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package pandoc-data.\n",
            "Preparing to unpack .../pandoc-data_2.9.2.1-3ubuntu2_all.deb ...\n",
            "Unpacking pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package pandoc.\n",
            "Preparing to unpack .../pandoc_2.9.2.1-3ubuntu2_amd64.deb ...\n",
            "Unpacking pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Setting up libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Setting up pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.15\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
            "Downloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New data extraction from drive"
      ],
      "metadata": {
        "id": "V-HqtTT_kr3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger('__main__')\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "class GoogleDriveManager:\n",
        "    \"\"\"\n",
        "    Manages Google Drive operations including listing, filtering, downloading, and uploading files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, folder_id):\n",
        "        \"\"\"\n",
        "        Initialize the GoogleDriveManager with authentication and folder ID.\n",
        "\n",
        "        Args:\n",
        "            folder_id: The Google Drive folder ID to work with\n",
        "        \"\"\"\n",
        "        self.folder_id = folder_id\n",
        "        self.drive_service = None\n",
        "        self.files_df = None  # Cache for file listings\n",
        "\n",
        "        # Authenticate and build service\n",
        "        self._authenticate()\n",
        "\n",
        "    def _authenticate(self):\n",
        "        \"\"\"Authenticate with Google Drive and build the service object.\"\"\"\n",
        "        try:\n",
        "            auth.authenticate_user()\n",
        "            self.drive_service = build('drive', 'v3')\n",
        "            logger.info(\"âœ… Successfully authenticated with Google Drive\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"âŒ Authentication failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def list_all_files(self, force_refresh=False):\n",
        "        \"\"\"\n",
        "        Recursively list all files in the folder and subfolders.\n",
        "\n",
        "        Args:\n",
        "            force_refresh: If True, force a new listing even if cached data exists\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with columns [file_name, file_path, file_id, file_url]\n",
        "        \"\"\"\n",
        "        if self.files_df is not None and not force_refresh:\n",
        "            logger.info(\"ðŸ“‹ Using cached file list\")\n",
        "            return self.files_df\n",
        "\n",
        "        logger.info(\"ðŸ” Listing all files in folder...\")\n",
        "        all_files = self._list_files_recursive(self.folder_id)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        if all_files:\n",
        "            self.files_df = pd.DataFrame(all_files)\n",
        "\n",
        "            # Deduplicate by folder+name (file_path already encodes folder)\n",
        "            self.files_df = self.files_df.drop_duplicates(\n",
        "                subset=[\"file_path\", \"file_name\"], keep=\"first\"\n",
        "            )\n",
        "\n",
        "            logger.info(f\"âœ… Found {len(self.files_df)} unique files\")\n",
        "        else:\n",
        "            self.files_df = pd.DataFrame(columns=['file_name', 'file_path', 'file_id', 'file_url'])\n",
        "            logger.info(\"ðŸ“ No files found in folder\")\n",
        "\n",
        "        return self.files_df\n",
        "\n",
        "    def _list_files_recursive(self, parent_id, parent_path=\"\"):\n",
        "        \"\"\"\n",
        "        Recursively list files in a folder.\n",
        "\n",
        "        Args:\n",
        "            parent_id: Google Drive folder ID\n",
        "            parent_path: Path string for tracking folder hierarchy\n",
        "\n",
        "        Returns:\n",
        "            list: List of file dictionaries\n",
        "        \"\"\"\n",
        "        all_files = []\n",
        "        query = f\"'{parent_id}' in parents and trashed=false\"\n",
        "        page_token = None\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                response = self.drive_service.files().list(\n",
        "                    q=query,\n",
        "                    spaces='drive',\n",
        "                    fields='nextPageToken, files(id, name, mimeType)',\n",
        "                    pageToken=page_token\n",
        "                ).execute()\n",
        "\n",
        "                for item in response.get('files', []):\n",
        "                    item_path = f\"{parent_path}/{item['name']}\" if parent_path else item['name']\n",
        "\n",
        "                    if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "                        # Recurse into subfolder\n",
        "                        all_files.extend(self._list_files_recursive(item['id'], item_path))\n",
        "                    else:\n",
        "                        all_files.append({\n",
        "                            \"file_name\": item['name'],\n",
        "                            \"file_path\": item_path,\n",
        "                            \"file_id\": item['id'],\n",
        "                            \"file_url\": f\"https://drive.google.com/file/d/{item['id']}/view?usp=sharing\"\n",
        "                        })\n",
        "\n",
        "                page_token = response.get('nextPageToken', None)\n",
        "                if page_token is None:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"âŒ Error listing files in {parent_path}: {e}\")\n",
        "                break\n",
        "\n",
        "        return all_files\n",
        "\n",
        "    def filter_files(self, df=None, years=None, chapters=None):\n",
        "        \"\"\"\n",
        "        Filter files based on specified years and chapters using exact matching.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to filter (if None, uses cached files_df)\n",
        "            years: List of years to include (e.g., [2021, 2022, 2023])\n",
        "            chapters: List of chapter numbers to include (e.g., [1, 2, 5, 10])\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Filtered DataFrame containing only requested files\n",
        "        \"\"\"\n",
        "        # Use provided df or cached one\n",
        "        if df is None:\n",
        "            if self.files_df is None:\n",
        "                logger.warning(\"âš ï¸ No files listed yet. Running list_all_files() first.\")\n",
        "                self.list_all_files()\n",
        "            df = self.files_df.copy()\n",
        "        else:\n",
        "            df = df.copy()\n",
        "\n",
        "        if df.empty:\n",
        "            logger.warning(\"âš ï¸ No files to filter\")\n",
        "            return df\n",
        "\n",
        "        # Apply year filter\n",
        "        if years is not None:\n",
        "            year_strings = [str(year) for year in years]\n",
        "            # Exact match: year must be a folder in the path\n",
        "            year_mask = df['file_path'].apply(\n",
        "                lambda path: any(f\"/{year}/\" in f\"/{path}\" or path.startswith(f\"{year}/\")\n",
        "                               for year in year_strings)\n",
        "            )\n",
        "            df = df[year_mask]\n",
        "            logger.info(f\"ðŸ“… Filtered for years: {years} - {len(df)} files\")\n",
        "\n",
        "        # Apply chapter filter\n",
        "        if chapters is not None:\n",
        "            # Exact match for filename pattern: 01.docx, 02.docx, etc.\n",
        "            chapter_filenames = [f\"{ch:02d}.docx\" for ch in chapters]\n",
        "            chapter_mask = df['file_name'].apply(\n",
        "                lambda name: name in chapter_filenames\n",
        "            )\n",
        "            df = df[chapter_mask]\n",
        "            logger.info(f\"ðŸ“– Filtered for chapters: {chapters} - {len(df)} files\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def download_files(self, filtered_df, download_dir=\"/content/reports\"):\n",
        "        \"\"\"\n",
        "        Download files from a filtered DataFrame.\n",
        "\n",
        "        Args:\n",
        "            filtered_df: DataFrame containing files to download\n",
        "            download_dir: Base directory for downloads\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping file paths to local paths\n",
        "        \"\"\"\n",
        "        if filtered_df is None or filtered_df.empty:\n",
        "            logger.warning(\"âš ï¸ No files to download\")\n",
        "            return {}\n",
        "\n",
        "        downloaded_files = {}\n",
        "        total_files = len(filtered_df)\n",
        "\n",
        "        logger.info(f\"ðŸ“¥ Starting download of {total_files} files...\")\n",
        "\n",
        "        for idx, row in filtered_df.iterrows():\n",
        "            file_id = row['file_id']\n",
        "            file_name = row['file_name']\n",
        "            file_path = row['file_path']\n",
        "\n",
        "            # Extract year from path (assuming structure: year/filename)\n",
        "            path_parts = file_path.split('/')\n",
        "            if len(path_parts) >= 2:\n",
        "                year = path_parts[0]\n",
        "                local_path = os.path.join(download_dir, year, file_name)\n",
        "            else:\n",
        "                local_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "            # Ensure directory exists\n",
        "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "            try:\n",
        "                # Download file\n",
        "                request = self.drive_service.files().get_media(fileId=file_id)\n",
        "                fh = io.FileIO(local_path, \"wb\")\n",
        "                downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "                done = False\n",
        "                while not done:\n",
        "                    status, done = downloader.next_chunk()\n",
        "                    if status:\n",
        "                        progress = int(status.progress() * 100)\n",
        "                        print(f\"â¬‡ï¸  Downloading {file_name}: {progress}%\", end='\\r')\n",
        "\n",
        "                logger.info(f\"âœ… Downloaded {file_name} to {local_path}\")\n",
        "                downloaded_files[file_path] = local_path\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"âš ï¸ Failed to download {file_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        logger.info(f\"âœ… Download complete: {len(downloaded_files)}/{total_files} files\")\n",
        "        return downloaded_files\n",
        "\n",
        "    def download_selective(self, years=None, chapters=None, download_dir=\"/content/reports\"):\n",
        "        \"\"\"\n",
        "        Convenience method to list, filter, and download files in one operation.\n",
        "\n",
        "        Args:\n",
        "            years: List of years to download (e.g., [2021, 2022, 2023])\n",
        "            chapters: List of chapter numbers to download (e.g., [1, 2, 5, 10])\n",
        "            download_dir: Base directory for downloads\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping file paths to local paths\n",
        "\n",
        "        Example:\n",
        "            # Download chapters 1-5 for years 2021-2023\n",
        "            manager.download_selective(\n",
        "                years=range(2021, 2024),\n",
        "                chapters=range(1, 6),\n",
        "                download_dir=\"/content/reports\"\n",
        "            )\n",
        "        \"\"\"\n",
        "        # Step 1: List all files\n",
        "        logger.info(\"ðŸš€ Starting selective download workflow...\")\n",
        "        all_files = self.list_all_files()\n",
        "\n",
        "        # Step 2: Filter files\n",
        "        filtered_files = self.filter_files(all_files, years=years, chapters=chapters)\n",
        "\n",
        "        if filtered_files is None or filtered_files.empty:\n",
        "            logger.warning(\"âš ï¸ No files match the specified criteria\")\n",
        "            return {}\n",
        "\n",
        "        logger.info(f\"ðŸ“Š Found {len(filtered_files)} files matching criteria\")\n",
        "\n",
        "        # Step 3: Download filtered files\n",
        "        downloaded = self.download_files(filtered_files, download_dir)\n",
        "\n",
        "        return downloaded\n",
        "\n",
        "    def get_summary(self, df=None):\n",
        "        \"\"\"\n",
        "        Get summary statistics about the files.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to summarize (if None, uses cached files_df)\n",
        "\n",
        "        Returns:\n",
        "            dict: Summary statistics\n",
        "        \"\"\"\n",
        "        if df is None:\n",
        "            if self.files_df is None:\n",
        "                logger.warning(\"âš ï¸ No files listed yet. Running list_all_files() first.\")\n",
        "                self.list_all_files()\n",
        "            df = self.files_df\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            return {\"total_files\": 0, \"years\": [], \"chapters\": []}\n",
        "\n",
        "        # Extract years from paths\n",
        "        years = df['file_path'].apply(lambda x: x.split('/')[0] if '/' in x else None)\n",
        "        years = sorted(years.dropna().unique())\n",
        "\n",
        "        # Extract chapters from filenames (assuming pattern: 01.docx, 02.docx)\n",
        "        chapters = df['file_name'].apply(\n",
        "            lambda x: int(x[:2]) if x[:2].isdigit() and x.endswith('.docx') else None\n",
        "        )\n",
        "        chapters = sorted(chapters.dropna().unique())\n",
        "\n",
        "        summary = {\n",
        "            \"total_files\": len(df),\n",
        "            \"years\": years,\n",
        "            \"year_count\": len(years),\n",
        "            \"chapters\": chapters,\n",
        "            \"chapter_count\": len(chapters),\n",
        "            \"file_types\": df['file_name'].apply(lambda x: x.split('.')[-1]).value_counts().to_dict()\n",
        "        }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def preview_files(self, df=None, n=10):\n",
        "        \"\"\"\n",
        "        Preview first n files from the DataFrame.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to preview (if None, uses cached files_df)\n",
        "            n: Number of files to preview\n",
        "        \"\"\"\n",
        "        if df is None:\n",
        "            if self.files_df is None:\n",
        "                logger.warning(\"âš ï¸ No files listed yet. Running list_all_files() first.\")\n",
        "                self.list_all_files()\n",
        "            df = self.files_df\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            logger.info(\"No files to preview\")\n",
        "            return\n",
        "\n",
        "        preview = df.head(n)[['file_name', 'file_path']]\n",
        "        logger.info(f\"\\nðŸ“‹ Preview of first {min(n, len(df))} files:\")\n",
        "        for idx, row in preview.iterrows():\n",
        "            logger.info(f\"  {row['file_path']}\")\n",
        "\n",
        "    def check_missing_files(self, years, chapters):\n",
        "        \"\"\"\n",
        "        Check which year/chapter combinations are missing.\n",
        "\n",
        "        Args:\n",
        "            years: List of years to check\n",
        "            chapters: List of chapter numbers to check\n",
        "\n",
        "        Returns:\n",
        "            list: List of missing (year, chapter) tuples\n",
        "        \"\"\"\n",
        "        if self.files_df is None:\n",
        "            self.list_all_files()\n",
        "\n",
        "        missing = []\n",
        "\n",
        "        for year in years:\n",
        "            for chapter in chapters:\n",
        "                # Check if this combination exists\n",
        "                filtered = self.filter_files(\n",
        "                    self.files_df,\n",
        "                    years=[year],\n",
        "                    chapters=[chapter]\n",
        "                )\n",
        "\n",
        "                if filtered is None or filtered.empty:\n",
        "                    missing.append((year, chapter))\n",
        "                    logger.warning(f\"âš ï¸ Missing: Year {year}, Chapter {chapter:02d}\")\n",
        "\n",
        "        if missing:\n",
        "            logger.info(f\"ðŸ“Š Total missing files: {len(missing)}\")\n",
        "        else:\n",
        "            logger.info(\"âœ… All requested files are present\")\n",
        "\n",
        "        return missing"
      ],
      "metadata": {
        "id": "Km-JPMKWmgBM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "folder_id = \"1e0eA-AIsz_BSwVHOppJMXECX42hBfG4J\"\n",
        "manager = GoogleDriveManager(folder_id)\n",
        "\n",
        "# Download specific years and chapters\n",
        "downloaded = manager.download_selective(\n",
        "    years=range(2001, 2025),\n",
        "    chapters=[1]\n",
        ")\n",
        "\n",
        "print(f\"Downloaded {len(downloaded)} files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QTrNaXWpI3Q",
        "outputId": "58f2d068-0330-41f9-b06a-cf76cc7202b2",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Successfully authenticated with Google Drive\n",
            "INFO:__main__:ðŸš€ Starting selective download workflow...\n",
            "INFO:__main__:ðŸ” Listing all files in folder...\n",
            "INFO:__main__:âœ… Found 882 unique files\n",
            "INFO:__main__:ðŸ“… Filtered for years: range(2001, 2025) - 882 files\n",
            "INFO:__main__:ðŸ“– Filtered for chapters: [1] - 24 files\n",
            "INFO:__main__:ðŸ“Š Found 24 files matching criteria\n",
            "INFO:__main__:ðŸ“¥ Starting download of 24 files...\n",
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2020/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2015/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2004/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2003/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2018/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2019/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2021/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2021/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2002/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2005/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2010/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2011/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2009/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2017/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2006/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2007/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2008/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2001/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2024/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2022/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2023/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2023/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2012/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:âœ… Downloaded 01.docx to /content/reports/2014/01.docx\n",
            "INFO:__main__:âœ… Download complete: 24/24 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬‡ï¸  Downloading 01.docx: 100%\rDownloaded 24 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data extraction class"
      ],
      "metadata": {
        "id": "OkcPb6r0r461"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "\n",
        "\n",
        "class TableExtractor:\n",
        "    \"\"\"Simple class for extracting tables from Word documents with statistics tracking.\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/reports\", out_dir=\"/content/tables\"):\n",
        "        \"\"\"Initialize the extractor with directories and statistics.\"\"\"\n",
        "        self.base_dir = base_dir\n",
        "        self.out_dir = out_dir\n",
        "\n",
        "        # Configuration constants\n",
        "        self.YEAR_RANGE = (2001, 2025)\n",
        "        self.VALID_EXTENSION = \".docx\"\n",
        "        self.TABLE_MARKER = \"×œ×•×—\"  # Hebrew for \"table\"\n",
        "        self.EXCLUDE_MARKER = \"×ª×¨×©×™×\"  # Hebrew for \"diagram\" - exclude these\n",
        "        self.ENCODING = \"utf-8-sig\"\n",
        "        self.SUMMARY_FILE = \"tables_summary.json\"\n",
        "        self.COLUMNS_FILE = \"tables_columns.json\"\n",
        "\n",
        "        # Statistics tracking\n",
        "        self.total_tables = 0\n",
        "\n",
        "        # Metadata collectors\n",
        "        self.all_summaries = {}\n",
        "        self.all_colnames = {}\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(self.out_dir, exist_ok=True)\n",
        "\n",
        "    def _is_valid_table(self, table):\n",
        "        \"\"\"\n",
        "        Check if a table is valid (contains Hebrew table marker in first row).\n",
        "\n",
        "        Args:\n",
        "            table: A docx table object\n",
        "\n",
        "        Returns:\n",
        "            tuple: (is_valid: bool, table_name: str)\n",
        "        \"\"\"\n",
        "        if len(table.rows) == 0:\n",
        "            return False, \"\"\n",
        "\n",
        "        # Check first row cells for table marker\n",
        "        for cell in table.rows[0].cells:\n",
        "            cell_text = cell.text\n",
        "            if self.TABLE_MARKER in cell_text and self.EXCLUDE_MARKER not in cell_text:\n",
        "                return True, cell_text.strip()\n",
        "\n",
        "        return False, \"\"\n",
        "\n",
        "    def _extract_table_data(self, table):\n",
        "        \"\"\"\n",
        "        Extract data from a docx table and convert to DataFrame.\n",
        "\n",
        "        Args:\n",
        "            table: A docx table object\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Table data as a DataFrame\n",
        "        \"\"\"\n",
        "        data = [[cell.text.strip() for cell in row.cells] for row in table.rows]\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def _save_table_data(self, df, identifier, year, chapter):\n",
        "        \"\"\"\n",
        "        Save DataFrame as CSV file in the appropriate directory structure.\n",
        "\n",
        "        Args:\n",
        "            df: pandas DataFrame to save\n",
        "            identifier: Unique identifier for the table\n",
        "            year: Year of the document\n",
        "            chapter: Chapter identifier\n",
        "\n",
        "        Returns:\n",
        "            str: Path where the file was saved\n",
        "        \"\"\"\n",
        "        save_dir = os.path.join(self.out_dir, str(year), chapter)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        save_path = os.path.join(save_dir, f\"{identifier}.csv\")\n",
        "        df.to_csv(save_path, index=False, encoding=self.ENCODING)\n",
        "\n",
        "        return save_path\n",
        "\n",
        "    def _process_document(self, fpath, year, chapter):\n",
        "        \"\"\"\n",
        "        Process a single Word document and extract all valid tables.\n",
        "\n",
        "        Args:\n",
        "            fpath: Full path to the document\n",
        "            year: Year of the document\n",
        "            chapter: Chapter identifier from filename\n",
        "\n",
        "        Returns:\n",
        "            int: Number of tables extracted from this document\n",
        "        \"\"\"\n",
        "        summary = {}\n",
        "        colnames_map = {}\n",
        "        tables_extracted = 0\n",
        "\n",
        "        try:\n",
        "            doc = Document(fpath)\n",
        "        except Exception as e:\n",
        "            print(f\"skip {fpath}: {e}\")\n",
        "            return 0\n",
        "\n",
        "        serial = 1\n",
        "\n",
        "        for table in doc.tables:\n",
        "            # Validate table\n",
        "            is_valid, table_name = self._is_valid_table(table)\n",
        "            if not is_valid:\n",
        "                continue\n",
        "\n",
        "            # Extract data\n",
        "            df = self._extract_table_data(table)\n",
        "\n",
        "            # Skip empty tables\n",
        "            if len(df) == 0:\n",
        "                continue\n",
        "\n",
        "            # Remove \".docx\" suffix from chapter name\n",
        "            if \".docx\" in chapter:\n",
        "              chapter = chapter.replace('.docx', '')\n",
        "\n",
        "            # Create identifier\n",
        "            identifier = f\"{serial}_{chapter}_{year}\"\n",
        "\n",
        "            # Record mapping for JSON\n",
        "            if len(df) > 0:\n",
        "                # Deduplicate consecutive repeated text in header\n",
        "                header_cells = df.iloc[0].astype(str).tolist()\n",
        "                unique_header = []\n",
        "                for cell in header_cells:\n",
        "                    if not unique_header or cell != unique_header[-1]:\n",
        "                        unique_header.append(cell)\n",
        "                summary[identifier] = \" \".join(unique_header)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            # Combine rows [1] and [2] for column names\n",
        "            if len(df) > 2:\n",
        "                row1 = df.iloc[1].astype(str).tolist()\n",
        "                row2 = df.iloc[2].astype(str).tolist()\n",
        "                colnames_map[identifier] = [f\"{r1} {r2}\".strip() for r1, r2 in zip(row1, row2)]\n",
        "            elif len(df) > 1:\n",
        "                colnames_map[identifier] = df.iloc[1].astype(str).tolist()\n",
        "            else:\n",
        "                colnames_map[identifier] = []\n",
        "\n",
        "            # Save to CSV\n",
        "            self._save_table_data(df, identifier, year, chapter)\n",
        "\n",
        "            tables_extracted += 1\n",
        "            serial += 1\n",
        "\n",
        "        # Update metadata collectors\n",
        "        self.all_summaries.update(summary)\n",
        "        self.all_colnames.update(colnames_map)\n",
        "\n",
        "        return tables_extracted\n",
        "\n",
        "    def _save_metadata(self):\n",
        "        \"\"\"Save summary and column metadata to JSON files.\"\"\"\n",
        "        with open(os.path.join(self.out_dir, self.SUMMARY_FILE), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.all_summaries, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        with open(os.path.join(self.out_dir, self.COLUMNS_FILE), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.all_colnames, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def process_files(self, years=None, chapters=None):\n",
        "        \"\"\"\n",
        "        Process Word documents filtered by years and chapters.\n",
        "\n",
        "        Args:\n",
        "            years: List/range of years to process (None = all years in YEAR_RANGE)\n",
        "            chapters: List of chapter identifiers to process (None = all chapters)\n",
        "\n",
        "        Example:\n",
        "            extractor.process_files()  # Process all files\n",
        "            extractor.process_files(years=[2023, 2024])  # Specific years\n",
        "            extractor.process_files(chapters=['1', '2', '3'])  # Specific chapters\n",
        "            extractor.process_files(years=range(2020, 2025), chapters=['1', '2'])  # Both\n",
        "        \"\"\"\n",
        "        # Reset statistics for new extraction session\n",
        "        self.total_tables = 0\n",
        "        self.tables_per_chapter_year = {}\n",
        "        self.all_summaries = {}\n",
        "        self.all_colnames = {}\n",
        "\n",
        "        # Determine which years to process\n",
        "        if years is None:\n",
        "            years_to_process = range(2001, 2025)\n",
        "        else:\n",
        "            years_to_process = years\n",
        "\n",
        "        # Convert chapters to set for faster lookup (if provided)\n",
        "        chapters_to_process = set(map(str, chapters)) if chapters else None\n",
        "\n",
        "        # Process each year\n",
        "        for year in years_to_process:\n",
        "            print(year)\n",
        "            year_path = os.path.join(self.base_dir, str(year))\n",
        "\n",
        "            if not os.path.isdir(year_path):\n",
        "                continue\n",
        "\n",
        "            # Process each document in the year directory\n",
        "            for fname in os.listdir(year_path):\n",
        "                if not fname.endswith(self.VALID_EXTENSION):\n",
        "                    continue\n",
        "\n",
        "                # Extract chapter from filename\n",
        "                chapter = fname.split(\"_\")[0]\n",
        "\n",
        "                # Skip if not in chapters to process\n",
        "                if chapters_to_process and chapter not in chapters_to_process:\n",
        "                    continue\n",
        "\n",
        "                fpath = os.path.join(year_path, fname)\n",
        "\n",
        "                # Process the document and get table count\n",
        "                tables_in_doc = self._process_document(fpath, year, chapter)\n",
        "\n",
        "\n",
        "                # Update statistics\n",
        "                self.total_tables += tables_in_doc\n",
        "                if chapter not in self.tables_per_chapter_year:\n",
        "                    self.tables_per_chapter_year[chapter] = {}\n",
        "                if year not in self.tables_per_chapter_year[chapter]:\n",
        "                    self.tables_per_chapter_year[chapter][year] = 0\n",
        "                self.tables_per_chapter_year[chapter][year] += tables_in_doc\n",
        "\n",
        "        # Save consolidated metadata\n",
        "        self._save_metadata()\n",
        "\n",
        "        print(f\"\\nExtraction complete! Total tables: {self.total_tables}\")\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"\n",
        "        Get extraction statistics.\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary with 'total' and 'per_chapter_year' statistics\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'total': self.total_tables,\n",
        "            'per_chapter_year': self.tables_per_chapter_year\n",
        "        }\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print a formatted summary of extraction statistics.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"EXTRACTION SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Total tables extracted: {self.total_tables}\")\n",
        "\n",
        "        if self.tables_per_chapter_year:\n",
        "            print(\"\\nTables per chapter per year:\")\n",
        "            for chapter in sorted(self.tables_per_chapter_year.keys()):\n",
        "                print(f\"\\nChapter {chapter}:\")\n",
        "                for year in sorted(self.tables_per_chapter_year[chapter].keys()):\n",
        "                    count = self.tables_per_chapter_year[chapter][year]\n",
        "                    if count > 0:  # Only show years with tables\n",
        "                        print(f\"  {year}: {count}\")\n",
        "        else:\n",
        "            print(\"\\nNo tables extracted.\")\n",
        "        print(\"=\"*50)"
      ],
      "metadata": {
        "id": "ErtYpb9Qr8O5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "extractor = TableExtractor(base_dir=\"/content/reports\", out_dir=\"/content/tables\")\n",
        "\n",
        "# Process everything\n",
        "extractor.process_files()\n",
        "\n",
        "# Get statistics\n",
        "stats = extractor.get_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeHTZd6gtq-X",
        "outputId": "e5b20c09-ce16-469a-c7ca-5eb04fdc2ea4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "\n",
            "Extraction complete! Total tables: 266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print summary\n",
        "extractor.print_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GePB3e9-v4UW",
        "outputId": "e5298840-1b7d-4947-c937-cc78eb4bc68d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "EXTRACTION SUMMARY\n",
            "==================================================\n",
            "Total tables extracted: 266\n",
            "\n",
            "Tables per chapter per year:\n",
            "\n",
            "Chapter 01.docx:\n",
            "  2001: 23\n",
            "  2002: 19\n",
            "  2003: 14\n",
            "  2004: 16\n",
            "  2005: 12\n",
            "  2006: 14\n",
            "  2007: 15\n",
            "  2008: 20\n",
            "  2009: 15\n",
            "  2010: 14\n",
            "  2011: 14\n",
            "  2012: 31\n",
            "  2014: 14\n",
            "  2015: 13\n",
            "  2017: 2\n",
            "  2018: 2\n",
            "  2019: 11\n",
            "  2020: 1\n",
            "  2021: 1\n",
            "  2022: 13\n",
            "  2023: 1\n",
            "  2024: 1\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}