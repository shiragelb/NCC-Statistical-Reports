{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "V-HqtTT_kr3E"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiragelb/NCC-Statistical-Reports/blob/main/Table_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install pandoc\n",
        "!pip install pypandoc\n",
        "!pip install python-docx\n",
        "!pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMvSuUElnUQ7",
        "outputId": "15b77bc1-23c3-4eb0-84a5-3641a62b616a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc-data\n",
            "Suggested packages:\n",
            "  texlive-latex-recommended texlive-xetex texlive-luatex pandoc-citeproc\n",
            "  texlive-latex-extra context wkhtmltopdf librsvg2-bin groff ghc nodejs php\n",
            "  python ruby libjs-mathjax libjs-katex citation-style-language-styles\n",
            "The following NEW packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc\n",
            "  pandoc-data\n",
            "0 upgraded, 4 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 20.6 MB of archives.\n",
            "After this operation, 156 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [115 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm-extensions0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [25.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc-data all 2.9.2.1-3ubuntu2 [81.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc amd64 2.9.2.1-3ubuntu2 [20.3 MB]\n",
            "Fetched 20.6 MB in 1s (14.9 MB/s)\n",
            "Selecting previously unselected package libcmark-gfm0.29.0.gfm.3:amd64.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../libcmark-gfm0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcmark-gfm-extensions0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../libcmark-gfm-extensions0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package pandoc-data.\n",
            "Preparing to unpack .../pandoc-data_2.9.2.1-3ubuntu2_all.deb ...\n",
            "Unpacking pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package pandoc.\n",
            "Preparing to unpack .../pandoc_2.9.2.1-3ubuntu2_amd64.deb ...\n",
            "Unpacking pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Setting up libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Setting up pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.15\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
            "Downloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New data extraction from drive"
      ],
      "metadata": {
        "id": "V-HqtTT_kr3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger('__main__')\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "class GoogleDriveManager:\n",
        "    \"\"\"\n",
        "    Manages Google Drive operations including listing, filtering, downloading, and uploading files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, folder_id):\n",
        "        \"\"\"\n",
        "        Initialize the GoogleDriveManager with authentication and folder ID.\n",
        "\n",
        "        Args:\n",
        "            folder_id: The Google Drive folder ID to work with\n",
        "        \"\"\"\n",
        "        self.folder_id = folder_id\n",
        "        self.drive_service = None\n",
        "        self.files_df = None  # Cache for file listings\n",
        "\n",
        "        # Authenticate and build service\n",
        "        self._authenticate()\n",
        "\n",
        "    def _authenticate(self):\n",
        "        \"\"\"Authenticate with Google Drive and build the service object.\"\"\"\n",
        "        try:\n",
        "            auth.authenticate_user()\n",
        "            self.drive_service = build('drive', 'v3')\n",
        "            logger.info(\"✅ Successfully authenticated with Google Drive\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Authentication failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def list_all_files(self, force_refresh=False):\n",
        "        \"\"\"\n",
        "        Recursively list all files in the folder and subfolders.\n",
        "\n",
        "        Args:\n",
        "            force_refresh: If True, force a new listing even if cached data exists\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with columns [file_name, file_path, file_id, file_url]\n",
        "        \"\"\"\n",
        "        if self.files_df is not None and not force_refresh:\n",
        "            logger.info(\"📋 Using cached file list\")\n",
        "            return self.files_df\n",
        "\n",
        "        logger.info(\"🔍 Listing all files in folder...\")\n",
        "        all_files = self._list_files_recursive(self.folder_id)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        if all_files:\n",
        "            self.files_df = pd.DataFrame(all_files)\n",
        "\n",
        "            # Deduplicate by folder+name (file_path already encodes folder)\n",
        "            self.files_df = self.files_df.drop_duplicates(\n",
        "                subset=[\"file_path\", \"file_name\"], keep=\"first\"\n",
        "            )\n",
        "\n",
        "            logger.info(f\"✅ Found {len(self.files_df)} unique files\")\n",
        "        else:\n",
        "            self.files_df = pd.DataFrame(columns=['file_name', 'file_path', 'file_id', 'file_url'])\n",
        "            logger.info(\"📁 No files found in folder\")\n",
        "\n",
        "        return self.files_df\n",
        "\n",
        "    def _list_files_recursive(self, parent_id, parent_path=\"\"):\n",
        "        \"\"\"\n",
        "        Recursively list files in a folder.\n",
        "\n",
        "        Args:\n",
        "            parent_id: Google Drive folder ID\n",
        "            parent_path: Path string for tracking folder hierarchy\n",
        "\n",
        "        Returns:\n",
        "            list: List of file dictionaries\n",
        "        \"\"\"\n",
        "        all_files = []\n",
        "        query = f\"'{parent_id}' in parents and trashed=false\"\n",
        "        page_token = None\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                response = self.drive_service.files().list(\n",
        "                    q=query,\n",
        "                    spaces='drive',\n",
        "                    fields='nextPageToken, files(id, name, mimeType)',\n",
        "                    pageToken=page_token\n",
        "                ).execute()\n",
        "\n",
        "                for item in response.get('files', []):\n",
        "                    item_path = f\"{parent_path}/{item['name']}\" if parent_path else item['name']\n",
        "\n",
        "                    if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "                        # Recurse into subfolder\n",
        "                        all_files.extend(self._list_files_recursive(item['id'], item_path))\n",
        "                    else:\n",
        "                        all_files.append({\n",
        "                            \"file_name\": item['name'],\n",
        "                            \"file_path\": item_path,\n",
        "                            \"file_id\": item['id'],\n",
        "                            \"file_url\": f\"https://drive.google.com/file/d/{item['id']}/view?usp=sharing\"\n",
        "                        })\n",
        "\n",
        "                page_token = response.get('nextPageToken', None)\n",
        "                if page_token is None:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"❌ Error listing files in {parent_path}: {e}\")\n",
        "                break\n",
        "\n",
        "        return all_files\n",
        "\n",
        "    def filter_files(self, df=None, years=None, chapters=None):\n",
        "        \"\"\"\n",
        "        Filter files based on specified years and chapters using exact matching.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to filter (if None, uses cached files_df)\n",
        "            years: List of years to include (e.g., [2021, 2022, 2023])\n",
        "            chapters: List of chapter numbers to include (e.g., [1, 2, 5, 10])\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Filtered DataFrame containing only requested files\n",
        "        \"\"\"\n",
        "        # Use provided df or cached one\n",
        "        if df is None:\n",
        "            if self.files_df is None:\n",
        "                logger.warning(\"⚠️ No files listed yet. Running list_all_files() first.\")\n",
        "                self.list_all_files()\n",
        "            df = self.files_df.copy()\n",
        "        else:\n",
        "            df = df.copy()\n",
        "\n",
        "        if df.empty:\n",
        "            logger.warning(\"⚠️ No files to filter\")\n",
        "            return df\n",
        "\n",
        "        # Apply year filter\n",
        "        if years is not None:\n",
        "            year_strings = [str(year) for year in years]\n",
        "            # Exact match: year must be a folder in the path\n",
        "            year_mask = df['file_path'].apply(\n",
        "                lambda path: any(f\"/{year}/\" in f\"/{path}\" or path.startswith(f\"{year}/\")\n",
        "                               for year in year_strings)\n",
        "            )\n",
        "            df = df[year_mask]\n",
        "            logger.info(f\"📅 Filtered for years: {years} - {len(df)} files\")\n",
        "\n",
        "        # Apply chapter filter\n",
        "        if chapters is not None:\n",
        "            # Exact match for filename pattern: 01.docx, 02.docx, etc.\n",
        "            chapter_filenames = [f\"{ch:02d}.docx\" for ch in chapters]\n",
        "            chapter_mask = df['file_name'].apply(\n",
        "                lambda name: name in chapter_filenames\n",
        "            )\n",
        "            df = df[chapter_mask]\n",
        "            logger.info(f\"📖 Filtered for chapters: {chapters} - {len(df)} files\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def download_files(self, filtered_df, download_dir=\"/content/reports\"):\n",
        "        \"\"\"\n",
        "        Download files from a filtered DataFrame.\n",
        "\n",
        "        Args:\n",
        "            filtered_df: DataFrame containing files to download\n",
        "            download_dir: Base directory for downloads\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping file paths to local paths\n",
        "        \"\"\"\n",
        "        if filtered_df is None or filtered_df.empty:\n",
        "            logger.warning(\"⚠️ No files to download\")\n",
        "            return {}\n",
        "\n",
        "        downloaded_files = {}\n",
        "        total_files = len(filtered_df)\n",
        "\n",
        "        logger.info(f\"📥 Starting download of {total_files} files...\")\n",
        "\n",
        "        for idx, row in filtered_df.iterrows():\n",
        "            file_id = row['file_id']\n",
        "            file_name = row['file_name']\n",
        "            file_path = row['file_path']\n",
        "\n",
        "            # Extract year from path (assuming structure: year/filename)\n",
        "            path_parts = file_path.split('/')\n",
        "            if len(path_parts) >= 2:\n",
        "                year = path_parts[0]\n",
        "                local_path = os.path.join(download_dir, year, file_name)\n",
        "            else:\n",
        "                local_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "            # Ensure directory exists\n",
        "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "            try:\n",
        "                # Download file\n",
        "                request = self.drive_service.files().get_media(fileId=file_id)\n",
        "                fh = io.FileIO(local_path, \"wb\")\n",
        "                downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "                done = False\n",
        "                while not done:\n",
        "                    status, done = downloader.next_chunk()\n",
        "                    if status:\n",
        "                        progress = int(status.progress() * 100)\n",
        "                        print(f\"⬇️  Downloading {file_name}: {progress}%\", end='\\r')\n",
        "\n",
        "                logger.info(f\"✅ Downloaded {file_name} to {local_path}\")\n",
        "                downloaded_files[file_path] = local_path\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"⚠️ Failed to download {file_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        logger.info(f\"✅ Download complete: {len(downloaded_files)}/{total_files} files\")\n",
        "        return downloaded_files\n",
        "\n",
        "    def download_selective(self, years=None, chapters=None, download_dir=\"/content/reports\"):\n",
        "        \"\"\"\n",
        "        Convenience method to list, filter, and download files in one operation.\n",
        "\n",
        "        Args:\n",
        "            years: List of years to download (e.g., [2021, 2022, 2023])\n",
        "            chapters: List of chapter numbers to download (e.g., [1, 2, 5, 10])\n",
        "            download_dir: Base directory for downloads\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping file paths to local paths\n",
        "\n",
        "        Example:\n",
        "            # Download chapters 1-5 for years 2021-2023\n",
        "            manager.download_selective(\n",
        "                years=range(2021, 2024),\n",
        "                chapters=range(1, 6),\n",
        "                download_dir=\"/content/reports\"\n",
        "            )\n",
        "        \"\"\"\n",
        "        # Step 1: List all files\n",
        "        logger.info(\"🚀 Starting selective download workflow...\")\n",
        "        all_files = self.list_all_files()\n",
        "\n",
        "        # Step 2: Filter files\n",
        "        filtered_files = self.filter_files(all_files, years=years, chapters=chapters)\n",
        "\n",
        "        if filtered_files is None or filtered_files.empty:\n",
        "            logger.warning(\"⚠️ No files match the specified criteria\")\n",
        "            return {}\n",
        "\n",
        "        logger.info(f\"📊 Found {len(filtered_files)} files matching criteria\")\n",
        "\n",
        "        # Step 3: Download filtered files\n",
        "        downloaded = self.download_files(filtered_files, download_dir)\n",
        "\n",
        "        return downloaded\n",
        "\n",
        "    def get_summary(self, df=None):\n",
        "        \"\"\"\n",
        "        Get summary statistics about the files.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to summarize (if None, uses cached files_df)\n",
        "\n",
        "        Returns:\n",
        "            dict: Summary statistics\n",
        "        \"\"\"\n",
        "        if df is None:\n",
        "            if self.files_df is None:\n",
        "                logger.warning(\"⚠️ No files listed yet. Running list_all_files() first.\")\n",
        "                self.list_all_files()\n",
        "            df = self.files_df\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            return {\"total_files\": 0, \"years\": [], \"chapters\": []}\n",
        "\n",
        "        # Extract years from paths\n",
        "        years = df['file_path'].apply(lambda x: x.split('/')[0] if '/' in x else None)\n",
        "        years = sorted(years.dropna().unique())\n",
        "\n",
        "        # Extract chapters from filenames (assuming pattern: 01.docx, 02.docx)\n",
        "        chapters = df['file_name'].apply(\n",
        "            lambda x: int(x[:2]) if x[:2].isdigit() and x.endswith('.docx') else None\n",
        "        )\n",
        "        chapters = sorted(chapters.dropna().unique())\n",
        "\n",
        "        summary = {\n",
        "            \"total_files\": len(df),\n",
        "            \"years\": years,\n",
        "            \"year_count\": len(years),\n",
        "            \"chapters\": chapters,\n",
        "            \"chapter_count\": len(chapters),\n",
        "            \"file_types\": df['file_name'].apply(lambda x: x.split('.')[-1]).value_counts().to_dict()\n",
        "        }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def preview_files(self, df=None, n=10):\n",
        "        \"\"\"\n",
        "        Preview first n files from the DataFrame.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to preview (if None, uses cached files_df)\n",
        "            n: Number of files to preview\n",
        "        \"\"\"\n",
        "        if df is None:\n",
        "            if self.files_df is None:\n",
        "                logger.warning(\"⚠️ No files listed yet. Running list_all_files() first.\")\n",
        "                self.list_all_files()\n",
        "            df = self.files_df\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            logger.info(\"No files to preview\")\n",
        "            return\n",
        "\n",
        "        preview = df.head(n)[['file_name', 'file_path']]\n",
        "        logger.info(f\"\\n📋 Preview of first {min(n, len(df))} files:\")\n",
        "        for idx, row in preview.iterrows():\n",
        "            logger.info(f\"  {row['file_path']}\")\n",
        "\n",
        "    def check_missing_files(self, years, chapters):\n",
        "        \"\"\"\n",
        "        Check which year/chapter combinations are missing.\n",
        "\n",
        "        Args:\n",
        "            years: List of years to check\n",
        "            chapters: List of chapter numbers to check\n",
        "\n",
        "        Returns:\n",
        "            list: List of missing (year, chapter) tuples\n",
        "        \"\"\"\n",
        "        if self.files_df is None:\n",
        "            self.list_all_files()\n",
        "\n",
        "        missing = []\n",
        "\n",
        "        for year in years:\n",
        "            for chapter in chapters:\n",
        "                # Check if this combination exists\n",
        "                filtered = self.filter_files(\n",
        "                    self.files_df,\n",
        "                    years=[year],\n",
        "                    chapters=[chapter]\n",
        "                )\n",
        "\n",
        "                if filtered is None or filtered.empty:\n",
        "                    missing.append((year, chapter))\n",
        "                    logger.warning(f\"⚠️ Missing: Year {year}, Chapter {chapter:02d}\")\n",
        "\n",
        "        if missing:\n",
        "            logger.info(f\"📊 Total missing files: {len(missing)}\")\n",
        "        else:\n",
        "            logger.info(\"✅ All requested files are present\")\n",
        "\n",
        "        return missing"
      ],
      "metadata": {
        "id": "Km-JPMKWmgBM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "folder_id = \"1e0eA-AIsz_BSwVHOppJMXECX42hBfG4J\"\n",
        "manager = GoogleDriveManager(folder_id)\n",
        "\n",
        "# Download specific years and chapters\n",
        "downloaded = manager.download_selective(\n",
        "    years=range(2001, 2025),\n",
        "    chapters=[1]\n",
        ")\n",
        "\n",
        "print(f\"Downloaded {len(downloaded)} files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QTrNaXWpI3Q",
        "outputId": "58f2d068-0330-41f9-b06a-cf76cc7202b2",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Successfully authenticated with Google Drive\n",
            "INFO:__main__:🚀 Starting selective download workflow...\n",
            "INFO:__main__:🔍 Listing all files in folder...\n",
            "INFO:__main__:✅ Found 882 unique files\n",
            "INFO:__main__:📅 Filtered for years: range(2001, 2025) - 882 files\n",
            "INFO:__main__:📖 Filtered for chapters: [1] - 24 files\n",
            "INFO:__main__:📊 Found 24 files matching criteria\n",
            "INFO:__main__:📥 Starting download of 24 files...\n",
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2020/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2015/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2004/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2003/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2018/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2019/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2021/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2021/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2002/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2005/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2010/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2011/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2009/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2017/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2006/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2007/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2008/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2001/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2024/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2022/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2023/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2023/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2012/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:✅ Downloaded 01.docx to /content/reports/2014/01.docx\n",
            "INFO:__main__:✅ Download complete: 24/24 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️  Downloading 01.docx: 100%\rDownloaded 24 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data extraction class"
      ],
      "metadata": {
        "id": "OkcPb6r0r461"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "\n",
        "\n",
        "class TableExtractor:\n",
        "    \"\"\"Simple class for extracting tables from Word documents with statistics tracking.\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/reports\", out_dir=\"/content/tables\"):\n",
        "        \"\"\"Initialize the extractor with directories and statistics.\"\"\"\n",
        "        self.base_dir = base_dir\n",
        "        self.out_dir = out_dir\n",
        "\n",
        "        # Configuration constants\n",
        "        self.YEAR_RANGE = (2001, 2025)\n",
        "        self.VALID_EXTENSION = \".docx\"\n",
        "        self.TABLE_MARKER = \"לוח\"  # Hebrew for \"table\"\n",
        "        self.EXCLUDE_MARKER = \"תרשים\"  # Hebrew for \"diagram\" - exclude these\n",
        "        self.ENCODING = \"utf-8-sig\"\n",
        "        self.SUMMARY_FILE = \"tables_summary.json\"\n",
        "        self.COLUMNS_FILE = \"tables_columns.json\"\n",
        "\n",
        "        # Statistics tracking\n",
        "        self.total_tables = 0\n",
        "\n",
        "        # Metadata collectors\n",
        "        self.all_summaries = {}\n",
        "        self.all_colnames = {}\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(self.out_dir, exist_ok=True)\n",
        "\n",
        "    def _is_valid_table(self, table):\n",
        "        \"\"\"\n",
        "        Check if a table is valid (contains Hebrew table marker in first row).\n",
        "\n",
        "        Args:\n",
        "            table: A docx table object\n",
        "\n",
        "        Returns:\n",
        "            tuple: (is_valid: bool, table_name: str)\n",
        "        \"\"\"\n",
        "        if len(table.rows) == 0:\n",
        "            return False, \"\"\n",
        "\n",
        "        # Check first row cells for table marker\n",
        "        for cell in table.rows[0].cells:\n",
        "            cell_text = cell.text\n",
        "            if self.TABLE_MARKER in cell_text and self.EXCLUDE_MARKER not in cell_text:\n",
        "                return True, cell_text.strip()\n",
        "\n",
        "        return False, \"\"\n",
        "\n",
        "    def _extract_table_data(self, table):\n",
        "        \"\"\"\n",
        "        Extract data from a docx table and convert to DataFrame.\n",
        "\n",
        "        Args:\n",
        "            table: A docx table object\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Table data as a DataFrame\n",
        "        \"\"\"\n",
        "        data = [[cell.text.strip() for cell in row.cells] for row in table.rows]\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def _save_table_data(self, df, identifier, year, chapter):\n",
        "        \"\"\"\n",
        "        Save DataFrame as CSV file in the appropriate directory structure.\n",
        "\n",
        "        Args:\n",
        "            df: pandas DataFrame to save\n",
        "            identifier: Unique identifier for the table\n",
        "            year: Year of the document\n",
        "            chapter: Chapter identifier\n",
        "\n",
        "        Returns:\n",
        "            str: Path where the file was saved\n",
        "        \"\"\"\n",
        "        save_dir = os.path.join(self.out_dir, str(year), chapter)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        save_path = os.path.join(save_dir, f\"{identifier}.csv\")\n",
        "        df.to_csv(save_path, index=False, encoding=self.ENCODING)\n",
        "\n",
        "        return save_path\n",
        "\n",
        "    def _process_document(self, fpath, year, chapter):\n",
        "        \"\"\"\n",
        "        Process a single Word document and extract all valid tables.\n",
        "\n",
        "        Args:\n",
        "            fpath: Full path to the document\n",
        "            year: Year of the document\n",
        "            chapter: Chapter identifier from filename\n",
        "\n",
        "        Returns:\n",
        "            int: Number of tables extracted from this document\n",
        "        \"\"\"\n",
        "        summary = {}\n",
        "        colnames_map = {}\n",
        "        tables_extracted = 0\n",
        "\n",
        "        try:\n",
        "            doc = Document(fpath)\n",
        "        except Exception as e:\n",
        "            print(f\"skip {fpath}: {e}\")\n",
        "            return 0\n",
        "\n",
        "        serial = 1\n",
        "\n",
        "        for table in doc.tables:\n",
        "            # Validate table\n",
        "            is_valid, table_name = self._is_valid_table(table)\n",
        "            if not is_valid:\n",
        "                continue\n",
        "\n",
        "            # Extract data\n",
        "            df = self._extract_table_data(table)\n",
        "\n",
        "            # Skip empty tables\n",
        "            if len(df) == 0:\n",
        "                continue\n",
        "\n",
        "            # Remove \".docx\" suffix from chapter name\n",
        "            if \".docx\" in chapter:\n",
        "              chapter = chapter.replace('.docx', '')\n",
        "\n",
        "            # Create identifier\n",
        "            identifier = f\"{serial}_{chapter}_{year}\"\n",
        "\n",
        "            # Record mapping for JSON\n",
        "            if len(df) > 0:\n",
        "                # Deduplicate consecutive repeated text in header\n",
        "                header_cells = df.iloc[0].astype(str).tolist()\n",
        "                unique_header = []\n",
        "                for cell in header_cells:\n",
        "                    if not unique_header or cell != unique_header[-1]:\n",
        "                        unique_header.append(cell)\n",
        "                summary[identifier] = \" \".join(unique_header)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            # Combine rows [1] and [2] for column names\n",
        "            if len(df) > 2:\n",
        "                row1 = df.iloc[1].astype(str).tolist()\n",
        "                row2 = df.iloc[2].astype(str).tolist()\n",
        "                colnames_map[identifier] = [f\"{r1} {r2}\".strip() for r1, r2 in zip(row1, row2)]\n",
        "            elif len(df) > 1:\n",
        "                colnames_map[identifier] = df.iloc[1].astype(str).tolist()\n",
        "            else:\n",
        "                colnames_map[identifier] = []\n",
        "\n",
        "            # Save to CSV\n",
        "            self._save_table_data(df, identifier, year, chapter)\n",
        "\n",
        "            tables_extracted += 1\n",
        "            serial += 1\n",
        "\n",
        "        # Update metadata collectors\n",
        "        self.all_summaries.update(summary)\n",
        "        self.all_colnames.update(colnames_map)\n",
        "\n",
        "        return tables_extracted\n",
        "\n",
        "    def _save_metadata(self):\n",
        "        \"\"\"Save summary and column metadata to JSON files.\"\"\"\n",
        "        with open(os.path.join(self.out_dir, self.SUMMARY_FILE), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.all_summaries, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        with open(os.path.join(self.out_dir, self.COLUMNS_FILE), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.all_colnames, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def process_files(self, years=None, chapters=None):\n",
        "        \"\"\"\n",
        "        Process Word documents filtered by years and chapters.\n",
        "\n",
        "        Args:\n",
        "            years: List/range of years to process (None = all years in YEAR_RANGE)\n",
        "            chapters: List of chapter identifiers to process (None = all chapters)\n",
        "\n",
        "        Example:\n",
        "            extractor.process_files()  # Process all files\n",
        "            extractor.process_files(years=[2023, 2024])  # Specific years\n",
        "            extractor.process_files(chapters=['1', '2', '3'])  # Specific chapters\n",
        "            extractor.process_files(years=range(2020, 2025), chapters=['1', '2'])  # Both\n",
        "        \"\"\"\n",
        "        # Reset statistics for new extraction session\n",
        "        self.total_tables = 0\n",
        "        self.tables_per_chapter_year = {}\n",
        "        self.all_summaries = {}\n",
        "        self.all_colnames = {}\n",
        "\n",
        "        # Determine which years to process\n",
        "        if years is None:\n",
        "            years_to_process = range(2001, 2025)\n",
        "        else:\n",
        "            years_to_process = years\n",
        "\n",
        "        # Convert chapters to set for faster lookup (if provided)\n",
        "        chapters_to_process = set(map(str, chapters)) if chapters else None\n",
        "\n",
        "        # Process each year\n",
        "        for year in years_to_process:\n",
        "            print(year)\n",
        "            year_path = os.path.join(self.base_dir, str(year))\n",
        "\n",
        "            if not os.path.isdir(year_path):\n",
        "                continue\n",
        "\n",
        "            # Process each document in the year directory\n",
        "            for fname in os.listdir(year_path):\n",
        "                if not fname.endswith(self.VALID_EXTENSION):\n",
        "                    continue\n",
        "\n",
        "                # Extract chapter from filename\n",
        "                chapter = fname.split(\"_\")[0]\n",
        "\n",
        "                # Skip if not in chapters to process\n",
        "                if chapters_to_process and chapter not in chapters_to_process:\n",
        "                    continue\n",
        "\n",
        "                fpath = os.path.join(year_path, fname)\n",
        "\n",
        "                # Process the document and get table count\n",
        "                tables_in_doc = self._process_document(fpath, year, chapter)\n",
        "\n",
        "\n",
        "                # Update statistics\n",
        "                self.total_tables += tables_in_doc\n",
        "                if chapter not in self.tables_per_chapter_year:\n",
        "                    self.tables_per_chapter_year[chapter] = {}\n",
        "                if year not in self.tables_per_chapter_year[chapter]:\n",
        "                    self.tables_per_chapter_year[chapter][year] = 0\n",
        "                self.tables_per_chapter_year[chapter][year] += tables_in_doc\n",
        "\n",
        "        # Save consolidated metadata\n",
        "        self._save_metadata()\n",
        "\n",
        "        print(f\"\\nExtraction complete! Total tables: {self.total_tables}\")\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"\n",
        "        Get extraction statistics.\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary with 'total' and 'per_chapter_year' statistics\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'total': self.total_tables,\n",
        "            'per_chapter_year': self.tables_per_chapter_year\n",
        "        }\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print a formatted summary of extraction statistics.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"EXTRACTION SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Total tables extracted: {self.total_tables}\")\n",
        "\n",
        "        if self.tables_per_chapter_year:\n",
        "            print(\"\\nTables per chapter per year:\")\n",
        "            for chapter in sorted(self.tables_per_chapter_year.keys()):\n",
        "                print(f\"\\nChapter {chapter}:\")\n",
        "                for year in sorted(self.tables_per_chapter_year[chapter].keys()):\n",
        "                    count = self.tables_per_chapter_year[chapter][year]\n",
        "                    if count > 0:  # Only show years with tables\n",
        "                        print(f\"  {year}: {count}\")\n",
        "        else:\n",
        "            print(\"\\nNo tables extracted.\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "    def _identify_continuation_groups(self, summaries):\n",
        "        \"\"\"\n",
        "        Identify groups of tables that should be combined (original + continuations).\n",
        "\n",
        "        Args:\n",
        "            summaries: Dictionary of table summaries\n",
        "\n",
        "        Returns:\n",
        "            dict: Groups of related tables {original_id: [original_id, continuation_ids...]}\n",
        "        \"\"\"\n",
        "        groups = {}\n",
        "        continuation_marker = \"(המשך)\"\n",
        "\n",
        "        # Sort identifiers to process them in order (important for maintaining sequence)\n",
        "        sorted_ids = sorted(summaries.keys(), key=lambda x: (\n",
        "            int(x.split('_')[2]),  # year\n",
        "            x.split('_')[1],        # chapter\n",
        "            int(x.split('_')[0])    # serial number\n",
        "        ))\n",
        "\n",
        "        current_group_original = None\n",
        "        current_base_header = None\n",
        "\n",
        "        for identifier in sorted_ids:\n",
        "            header = summaries[identifier]\n",
        "\n",
        "            # Check if this is a continuation\n",
        "            if continuation_marker in header:\n",
        "                # Extract base header (without continuation marker)\n",
        "                base_header = header.replace(continuation_marker, \"\").strip()\n",
        "\n",
        "                # If we have a current group and the headers are similar enough, add to group\n",
        "                if current_group_original and current_base_header:\n",
        "                    if self._headers_match(base_header, current_base_header):\n",
        "                        groups[current_group_original].append(identifier)\n",
        "                    else:\n",
        "                        print(f\"Warning: Orphaned continuation found: {identifier}\")\n",
        "                        print(f\"  Looking for base: '{base_header}'\")\n",
        "                        print(f\"  Current base: '{current_base_header}'\")\n",
        "            else:\n",
        "                # This is an original table (not a continuation)\n",
        "                # Start a new group\n",
        "                current_group_original = identifier\n",
        "                current_base_header = header  # Store the full header as the base\n",
        "                groups[identifier] = [identifier]  # Group starts with the original\n",
        "\n",
        "        # Filter out groups with only one table (no continuations)\n",
        "        groups_with_continuations = {k: v for k, v in groups.items() if len(v) > 1}\n",
        "\n",
        "        return groups_with_continuations\n",
        "\n",
        "    def _headers_match(self, header1, header2, threshold=0.85):\n",
        "        \"\"\"\n",
        "        Check if two headers are similar enough to be considered the same table.\n",
        "        Allows for small differences in whitespace, punctuation, etc.\n",
        "\n",
        "        Args:\n",
        "            header1: First header string\n",
        "            header2: Second header string\n",
        "            threshold: Similarity threshold (0.0 to 1.0)\n",
        "\n",
        "        Returns:\n",
        "            bool: True if headers match\n",
        "        \"\"\"\n",
        "        # Normalize headers for comparison\n",
        "        def normalize(text):\n",
        "            # Remove extra whitespace, normalize spaces\n",
        "            text = ' '.join(text.split())\n",
        "            # Remove common variations\n",
        "            text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
        "            text = text.replace('  ', ' ')\n",
        "            return text.strip()\n",
        "\n",
        "        h1_normalized = normalize(header1)\n",
        "        h2_normalized = normalize(header2)\n",
        "\n",
        "        # Exact match after normalization\n",
        "        if h1_normalized == h2_normalized:\n",
        "            return True\n",
        "\n",
        "        # Calculate similarity ratio\n",
        "        # Use character-level similarity\n",
        "        longer = max(len(h1_normalized), len(h2_normalized))\n",
        "        if longer == 0:\n",
        "            return True\n",
        "\n",
        "        # Simple character matching approach\n",
        "        matches = 0\n",
        "        shorter_text = h1_normalized if len(h1_normalized) <= len(h2_normalized) else h2_normalized\n",
        "        longer_text = h2_normalized if len(h1_normalized) <= len(h2_normalized) else h1_normalized\n",
        "\n",
        "        # Check if shorter text is substantially contained in longer text\n",
        "        if shorter_text in longer_text:\n",
        "            return True\n",
        "\n",
        "        # Calculate overlap by finding longest common substring ratio\n",
        "        # For Hebrew text with potential small differences\n",
        "        words1 = set(h1_normalized.split())\n",
        "        words2 = set(h2_normalized.split())\n",
        "\n",
        "        if not words1 or not words2:\n",
        "            return False\n",
        "\n",
        "        # Calculate Jaccard similarity of words\n",
        "        intersection = words1.intersection(words2)\n",
        "        union = words1.union(words2)\n",
        "\n",
        "        if not union:\n",
        "            return False\n",
        "\n",
        "        similarity = len(intersection) / len(union)\n",
        "\n",
        "        return similarity >= threshold\n",
        "\n",
        "    def _combine_csv_files(self, identifiers, summaries):\n",
        "        \"\"\"\n",
        "        Load and combine multiple CSV files into one, removing duplicate headers.\n",
        "\n",
        "        Args:\n",
        "            identifiers: List of table identifiers [original, continuation1, ...]\n",
        "            summaries: Dictionary of table summaries for header detection\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Combined dataframe\n",
        "        \"\"\"\n",
        "        if not identifiers:\n",
        "            return None\n",
        "\n",
        "        combined_df = None\n",
        "        original_id = identifiers[0]\n",
        "        original_header = summaries[original_id]\n",
        "\n",
        "        # Parse identifier to get year and chapter\n",
        "        parts = original_id.split('_')\n",
        "        year = parts[2]\n",
        "        chapter = parts[1]\n",
        "\n",
        "        for i, identifier in enumerate(identifiers):\n",
        "            # Build path to CSV file\n",
        "            csv_path = os.path.join(self.out_dir, year, chapter, f\"{identifier}.csv\")\n",
        "\n",
        "            if not os.path.exists(csv_path):\n",
        "                print(f\"Warning: CSV file not found: {csv_path}\")\n",
        "                continue\n",
        "\n",
        "            # Load the CSV\n",
        "            df = pd.read_csv(csv_path, encoding=self.ENCODING)\n",
        "\n",
        "            if i == 0:\n",
        "                # First table (original) - keep everything\n",
        "                combined_df = df\n",
        "            else:\n",
        "                # Continuation table - need to remove duplicate header rows\n",
        "                # Get the header text from the first table to compare\n",
        "                if len(combined_df) > 0:\n",
        "                    # Create a string representation of the first row for comparison\n",
        "                    original_first_row = combined_df.iloc[0].astype(str).tolist()\n",
        "\n",
        "                    # Find and skip duplicate header rows in continuation\n",
        "                    start_idx = 0\n",
        "                    for idx in range(min(3, len(df))):  # Check first 3 rows max\n",
        "                        current_row = df.iloc[idx].astype(str).tolist()\n",
        "                        # Check if this row matches the original header pattern\n",
        "                        if self._is_header_row(current_row, original_first_row):\n",
        "                            start_idx = idx + 1\n",
        "                            break\n",
        "\n",
        "                    # Append from after the header\n",
        "                    if start_idx < len(df):\n",
        "                        combined_df = pd.concat([combined_df, df.iloc[start_idx:]],\n",
        "                                               ignore_index=True)\n",
        "                    else:\n",
        "                        # If all rows were headers, just append everything\n",
        "                        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "                else:\n",
        "                    # Original was empty, just append\n",
        "                    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    def _is_header_row(self, row, original_header_row):\n",
        "        \"\"\"\n",
        "        Check if a row appears to be a duplicate header row.\n",
        "\n",
        "        Args:\n",
        "            row: Row to check (as list of strings)\n",
        "            original_header_row: Original header row to compare against\n",
        "\n",
        "        Returns:\n",
        "            bool: True if this appears to be a duplicate header\n",
        "        \"\"\"\n",
        "        # Check if the rows have significant overlap in content\n",
        "        # This handles cases where headers might have slight variations\n",
        "        if len(row) != len(original_header_row):\n",
        "            return False\n",
        "\n",
        "        # Count matching cells (allowing for continuation marker)\n",
        "        matches = 0\n",
        "        for cell1, cell2 in zip(row, original_header_row):\n",
        "            # Remove continuation marker for comparison\n",
        "            cell1_clean = cell1.replace(\"(המשך)\", \"\").strip()\n",
        "            cell2_clean = cell2.replace(\"(המשך)\", \"\").strip()\n",
        "\n",
        "            if cell1_clean == cell2_clean and cell1_clean:  # Non-empty match\n",
        "                matches += 1\n",
        "\n",
        "        # If most cells match, it's likely a header row\n",
        "        return matches >= len(row) * 0.7  # 70% threshold\n",
        "\n",
        "    def combine_continuation_tables(self):\n",
        "        \"\"\"\n",
        "        Combine continuation tables with their originals after extraction.\n",
        "        This should be called after process_files() to merge any continuation tables.\n",
        "\n",
        "        Returns:\n",
        "            dict: Information about combined tables\n",
        "        \"\"\"\n",
        "        # Load current summaries\n",
        "        summary_path = os.path.join(self.out_dir, self.SUMMARY_FILE)\n",
        "        columns_path = os.path.join(self.out_dir, self.COLUMNS_FILE)\n",
        "\n",
        "        if not os.path.exists(summary_path):\n",
        "            print(\"No summaries file found. Run process_files() first.\")\n",
        "            return {}\n",
        "\n",
        "        # Load metadata\n",
        "        with open(summary_path, 'r', encoding='utf-8') as f:\n",
        "            summaries = json.load(f)\n",
        "\n",
        "        with open(columns_path, 'r', encoding='utf-8') as f:\n",
        "            colnames = json.load(f)\n",
        "\n",
        "        # Identify continuation groups\n",
        "        groups = self._identify_continuation_groups(summaries)\n",
        "\n",
        "        if not groups:\n",
        "            print(\"No continuation tables found.\")\n",
        "            return {}\n",
        "\n",
        "        print(f\"\\nFound {len(groups)} table(s) with continuations to combine...\")\n",
        "\n",
        "        # Track what we combined\n",
        "        combined_info = {}\n",
        "\n",
        "        # Process each group\n",
        "        for original_id, identifier_list in groups.items():\n",
        "            print(f\"\\nCombining {original_id} with {len(identifier_list)-1} continuation(s)...\")\n",
        "\n",
        "            # Combine the CSV files\n",
        "            combined_df = self._combine_csv_files(identifier_list, summaries)\n",
        "\n",
        "            if combined_df is not None:\n",
        "                # Parse identifier to get year and chapter\n",
        "                parts = original_id.split('_')\n",
        "                year = parts[2]\n",
        "                chapter = parts[1]\n",
        "\n",
        "                # Save the combined CSV (overwriting the original)\n",
        "                save_path = os.path.join(self.out_dir, year, chapter, f\"{original_id}.csv\")\n",
        "                combined_df.to_csv(save_path, index=False, encoding=self.ENCODING)\n",
        "\n",
        "                # Delete continuation CSV files\n",
        "                for continuation_id in identifier_list[1:]:  # Skip the original\n",
        "                    continuation_path = os.path.join(self.out_dir, year, chapter, f\"{continuation_id}.csv\")\n",
        "                    if os.path.exists(continuation_path):\n",
        "                        os.remove(continuation_path)\n",
        "                        print(f\"  Removed: {continuation_id}.csv\")\n",
        "\n",
        "                # Track combination info\n",
        "                combined_info[original_id] = {\n",
        "                    'parts_combined': len(identifier_list),\n",
        "                    'continuation_ids': identifier_list[1:],\n",
        "                    'rows_in_combined': len(combined_df)\n",
        "                }\n",
        "\n",
        "                print(f\"  Combined table saved as: {original_id}.csv ({len(combined_df)} rows)\")\n",
        "\n",
        "        # Update metadata files (remove continuation entries)\n",
        "        updated_summaries = {k: v for k, v in summaries.items()\n",
        "                            if \"(המשך)\" not in v}\n",
        "        updated_colnames = {k: v for k, v in colnames.items()\n",
        "                           if \"(המשך)\" not in summaries.get(k, \"\")}\n",
        "\n",
        "        # Save updated metadata\n",
        "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(updated_summaries, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        with open(columns_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(updated_colnames, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        # Save combination tracking info\n",
        "        tracking_path = os.path.join(self.out_dir, \"combined_tables_info.json\")\n",
        "        with open(tracking_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(combined_info, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"\\n✓ Combination complete! Combined {len(groups)} table(s)\")\n",
        "        print(f\"  Combination details saved to: combined_tables_info.json\")\n",
        "\n",
        "        # Update statistics to reflect combinations\n",
        "        self._update_statistics_after_combination(combined_info)\n",
        "\n",
        "        return combined_info\n",
        "\n",
        "    def _update_statistics_after_combination(self, combined_info):\n",
        "        \"\"\"\n",
        "        Update statistics after combining continuation tables.\n",
        "\n",
        "        Args:\n",
        "            combined_info: Dictionary with combination information\n",
        "        \"\"\"\n",
        "        if not combined_info:\n",
        "            return\n",
        "\n",
        "        # Recalculate statistics based on combinations\n",
        "        for original_id, info in combined_info.items():\n",
        "            parts = original_id.split('_')\n",
        "            year = int(parts[2])\n",
        "            chapter = parts[1]\n",
        "\n",
        "            # Reduce count by number of continuations removed\n",
        "            continuations_removed = info['parts_combined'] - 1\n",
        "\n",
        "            # Update total\n",
        "            self.total_tables -= continuations_removed\n",
        "\n",
        "            # Update per-chapter-year statistics\n",
        "            if chapter in self.tables_per_chapter_year:\n",
        "                if year in self.tables_per_chapter_year[chapter]:\n",
        "                    self.tables_per_chapter_year[chapter][year] -= continuations_removed\n",
        "\n",
        "                    # Clean up if count becomes 0\n",
        "                    if self.tables_per_chapter_year[chapter][year] <= 0:\n",
        "                        self.tables_per_chapter_year[chapter][year] = 1  # At least the combined table exists\n",
        "\n",
        "        print(f\"\\n📊 Statistics updated: {self.total_tables} unique tables after combination\")"
      ],
      "metadata": {
        "id": "ErtYpb9Qr8O5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "extractor = TableExtractor(base_dir=\"/content/reports\", out_dir=\"/content/tables\")\n",
        "\n",
        "# Process everything\n",
        "extractor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeHTZd6gtq-X",
        "outputId": "046cb7a1-07aa-4ede-93a8-168db392509a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n",
            "\n",
            "Extraction complete! Total tables: 266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine continuation tables\n",
        "combined = extractor.combine_continuation_tables()\n",
        "\n",
        "# Get statistics\n",
        "stats = extractor.get_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1vmHozAnCNP",
        "outputId": "cfda18d0-47b1-45e8-e132-f4cca184f30e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Orphaned continuation found: 23_01_2001\n",
            "  Looking for base: 'לידות חי \n",
            "לפי יישוב* \n",
            "(מספרים מוחלטים) 2000-1995  לוח 1.13'\n",
            "  Current base: 'לידות חי \n",
            " לפי יישוב* (מספרים)\n",
            "2000-1995 לוח 1.13'\n",
            "Warning: Orphaned continuation found: 18_01_2002\n",
            "  Looking for base: 'לידות חי \n",
            "לפי יישוב* \n",
            "(מספרים מוחלטים) 2001-1995  לוח 1.11'\n",
            "  Current base: 'לידות חי \n",
            " לפי יישוב* (מספרים)\n",
            "2001-1995 לוח 1.11'\n",
            "Warning: Orphaned continuation found: 19_01_2002\n",
            "  Looking for base: 'לידות חי \n",
            "לפי יישוב* \n",
            "(מספרים מוחלטים) 2001-1995  לוח 1.11'\n",
            "  Current base: 'לידות חי \n",
            " לפי יישוב* (מספרים)\n",
            "2001-1995 לוח 1.11'\n",
            "Warning: Orphaned continuation found: 7_01_2004\n",
            "  Looking for base: 'מספר הילדים לפי גיל וחלקם באוכלוסיית היישובים* \n",
            "(אלפים ואחוזים)\n",
            "סוףשנת2003  לוח 1.6'\n",
            "  Current base: 'מספר הילדים לפי גיל וחלקם באוכלוסיית היישובים* \n",
            "(אלפים ואחוזים)\n",
            "סוף שנת 2003 לוח 1.6'\n",
            "Warning: Orphaned continuation found: 9_01_2012\n",
            "  Looking for base: 'מספר הילדים \n",
            "לפי גיל וחלקם באוכלוסיית היישוב* (אלפים ואחוזים)\n",
            "דצמבר 2011  לוח 1.5'\n",
            "  Current base: 'מספר הילדים  להוסיף את דייר אל אסד ב10000+\n",
            "לפי גיל וחלקם באוכלוסיית היישוב* (מספרים ואחוזים)\n",
            "דצמבר 2011 לוח 1.5'\n",
            "Warning: Orphaned continuation found: 10_01_2012\n",
            "  Looking for base: 'מספר הילדים \n",
            "לפי גיל וחלקם באוכלוסיית היישוב* (אלפים ואחוזים)\n",
            "דצמבר 2011  לוח 1.5'\n",
            "  Current base: 'מספר הילדים  להוסיף את דייר אל אסד ב10000+\n",
            "לפי גיל וחלקם באוכלוסיית היישוב* (מספרים ואחוזים)\n",
            "דצמבר 2011 לוח 1.5'\n",
            "Warning: Orphaned continuation found: 16_01_2012\n",
            "  Looking for base: 'לוח\n",
            "2.2 ילדים עולים \n",
            "לפי תקופת העלייה לישראל, ארץ מוצא וגילם היום (מספרים)\n",
            "ינואר 2012'\n",
            "  Current base: 'לוח\n",
            "2.2 ילדים עולים   הופך לתרשים עמודות של סך הכל באחוזים של מי שעלה עד 2001 ואחרי לפי מוצא\n",
            "לפי תקופת העלייה לישראל, ארץ מוצא וגילם היום (מספרים)\n",
            "ינואר 2012'\n",
            "\n",
            "Found 30 table(s) with continuations to combine...\n",
            "\n",
            "Combining 3_01_2001 with 1 continuation(s)...\n",
            "  Removed: 4_01_2001.csv\n",
            "  Combined table saved as: 3_01_2001.csv (74 rows)\n",
            "\n",
            "Combining 7_01_2001 with 4 continuation(s)...\n",
            "  Removed: 8_01_2001.csv\n",
            "  Removed: 9_01_2001.csv\n",
            "  Removed: 10_01_2001.csv\n",
            "  Removed: 11_01_2001.csv\n",
            "  Combined table saved as: 7_01_2001.csv (172 rows)\n",
            "\n",
            "Combining 15_01_2001 with 4 continuation(s)...\n",
            "  Removed: 16_01_2001.csv\n",
            "  Removed: 17_01_2001.csv\n",
            "  Removed: 18_01_2001.csv\n",
            "  Removed: 19_01_2001.csv\n",
            "  Combined table saved as: 15_01_2001.csv (167 rows)\n",
            "\n",
            "Combining 4_01_2002 with 2 continuation(s)...\n",
            "  Removed: 5_01_2002.csv\n",
            "  Removed: 6_01_2002.csv\n",
            "  Combined table saved as: 4_01_2002.csv (83 rows)\n",
            "\n",
            "Combining 8_01_2002 with 4 continuation(s)...\n",
            "  Removed: 9_01_2002.csv\n",
            "  Removed: 10_01_2002.csv\n",
            "  Removed: 11_01_2002.csv\n",
            "  Removed: 12_01_2002.csv\n",
            "  Combined table saved as: 8_01_2002.csv (182 rows)\n",
            "\n",
            "Combining 6_01_2003 with 4 continuation(s)...\n",
            "  Removed: 7_01_2003.csv\n",
            "  Removed: 8_01_2003.csv\n",
            "  Removed: 9_01_2003.csv\n",
            "  Removed: 10_01_2003.csv\n",
            "  Combined table saved as: 6_01_2003.csv (183 rows)\n",
            "\n",
            "Combining 12_01_2003 with 2 continuation(s)...\n",
            "  Removed: 13_01_2003.csv\n",
            "  Removed: 14_01_2003.csv\n",
            "  Combined table saved as: 12_01_2003.csv (69 rows)\n",
            "\n",
            "Combining 6_01_2004 with 3 continuation(s)...\n",
            "  Removed: 8_01_2004.csv\n",
            "  Removed: 9_01_2004.csv\n",
            "  Removed: 10_01_2004.csv\n",
            "  Combined table saved as: 6_01_2004.csv (139 rows)\n",
            "\n",
            "Combining 13_01_2004 with 3 continuation(s)...\n",
            "  Removed: 14_01_2004.csv\n",
            "  Removed: 15_01_2004.csv\n",
            "  Removed: 16_01_2004.csv\n",
            "  Combined table saved as: 13_01_2004.csv (111 rows)\n",
            "\n",
            "Combining 9_01_2005 with 3 continuation(s)...\n",
            "  Removed: 10_01_2005.csv\n",
            "  Removed: 11_01_2005.csv\n",
            "  Removed: 12_01_2005.csv\n",
            "  Combined table saved as: 9_01_2005.csv (111 rows)\n",
            "\n",
            "Combining 6_01_2006 with 1 continuation(s)...\n",
            "  Removed: 7_01_2006.csv\n",
            "  Combined table saved as: 6_01_2006.csv (190 rows)\n",
            "\n",
            "Combining 10_01_2006 with 3 continuation(s)...\n",
            "  Removed: 11_01_2006.csv\n",
            "  Removed: 12_01_2006.csv\n",
            "  Removed: 13_01_2006.csv\n",
            "  Combined table saved as: 10_01_2006.csv (111 rows)\n",
            "\n",
            "Combining 8_01_2007 with 1 continuation(s)...\n",
            "  Removed: 9_01_2007.csv\n",
            "  Combined table saved as: 8_01_2007.csv (193 rows)\n",
            "\n",
            "Combining 12_01_2007 with 2 continuation(s)...\n",
            "  Removed: 13_01_2007.csv\n",
            "  Removed: 14_01_2007.csv\n",
            "  Combined table saved as: 12_01_2007.csv (111 rows)\n",
            "\n",
            "Combining 8_01_2008 with 5 continuation(s)...\n",
            "  Removed: 9_01_2008.csv\n",
            "  Removed: 10_01_2008.csv\n",
            "  Removed: 11_01_2008.csv\n",
            "  Removed: 12_01_2008.csv\n",
            "  Removed: 13_01_2008.csv\n",
            "  Combined table saved as: 8_01_2008.csv (194 rows)\n",
            "\n",
            "Combining 16_01_2008 with 3 continuation(s)...\n",
            "  Removed: 17_01_2008.csv\n",
            "  Removed: 18_01_2008.csv\n",
            "  Removed: 19_01_2008.csv\n",
            "  Combined table saved as: 16_01_2008.csv (125 rows)\n",
            "\n",
            "Combining 8_01_2009 with 3 continuation(s)...\n",
            "  Removed: 9_01_2009.csv\n",
            "  Removed: 10_01_2009.csv\n",
            "  Removed: 11_01_2009.csv\n",
            "  Combined table saved as: 8_01_2009.csv (194 rows)\n",
            "\n",
            "Combining 6_01_2010 with 1 continuation(s)...\n",
            "  Removed: 7_01_2010.csv\n",
            "  Combined table saved as: 6_01_2010.csv (34 rows)\n",
            "\n",
            "Combining 8_01_2010 with 2 continuation(s)...\n",
            "  Removed: 9_01_2010.csv\n",
            "  Removed: 10_01_2010.csv\n",
            "  Combined table saved as: 8_01_2010.csv (195 rows)\n",
            "\n",
            "Combining 6_01_2011 with 1 continuation(s)...\n",
            "  Removed: 7_01_2011.csv\n",
            "  Combined table saved as: 6_01_2011.csv (33 rows)\n",
            "\n",
            "Combining 8_01_2011 with 2 continuation(s)...\n",
            "  Removed: 9_01_2011.csv\n",
            "  Removed: 10_01_2011.csv\n",
            "  Combined table saved as: 8_01_2011.csv (196 rows)\n",
            "\n",
            "Combining 12_01_2011 with 1 continuation(s)...\n",
            "  Removed: 13_01_2011.csv\n",
            "  Combined table saved as: 12_01_2011.csv (129 rows)\n",
            "\n",
            "Combining 12_01_2012 with 1 continuation(s)...\n",
            "  Removed: 13_01_2012.csv\n",
            "  Combined table saved as: 12_01_2012.csv (132 rows)\n",
            "\n",
            "Combining 22_01_2012 with 1 continuation(s)...\n",
            "  Removed: 23_01_2012.csv\n",
            "  Combined table saved as: 22_01_2012.csv (122 rows)\n",
            "\n",
            "Combining 6_01_2014 with 1 continuation(s)...\n",
            "  Removed: 7_01_2014.csv\n",
            "  Combined table saved as: 6_01_2014.csv (32 rows)\n",
            "\n",
            "Combining 8_01_2014 with 3 continuation(s)...\n",
            "  Removed: 9_01_2014.csv\n",
            "  Removed: 10_01_2014.csv\n",
            "  Removed: 11_01_2014.csv\n",
            "  Combined table saved as: 8_01_2014.csv (200 rows)\n",
            "\n",
            "Combining 6_01_2015 with 1 continuation(s)...\n",
            "  Removed: 7_01_2015.csv\n",
            "  Combined table saved as: 6_01_2015.csv (32 rows)\n",
            "\n",
            "Combining 8_01_2015 with 2 continuation(s)...\n",
            "  Removed: 9_01_2015.csv\n",
            "  Removed: 10_01_2015.csv\n",
            "  Combined table saved as: 8_01_2015.csv (200 rows)\n",
            "\n",
            "Combining 6_01_2019 with 1 continuation(s)...\n",
            "  Removed: 7_01_2019.csv\n",
            "  Combined table saved as: 6_01_2019.csv (137 rows)\n",
            "\n",
            "Combining 4_01_2022 with 4 continuation(s)...\n",
            "  Removed: 5_01_2022.csv\n",
            "  Removed: 6_01_2022.csv\n",
            "  Removed: 7_01_2022.csv\n",
            "  Removed: 8_01_2022.csv\n",
            "  Combined table saved as: 4_01_2022.csv (214 rows)\n",
            "\n",
            "✓ Combination complete! Combined 30 table(s)\n",
            "  Combination details saved to: combined_tables_info.json\n",
            "\n",
            "📊 Statistics updated: 197 unique tables after combination\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print summary\n",
        "extractor.print_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GePB3e9-v4UW",
        "outputId": "3a07ba6b-4d5b-41da-d84d-6f53cbc706c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "EXTRACTION SUMMARY\n",
            "==================================================\n",
            "Total tables extracted: 197\n",
            "\n",
            "Tables per chapter per year:\n",
            "\n",
            "Chapter 01.docx:\n",
            "  2001: 23\n",
            "  2002: 19\n",
            "  2003: 14\n",
            "  2004: 16\n",
            "  2005: 12\n",
            "  2006: 14\n",
            "  2007: 15\n",
            "  2008: 20\n",
            "  2009: 15\n",
            "  2010: 14\n",
            "  2011: 14\n",
            "  2012: 31\n",
            "  2014: 14\n",
            "  2015: 13\n",
            "  2017: 2\n",
            "  2018: 2\n",
            "  2019: 11\n",
            "  2020: 1\n",
            "  2021: 1\n",
            "  2022: 13\n",
            "  2023: 1\n",
            "  2024: 1\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}