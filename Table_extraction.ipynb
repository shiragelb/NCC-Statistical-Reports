{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "V-HqtTT_kr3E"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiragelb/NCC-Statistical-Reports/blob/main/Table_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install pandoc\n",
        "!pip install pypandoc\n",
        "!pip install python-docx\n",
        "!pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMvSuUElnUQ7",
        "outputId": "d56d8894-928e-439c-f021-d78506be88d5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc-data\n",
            "Suggested packages:\n",
            "  texlive-latex-recommended texlive-xetex texlive-luatex pandoc-citeproc\n",
            "  texlive-latex-extra context wkhtmltopdf librsvg2-bin groff ghc nodejs php\n",
            "  python ruby libjs-mathjax libjs-katex citation-style-language-styles\n",
            "The following NEW packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc\n",
            "  pandoc-data\n",
            "0 upgraded, 4 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 20.6 MB of archives.\n",
            "After this operation, 156 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [115 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm-extensions0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [25.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc-data all 2.9.2.1-3ubuntu2 [81.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc amd64 2.9.2.1-3ubuntu2 [20.3 MB]\n",
            "Fetched 20.6 MB in 1s (18.7 MB/s)\n",
            "Selecting previously unselected package libcmark-gfm0.29.0.gfm.3:amd64.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../libcmark-gfm0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcmark-gfm-extensions0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../libcmark-gfm-extensions0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package pandoc-data.\n",
            "Preparing to unpack .../pandoc-data_2.9.2.1-3ubuntu2_all.deb ...\n",
            "Unpacking pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package pandoc.\n",
            "Preparing to unpack .../pandoc_2.9.2.1-3ubuntu2_amd64.deb ...\n",
            "Unpacking pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Setting up libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Setting up pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.15\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
            "Downloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data extraction from drive"
      ],
      "metadata": {
        "id": "V-HqtTT_kr3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger('__main__')\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "\n",
        "class GoogleDriveManager:\n",
        "    \"\"\"\n",
        "    Manages Google Drive operations including listing, filtering, downloading, and uploading files.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, folder_id):\n",
        "        \"\"\"\n",
        "        Initialize the GoogleDriveManager with authentication and folder ID.\n",
        "\n",
        "        Args:\n",
        "            folder_id: The Google Drive folder ID to work with\n",
        "        \"\"\"\n",
        "        self.folder_id = folder_id\n",
        "        self.drive_service = None\n",
        "        self.files_df = None  # Cache for file listings\n",
        "\n",
        "        # Authenticate and build service\n",
        "        self._authenticate()\n",
        "\n",
        "    def _authenticate(self):\n",
        "        \"\"\"Authenticate with Google Drive and build the service object.\"\"\"\n",
        "        try:\n",
        "            auth.authenticate_user()\n",
        "            self.drive_service = build('drive', 'v3')\n",
        "            logger.info(\"‚úÖ Successfully authenticated with Google Drive\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Authentication failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def list_all_files(self, force_refresh=False):\n",
        "        \"\"\"\n",
        "        Recursively list all files in the folder and subfolders.\n",
        "\n",
        "        Args:\n",
        "            force_refresh: If True, force a new listing even if cached data exists\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame with columns [file_name, file_path, file_id, file_url]\n",
        "        \"\"\"\n",
        "        if self.files_df is not None and not force_refresh:\n",
        "            logger.info(\"üìã Using cached file list\")\n",
        "            return self.files_df\n",
        "\n",
        "        logger.info(\"üîç Listing all files in folder...\")\n",
        "        all_files = self._list_files_recursive(self.folder_id)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        if all_files:\n",
        "            self.files_df = pd.DataFrame(all_files)\n",
        "\n",
        "            # Deduplicate by folder+name (file_path already encodes folder)\n",
        "            self.files_df = self.files_df.drop_duplicates(\n",
        "                subset=[\"file_path\", \"file_name\"], keep=\"first\"\n",
        "            )\n",
        "\n",
        "            logger.info(f\"‚úÖ Found {len(self.files_df)} unique files\")\n",
        "        else:\n",
        "            self.files_df = pd.DataFrame(columns=['file_name', 'file_path', 'file_id', 'file_url'])\n",
        "            logger.info(\"üìÅ No files found in folder\")\n",
        "\n",
        "        return self.files_df\n",
        "\n",
        "    def _list_files_recursive(self, parent_id, parent_path=\"\"):\n",
        "        \"\"\"\n",
        "        Recursively list files in a folder.\n",
        "\n",
        "        Args:\n",
        "            parent_id: Google Drive folder ID\n",
        "            parent_path: Path string for tracking folder hierarchy\n",
        "\n",
        "        Returns:\n",
        "            list: List of file dictionaries\n",
        "        \"\"\"\n",
        "        all_files = []\n",
        "        query = f\"'{parent_id}' in parents and trashed=false\"\n",
        "        page_token = None\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                response = self.drive_service.files().list(\n",
        "                    q=query,\n",
        "                    spaces='drive',\n",
        "                    fields='nextPageToken, files(id, name, mimeType)',\n",
        "                    pageToken=page_token\n",
        "                ).execute()\n",
        "\n",
        "                for item in response.get('files', []):\n",
        "                    item_path = f\"{parent_path}/{item['name']}\" if parent_path else item['name']\n",
        "\n",
        "                    if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "                        # Recurse into subfolder\n",
        "                        all_files.extend(self._list_files_recursive(item['id'], item_path))\n",
        "                    else:\n",
        "                        all_files.append({\n",
        "                            \"file_name\": item['name'],\n",
        "                            \"file_path\": item_path,\n",
        "                            \"file_id\": item['id'],\n",
        "                            \"file_url\": f\"https://drive.google.com/file/d/{item['id']}/view?usp=sharing\"\n",
        "                        })\n",
        "\n",
        "                page_token = response.get('nextPageToken', None)\n",
        "                if page_token is None:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"‚ùå Error listing files in {parent_path}: {e}\")\n",
        "                break\n",
        "\n",
        "        return all_files\n",
        "\n",
        "    def filter_files(self, df=None, years=None, chapters=None):\n",
        "        \"\"\"\n",
        "        Filter files based on specified years and chapters using exact matching.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to filter (if None, uses cached files_df)\n",
        "            years: List of years to include (e.g., [2021, 2022, 2023])\n",
        "            chapters: List of chapter numbers to include (e.g., [1, 2, 5, 10])\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Filtered DataFrame containing only requested files\n",
        "        \"\"\"\n",
        "        # Use provided df or cached one\n",
        "        if df is None:\n",
        "            if self.files_df is None:\n",
        "                logger.warning(\"‚ö†Ô∏è No files listed yet. Running list_all_files() first.\")\n",
        "                self.list_all_files()\n",
        "            df = self.files_df.copy()\n",
        "        else:\n",
        "            df = df.copy()\n",
        "\n",
        "        if df.empty:\n",
        "            logger.warning(\"‚ö†Ô∏è No files to filter\")\n",
        "            return df\n",
        "\n",
        "        # Apply year filter\n",
        "        if years is not None:\n",
        "            year_strings = [str(year) for year in years]\n",
        "            # Exact match: year must be a folder in the path\n",
        "            year_mask = df['file_path'].apply(\n",
        "                lambda path: any(f\"/{year}/\" in f\"/{path}\" or path.startswith(f\"{year}/\")\n",
        "                               for year in year_strings)\n",
        "            )\n",
        "            df = df[year_mask]\n",
        "            logger.info(f\"üìÖ Filtered for years: {years} - {len(df)} files\")\n",
        "\n",
        "        # Apply chapter filter\n",
        "        if chapters is not None:\n",
        "            # Exact match for filename pattern: 01.docx, 02.docx, etc.\n",
        "            chapter_filenames = [f\"{ch:02d}.docx\" for ch in chapters]\n",
        "            chapter_mask = df['file_name'].apply(\n",
        "                lambda name: name in chapter_filenames\n",
        "            )\n",
        "            df = df[chapter_mask]\n",
        "            logger.info(f\"üìñ Filtered for chapters: {chapters} - {len(df)} files\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def download_files(self, filtered_df, download_dir=\"/content/reports\"):\n",
        "        \"\"\"\n",
        "        Download files from a filtered DataFrame.\n",
        "\n",
        "        Args:\n",
        "            filtered_df: DataFrame containing files to download\n",
        "            download_dir: Base directory for downloads\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping file paths to local paths\n",
        "        \"\"\"\n",
        "        if filtered_df is None or filtered_df.empty:\n",
        "            logger.warning(\"‚ö†Ô∏è No files to download\")\n",
        "            return {}\n",
        "\n",
        "        downloaded_files = {}\n",
        "        total_files = len(filtered_df)\n",
        "\n",
        "        logger.info(f\"üì• Starting download of {total_files} files...\")\n",
        "\n",
        "        for idx, row in filtered_df.iterrows():\n",
        "            file_id = row['file_id']\n",
        "            file_name = row['file_name']\n",
        "            file_path = row['file_path']\n",
        "\n",
        "            # Extract year from path (assuming structure: year/filename)\n",
        "            path_parts = file_path.split('/')\n",
        "            if len(path_parts) >= 2:\n",
        "                year = path_parts[0]\n",
        "                local_path = os.path.join(download_dir, year, file_name)\n",
        "            else:\n",
        "                local_path = os.path.join(download_dir, file_name)\n",
        "\n",
        "            # Ensure directory exists\n",
        "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "            try:\n",
        "                # Download file\n",
        "                request = self.drive_service.files().get_media(fileId=file_id)\n",
        "                fh = io.FileIO(local_path, \"wb\")\n",
        "                downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "                done = False\n",
        "                while not done:\n",
        "                    status, done = downloader.next_chunk()\n",
        "                    if status:\n",
        "                        progress = int(status.progress() * 100)\n",
        "                        print(f\"‚¨áÔ∏è  Downloading {file_name}: {progress}%\", end='\\r')\n",
        "\n",
        "                logger.info(f\"‚úÖ Downloaded {file_name} to {local_path}\")\n",
        "                downloaded_files[file_path] = local_path\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"‚ö†Ô∏è Failed to download {file_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        logger.info(f\"‚úÖ Download complete: {len(downloaded_files)}/{total_files} files\")\n",
        "        return downloaded_files\n",
        "\n",
        "    def download_selective(self, years=None, chapters=None, download_dir=\"/content/reports\"):\n",
        "        \"\"\"\n",
        "        Convenience method to list, filter, and download files in one operation.\n",
        "\n",
        "        Args:\n",
        "            years: List of years to download (e.g., [2021, 2022, 2023])\n",
        "            chapters: List of chapter numbers to download (e.g., [1, 2, 5, 10])\n",
        "            download_dir: Base directory for downloads\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary mapping file paths to local paths\n",
        "\n",
        "        Example:\n",
        "            # Download chapters 1-5 for years 2021-2023\n",
        "            manager.download_selective(\n",
        "                years=range(2021, 2024),\n",
        "                chapters=range(1, 6),\n",
        "                download_dir=\"/content/reports\"\n",
        "            )\n",
        "        \"\"\"\n",
        "        # Step 1: List all files\n",
        "        logger.info(\"üöÄ Starting selective download workflow...\")\n",
        "        all_files = self.list_all_files()\n",
        "\n",
        "        # Step 2: Filter files\n",
        "        filtered_files = self.filter_files(all_files, years=years, chapters=chapters)\n",
        "\n",
        "        if filtered_files is None or filtered_files.empty:\n",
        "            logger.warning(\"‚ö†Ô∏è No files match the specified criteria\")\n",
        "            return {}\n",
        "\n",
        "        logger.info(f\"üìä Found {len(filtered_files)} files matching criteria\")\n",
        "\n",
        "        # Step 3: Download filtered files\n",
        "        downloaded = self.download_files(filtered_files, download_dir)\n",
        "\n",
        "        return downloaded\n",
        "\n",
        "    def get_summary(self, df=None):\n",
        "        \"\"\"\n",
        "        Get summary statistics about the files.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to summarize (if None, uses cached files_df)\n",
        "\n",
        "        Returns:\n",
        "            dict: Summary statistics\n",
        "        \"\"\"\n",
        "        if df is None:\n",
        "            if self.files_df is None:\n",
        "                logger.warning(\"‚ö†Ô∏è No files listed yet. Running list_all_files() first.\")\n",
        "                self.list_all_files()\n",
        "            df = self.files_df\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            return {\"total_files\": 0, \"years\": [], \"chapters\": []}\n",
        "\n",
        "        # Extract years from paths\n",
        "        years = df['file_path'].apply(lambda x: x.split('/')[0] if '/' in x else None)\n",
        "        years = sorted(years.dropna().unique())\n",
        "\n",
        "        # Extract chapters from filenames (assuming pattern: 01.docx, 02.docx)\n",
        "        chapters = df['file_name'].apply(\n",
        "            lambda x: int(x[:2]) if x[:2].isdigit() and x.endswith('.docx') else None\n",
        "        )\n",
        "        chapters = sorted(chapters.dropna().unique())\n",
        "\n",
        "        summary = {\n",
        "            \"total_files\": len(df),\n",
        "            \"years\": years,\n",
        "            \"year_count\": len(years),\n",
        "            \"chapters\": chapters,\n",
        "            \"chapter_count\": len(chapters),\n",
        "            \"file_types\": df['file_name'].apply(lambda x: x.split('.')[-1]).value_counts().to_dict()\n",
        "        }\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def preview_files(self, df=None, n=10):\n",
        "        \"\"\"\n",
        "        Preview first n files from the DataFrame.\n",
        "\n",
        "        Args:\n",
        "            df: DataFrame to preview (if None, uses cached files_df)\n",
        "            n: Number of files to preview\n",
        "        \"\"\"\n",
        "        if df is None:\n",
        "            if self.files_df is None:\n",
        "                logger.warning(\"‚ö†Ô∏è No files listed yet. Running list_all_files() first.\")\n",
        "                self.list_all_files()\n",
        "            df = self.files_df\n",
        "\n",
        "        if df is None or df.empty:\n",
        "            logger.info(\"No files to preview\")\n",
        "            return\n",
        "\n",
        "        preview = df.head(n)[['file_name', 'file_path']]\n",
        "        logger.info(f\"\\nüìã Preview of first {min(n, len(df))} files:\")\n",
        "        for idx, row in preview.iterrows():\n",
        "            logger.info(f\"  {row['file_path']}\")\n",
        "\n",
        "    def check_missing_files(self, years, chapters):\n",
        "        \"\"\"\n",
        "        Check which year/chapter combinations are missing.\n",
        "\n",
        "        Args:\n",
        "            years: List of years to check\n",
        "            chapters: List of chapter numbers to check\n",
        "\n",
        "        Returns:\n",
        "            list: List of missing (year, chapter) tuples\n",
        "        \"\"\"\n",
        "        if self.files_df is None:\n",
        "            self.list_all_files()\n",
        "\n",
        "        missing = []\n",
        "\n",
        "        for year in years:\n",
        "            for chapter in chapters:\n",
        "                # Check if this combination exists\n",
        "                filtered = self.filter_files(\n",
        "                    self.files_df,\n",
        "                    years=[year],\n",
        "                    chapters=[chapter]\n",
        "                )\n",
        "\n",
        "                if filtered is None or filtered.empty:\n",
        "                    missing.append((year, chapter))\n",
        "                    logger.warning(f\"‚ö†Ô∏è Missing: Year {year}, Chapter {chapter:02d}\")\n",
        "\n",
        "        if missing:\n",
        "            logger.info(f\"üìä Total missing files: {len(missing)}\")\n",
        "        else:\n",
        "            logger.info(\"‚úÖ All requested files are present\")\n",
        "\n",
        "        return missing"
      ],
      "metadata": {
        "id": "Km-JPMKWmgBM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "folder_id = \"1e0eA-AIsz_BSwVHOppJMXECX42hBfG4J\"\n",
        "manager = GoogleDriveManager(folder_id)\n",
        "\n",
        "# Download specific years and chapters\n",
        "downloaded = manager.download_selective(\n",
        "    years=range(2001,2025),\n",
        "    chapters=[1, 2]\n",
        ")\n",
        "\n",
        "print(f\"Downloaded {len(downloaded)} files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QTrNaXWpI3Q",
        "outputId": "3dd96514-2353-43bf-e3a5-320e81f4d35d",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Successfully authenticated with Google Drive\n",
            "INFO:__main__:üöÄ Starting selective download workflow...\n",
            "INFO:__main__:üîç Listing all files in folder...\n",
            "INFO:__main__:‚úÖ Found 896 unique files\n",
            "INFO:__main__:üìÖ Filtered for years: range(2001, 2025) - 895 files\n",
            "INFO:__main__:üìñ Filtered for chapters: [1, 2] - 51 files\n",
            "INFO:__main__:üìä Found 51 files matching criteria\n",
            "INFO:__main__:üì• Starting download of 51 files...\n",
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2016/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2020/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2020/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2015/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2015/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2004/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2004/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2003/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2003/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2018/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2018/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2019/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2019/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2021/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2021/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2021/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2021/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2002/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2002/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2005/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2005/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2010/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2010/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2011/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2011/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2009/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2009/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2017/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2017/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2013/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2006/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2006/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2007/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2007/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2008/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2008/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2001/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2001/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2024/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2024/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2024/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2024/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2022/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2022/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2023/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2023/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2023/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2012/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2012/02.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 01.docx to /content/reports/2014/01.docx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 01.docx: 100%\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:‚úÖ Downloaded 02.docx to /content/reports/2014/02.docx\n",
            "INFO:__main__:‚úÖ Download complete: 51/51 files\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è  Downloading 02.docx: 100%\rDownloaded 51 files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data extraction class"
      ],
      "metadata": {
        "id": "OkcPb6r0r461"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "\n",
        "\n",
        "class TableExtractor:\n",
        "    \"\"\"Simple class for extracting tables from Word documents with statistics tracking.\"\"\"\n",
        "\n",
        "    def __init__(self, base_dir=\"/content/reports\", out_dir=\"/content/tables\"):\n",
        "        \"\"\"Initialize the extractor with directories and statistics.\"\"\"\n",
        "        self.base_dir = base_dir\n",
        "        self.out_dir = out_dir\n",
        "\n",
        "        # Configuration constants\n",
        "        self.YEAR_RANGE = (2001, 2025)\n",
        "        self.VALID_EXTENSION = \".docx\"\n",
        "        self.TABLE_MARKER = \"◊ú◊ï◊ó\"  # Hebrew for \"table\"\n",
        "        self.EXCLUDE_MARKER = \"◊™◊®◊©◊ô◊ù\"  # Hebrew for \"diagram\" - exclude these\n",
        "        self.ENCODING = \"utf-8-sig\"\n",
        "        self.SUMMARY_FILE = \"tables_summary.json\"\n",
        "        self.COLUMNS_FILE = \"tables_columns.json\"\n",
        "\n",
        "        # Metadata collectors\n",
        "        self.all_summaries = {}\n",
        "        self.all_colnames = {}\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(self.out_dir, exist_ok=True)\n",
        "\n",
        "    def _is_valid_table(self, table):\n",
        "        \"\"\"\n",
        "        Check if a table is valid (contains Hebrew table marker in first row).\n",
        "\n",
        "        Args:\n",
        "            table: A docx table object\n",
        "\n",
        "        Returns:\n",
        "            tuple: (is_valid: bool, table_name: str)\n",
        "        \"\"\"\n",
        "        if len(table.rows) <= 1:\n",
        "            return False, \"\"\n",
        "\n",
        "        # Check first row cells for table marker\n",
        "        for cell in table.rows[0].cells:\n",
        "            cell_text = cell.text\n",
        "            if self.TABLE_MARKER in cell_text and self.EXCLUDE_MARKER not in cell_text:\n",
        "                return True, cell_text.strip()\n",
        "\n",
        "        return False, \"\"\n",
        "\n",
        "    def _extract_table_data(self, table):\n",
        "        \"\"\"\n",
        "        Extract data from a docx table and convert to DataFrame.\n",
        "\n",
        "        Args:\n",
        "            table: A docx table object\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Table data as a DataFrame\n",
        "        \"\"\"\n",
        "        data = [[cell.text.strip() for cell in row.cells] for row in table.rows]\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def _save_table_data(self, df, identifier, year, chapter):\n",
        "        \"\"\"\n",
        "        Save DataFrame as CSV file in the appropriate directory structure.\n",
        "\n",
        "        Args:\n",
        "            df: pandas DataFrame to save\n",
        "            identifier: Unique identifier for the table\n",
        "            year: Year of the document\n",
        "            chapter: Chapter identifier\n",
        "\n",
        "        Returns:\n",
        "            str: Path where the file was saved\n",
        "        \"\"\"\n",
        "        save_dir = os.path.join(self.out_dir, str(year), chapter)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        save_path = os.path.join(save_dir, f\"{identifier}.csv\")\n",
        "        df.to_csv(save_path, index=False, encoding=self.ENCODING)\n",
        "\n",
        "        return save_path\n",
        "\n",
        "    def _process_document(self, fpath, year, chapter):\n",
        "        \"\"\"\n",
        "        Process a single Word document and extract all valid tables.\n",
        "\n",
        "        Args:\n",
        "            fpath: Full path to the document\n",
        "            year: Year of the document\n",
        "            chapter: Chapter identifier from filename\n",
        "\n",
        "        Returns:\n",
        "            int: Number of tables extracted from this document\n",
        "        \"\"\"\n",
        "        summary = {}\n",
        "        colnames_map = {}\n",
        "        tables_extracted = 0\n",
        "\n",
        "        try:\n",
        "            doc = Document(fpath)\n",
        "        except Exception as e:\n",
        "            print(f\"skip {fpath}: {e}\")\n",
        "            return 0\n",
        "\n",
        "        serial = 1\n",
        "\n",
        "        for table in doc.tables:\n",
        "            # Validate table\n",
        "            is_valid, table_name = self._is_valid_table(table)\n",
        "            if not is_valid:\n",
        "                continue\n",
        "\n",
        "            # Extract data\n",
        "            df = self._extract_table_data(table)\n",
        "\n",
        "            # Skip empty tables\n",
        "            if len(df) == 0:\n",
        "                continue\n",
        "\n",
        "            # Create identifier\n",
        "            chapter = chapter.replace(\".docx\", \"\")\n",
        "            identifier = f\"{serial}_{chapter}_{year}\"\n",
        "\n",
        "            # Record mapping for JSON\n",
        "            if len(df) > 0:\n",
        "                # Deduplicate consecutive repeated text in header\n",
        "                header_cells = df.iloc[0].astype(str).tolist()\n",
        "                unique_header = []\n",
        "                for cell in header_cells:\n",
        "                    if not unique_header or cell != unique_header[-1]:\n",
        "                        unique_header.append(cell)\n",
        "                summary[identifier] = \" \".join(unique_header)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            # Combine rows [1] and [2] for column names\n",
        "            if len(df) > 2:\n",
        "                row1 = df.iloc[1].astype(str).tolist()\n",
        "                row2 = df.iloc[2].astype(str).tolist()\n",
        "                colnames_map[identifier] = [f\"{r1} {r2}\".strip() for r1, r2 in zip(row1, row2)]\n",
        "            elif len(df) > 1:\n",
        "                colnames_map[identifier] = df.iloc[1].astype(str).tolist()\n",
        "            else:\n",
        "                colnames_map[identifier] = []\n",
        "\n",
        "            # Save to CSV\n",
        "            self._save_table_data(df, identifier, year, chapter)\n",
        "\n",
        "            serial += 1\n",
        "\n",
        "        # Update metadata collectors\n",
        "        self.all_summaries.update(summary)\n",
        "        self.all_colnames.update(colnames_map)\n",
        "\n",
        "    def _save_metadata(self):\n",
        "        \"\"\"Save summary and column metadata to JSON files.\"\"\"\n",
        "        with open(os.path.join(self.out_dir, self.SUMMARY_FILE), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.all_summaries, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        with open(os.path.join(self.out_dir, self.COLUMNS_FILE), \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.all_colnames, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    def process_files(self, years=None, chapters=None):\n",
        "        \"\"\"\n",
        "        Process Word documents filtered by years and chapters.\n",
        "\n",
        "        Args:\n",
        "            years: List/range of years to process (None = all years in YEAR_RANGE)\n",
        "            chapters: List of chapter identifiers to process (None = all chapters)\n",
        "\n",
        "        Example:\n",
        "            extractor.process_files()  # Process all files\n",
        "            extractor.process_files(years=[2023, 2024])  # Specific years\n",
        "            extractor.process_files(chapters=['1', '2', '3'])  # Specific chapters\n",
        "            extractor.process_files(years=range(2020, 2025), chapters=['1', '2'])  # Both\n",
        "        \"\"\"\n",
        "        # Reset statistics for new extraction session\n",
        "        self.all_summaries = {}\n",
        "        self.all_colnames = {}\n",
        "\n",
        "        # Determine which years to process\n",
        "        if years is None:\n",
        "            years_to_process = range(*self.YEAR_RANGE)\n",
        "        else:\n",
        "            years_to_process = years\n",
        "\n",
        "        # Convert chapters to set for faster lookup (if provided)\n",
        "        chapters_to_process = set(map(str, chapters)) if chapters else None\n",
        "\n",
        "        # Process each year\n",
        "        for year in years_to_process:\n",
        "            print(year)\n",
        "            year_path = os.path.join(self.base_dir, str(year))\n",
        "\n",
        "            if not os.path.isdir(year_path):\n",
        "                continue\n",
        "\n",
        "            # Process each document in the year directory\n",
        "            for fname in os.listdir(year_path):\n",
        "                if not fname.endswith(self.VALID_EXTENSION):\n",
        "                    continue\n",
        "\n",
        "                # Extract chapter from filename\n",
        "                chapter = fname.split(\"_\")[0]\n",
        "\n",
        "                # Skip if not in chapters to process\n",
        "                if chapters_to_process and chapter not in chapters_to_process:\n",
        "                    continue\n",
        "\n",
        "                fpath = os.path.join(year_path, fname)\n",
        "\n",
        "                # Process the document\n",
        "                self._process_document(fpath, year, chapter)\n",
        "\n",
        "        # Save consolidated metadata\n",
        "        self._save_metadata()\n",
        "\n",
        "    def _identify_continuation_groups(self, summaries):\n",
        "        \"\"\"\n",
        "        Identify groups of tables that should be combined (original + continuations).\n",
        "        Groups are formed by sequential position - any table with \"(◊î◊û◊©◊ö)\" belongs\n",
        "        to the most recent table without \"(◊î◊û◊©◊ö)\".\n",
        "\n",
        "        Args:\n",
        "            summaries: Dictionary of table summaries\n",
        "\n",
        "        Returns:\n",
        "            dict: Groups of related tables {original_id: [original_id, continuation_ids...]}\n",
        "        \"\"\"\n",
        "        groups = {}\n",
        "        continuation_marker = \"(◊î◊û◊©◊ö)\"\n",
        "\n",
        "        # Sort identifiers to process them in order (important for maintaining sequence)\n",
        "        sorted_ids = sorted(summaries.keys(), key=lambda x: (\n",
        "            int(x.split('_')[2]),  # year\n",
        "            x.split('_')[1],        # chapter\n",
        "            int(x.split('_')[0])    # serial number\n",
        "        ))\n",
        "\n",
        "        current_group_original = None\n",
        "\n",
        "        for identifier in sorted_ids:\n",
        "            header = summaries[identifier]\n",
        "\n",
        "            # Check if this is a continuation\n",
        "            if continuation_marker in header:\n",
        "                # This is a continuation - add to current group\n",
        "                if current_group_original:\n",
        "                    groups[current_group_original].append(identifier)\n",
        "                else:\n",
        "                    # This shouldn't happen - continuation without an original\n",
        "                    print(f\"Warning: Continuation table found without a preceding original: {identifier}\")\n",
        "            else:\n",
        "                # This is an original table (not a continuation)\n",
        "                # Start a new group\n",
        "                current_group_original = identifier\n",
        "                groups[identifier] = [identifier]  # Group starts with the original\n",
        "\n",
        "        # Filter out groups with only one table (no continuations)\n",
        "        groups_with_continuations = {k: v for k, v in groups.items() if len(v) > 1}\n",
        "\n",
        "        return groups_with_continuations\n",
        "\n",
        "    def _combine_csv_files(self, identifiers, summaries):\n",
        "        \"\"\"\n",
        "        Load and combine multiple CSV files into one, removing duplicate headers.\n",
        "\n",
        "        Args:\n",
        "            identifiers: List of table identifiers [original, continuation1, ...]\n",
        "            summaries: Dictionary of table summaries (not used in simplified version)\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: Combined dataframe\n",
        "        \"\"\"\n",
        "        if not identifiers:\n",
        "            return None\n",
        "\n",
        "        combined_df = None\n",
        "        original_id = identifiers[0]\n",
        "\n",
        "        # Parse identifier to get year and chapter\n",
        "        parts = original_id.split('_')\n",
        "        year = parts[2]\n",
        "        chapter = parts[1]\n",
        "\n",
        "        for i, identifier in enumerate(identifiers):\n",
        "            # Build path to CSV file\n",
        "            csv_path = os.path.join(self.out_dir, year, chapter, f\"{identifier}.csv\")\n",
        "\n",
        "            if not os.path.exists(csv_path):\n",
        "                print(f\"Warning: CSV file not found: {csv_path}\")\n",
        "                continue\n",
        "\n",
        "            # Load the CSV\n",
        "            df = pd.read_csv(csv_path, encoding=self.ENCODING)\n",
        "\n",
        "            if i == 0:\n",
        "                # First table (original) - keep everything\n",
        "                combined_df = df\n",
        "            else:\n",
        "                # Continuation table - skip first row (the title row)\n",
        "                if len(df) > 1:\n",
        "                    combined_df = pd.concat([combined_df, df.iloc[1:]],\n",
        "                                           ignore_index=True)\n",
        "                else:\n",
        "                    # If continuation only has header, skip it entirely\n",
        "                    print(f\"  Note: Continuation {identifier} has no data rows\")\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    def combine_continuation_tables(self):\n",
        "      \"\"\"\n",
        "      Combine continuation tables with their originals after extraction.\n",
        "      This should be called after process_files() to merge any continuation tables.\n",
        "\n",
        "      Returns:\n",
        "          dict: Information about combined tables\n",
        "      \"\"\"\n",
        "      # Load current summaries\n",
        "      summary_path = os.path.join(self.out_dir, self.SUMMARY_FILE)\n",
        "      columns_path = os.path.join(self.out_dir, self.COLUMNS_FILE)\n",
        "\n",
        "      if not os.path.exists(summary_path):\n",
        "          print(\"No summaries file found. Run process_files() first.\")\n",
        "          return {}\n",
        "\n",
        "      # Load metadata\n",
        "      with open(summary_path, 'r', encoding='utf-8') as f:\n",
        "          summaries = json.load(f)\n",
        "\n",
        "      with open(columns_path, 'r', encoding='utf-8') as f:\n",
        "          colnames = json.load(f)\n",
        "\n",
        "      # Identify continuation groups\n",
        "      groups = self._identify_continuation_groups(summaries)\n",
        "\n",
        "      if not groups:\n",
        "          print(\"No continuation tables found.\")\n",
        "          return {}\n",
        "\n",
        "      print(f\"\\nFound {len(groups)} table(s) with continuations to combine...\")\n",
        "\n",
        "      # Track what we combined\n",
        "      combined_info = {}\n",
        "\n",
        "      # Process each group\n",
        "      for original_id, identifier_list in groups.items():\n",
        "          print(f\"\\nCombining {original_id} with {len(identifier_list)-1} continuation(s)...\")\n",
        "\n",
        "          # Combine the CSV files\n",
        "          combined_df = self._combine_csv_files(identifier_list, summaries)\n",
        "\n",
        "          if combined_df is not None:\n",
        "              # Parse identifier to get year and chapter\n",
        "              parts = original_id.split('_')\n",
        "              year = parts[2]\n",
        "              chapter = parts[1]\n",
        "\n",
        "              # Save the combined CSV (overwriting the original)\n",
        "              save_path = os.path.join(self.out_dir, year, chapter, f\"{original_id}.csv\")\n",
        "              combined_df.to_csv(save_path, index=False, encoding=self.ENCODING)\n",
        "\n",
        "              # Delete continuation CSV files\n",
        "              for continuation_id in identifier_list[1:]:  # Skip the original\n",
        "                  continuation_path = os.path.join(self.out_dir, year, chapter, f\"{continuation_id}.csv\")\n",
        "                  if os.path.exists(continuation_path):\n",
        "                      os.remove(continuation_path)\n",
        "                      print(f\"  Removed: {continuation_id}.csv\")\n",
        "\n",
        "              # Track combination info\n",
        "              combined_info[original_id] = {\n",
        "                  'parts_combined': len(identifier_list),\n",
        "                  'continuation_ids': identifier_list[1:],\n",
        "                  'rows_in_combined': len(combined_df)\n",
        "              }\n",
        "\n",
        "              print(f\"  Combined table saved as: {original_id}.csv ({len(combined_df)} rows)\")\n",
        "\n",
        "      # Remove continuation entries from metadata\n",
        "      summaries_without_continuations = {k: v for k, v in summaries.items()\n",
        "                                        if \"(◊î◊û◊©◊ö)\" not in v}\n",
        "      colnames_without_continuations = {k: v for k, v in colnames.items()\n",
        "                                      if \"(◊î◊û◊©◊ö)\" not in summaries.get(k, \"\")}\n",
        "\n",
        "      # Renumber tables sequentially per chapter-year\n",
        "      print(\"\\nRenumbering tables sequentially...\")\n",
        "\n",
        "      # Group by chapter and year\n",
        "      grouped = {}\n",
        "      for identifier in summaries_without_continuations.keys():\n",
        "          parts = identifier.split('_')\n",
        "          if len(parts) >= 3:\n",
        "              chapter = parts[1]\n",
        "              year = parts[2]\n",
        "              key = f\"{chapter}_{year}\"\n",
        "              if key not in grouped:\n",
        "                  grouped[key] = []\n",
        "              grouped[key].append(identifier)\n",
        "\n",
        "      # Sort each group by original serial number\n",
        "      for key in grouped:\n",
        "          grouped[key].sort(key=lambda x: int(x.split('_')[0]))\n",
        "\n",
        "      # Create new dictionaries with sequential numbering\n",
        "      new_summaries = {}\n",
        "      new_colnames = {}\n",
        "      rename_map = {}  # Track old -> new identifier mapping\n",
        "\n",
        "      for chapter_year, identifiers in grouped.items():\n",
        "          chapter, year = chapter_year.split('_')\n",
        "\n",
        "          for new_serial, old_identifier in enumerate(identifiers, start=1):\n",
        "              new_identifier = f\"{new_serial}_{chapter}_{year}\"\n",
        "              rename_map[old_identifier] = new_identifier\n",
        "\n",
        "              # Copy to new dictionaries with new key\n",
        "              new_summaries[new_identifier] = summaries_without_continuations[old_identifier]\n",
        "              if old_identifier in colnames_without_continuations:\n",
        "                  new_colnames[new_identifier] = colnames_without_continuations[old_identifier]\n",
        "\n",
        "      # Rename CSV files\n",
        "      for old_id, new_id in rename_map.items():\n",
        "          if old_id != new_id:  # Only rename if different\n",
        "              parts_old = old_id.split('_')\n",
        "              parts_new = new_id.split('_')\n",
        "              year = parts_old[2]\n",
        "              chapter = parts_old[1]\n",
        "\n",
        "              old_path = os.path.join(self.out_dir, year, chapter, f\"{old_id}.csv\")\n",
        "              new_path = os.path.join(self.out_dir, year, chapter, f\"{new_id}.csv\")\n",
        "\n",
        "              if os.path.exists(old_path):\n",
        "                  os.rename(old_path, new_path)\n",
        "                  print(f\"  Renamed: {old_id}.csv -> {new_id}.csv\")\n",
        "\n",
        "      # Update combined_info with new identifiers\n",
        "      updated_combined_info = {}\n",
        "      for old_id, info in combined_info.items():\n",
        "          new_id = rename_map.get(old_id, old_id)\n",
        "          updated_combined_info[new_id] = info\n",
        "\n",
        "      # Save updated metadata with sequential numbering\n",
        "      with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "          json.dump(new_summaries, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "      with open(columns_path, 'w', encoding='utf-8') as f:\n",
        "          json.dump(new_colnames, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "      # Save combination tracking info\n",
        "      tracking_path = os.path.join(self.out_dir, \"combined_tables_info.json\")\n",
        "      with open(tracking_path, 'w', encoding='utf-8') as f:\n",
        "          json.dump(updated_combined_info, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "      print(f\"\\n‚úì Combination complete! Combined {len(groups)} table(s)\")\n",
        "      print(f\"  Tables renumbered sequentially per chapter-year\")\n",
        "      print(f\"  Combination details saved to: combined_tables_info.json\")\n",
        "\n",
        "      return updated_combined_info\n",
        "\n",
        "    def calculate_statistics(self):\n",
        "      \"\"\"\n",
        "      Calculate statistics from the table_summary.json file.\n",
        "\n",
        "      Returns:\n",
        "          dict: Statistics with 'total' and 'per_chapter_year' breakdown\n",
        "      \"\"\"\n",
        "      summary_path = os.path.join(self.out_dir, self.SUMMARY_FILE)\n",
        "\n",
        "      if not os.path.exists(summary_path):\n",
        "          return {'total': 0, 'per_chapter_year': {}}\n",
        "\n",
        "      # Load summaries\n",
        "      with open(summary_path, 'r', encoding='utf-8') as f:\n",
        "          summaries = json.load(f)\n",
        "\n",
        "      # Calculate statistics\n",
        "      total = len(summaries)\n",
        "      per_chapter_year = {}\n",
        "\n",
        "      for identifier in summaries.keys():\n",
        "          # Parse identifier: \"serial_chapter_year\"\n",
        "          parts = identifier.split('_')\n",
        "          if len(parts) >= 3:\n",
        "              chapter = parts[1]\n",
        "              year = int(parts[2])\n",
        "\n",
        "              if chapter not in per_chapter_year:\n",
        "                  per_chapter_year[chapter] = {}\n",
        "              if year not in per_chapter_year[chapter]:\n",
        "                  per_chapter_year[chapter][year] = 0\n",
        "              per_chapter_year[chapter][year] += 1\n",
        "\n",
        "      return {\n",
        "          'total': total,\n",
        "          'per_chapter_year': per_chapter_year\n",
        "      }\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print a formatted summary of extraction statistics.\"\"\"\n",
        "        stats = self.calculate_statistics()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"EXTRACTION SUMMARY\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Total tables extracted: {stats['total']}\")\n",
        "\n",
        "        if stats['per_chapter_year']:\n",
        "            print(\"\\nTables per chapter per year:\")\n",
        "            for chapter in sorted(stats['per_chapter_year'].keys()):\n",
        "                print(f\"\\nChapter {chapter}:\")\n",
        "                for year in sorted(stats['per_chapter_year'][chapter].keys()):\n",
        "                    count = stats['per_chapter_year'][chapter][year]\n",
        "                    if count > 0:  # Only show years with tables\n",
        "                        print(f\"  {year}: {count}\")\n",
        "        else:\n",
        "            print(\"\\nNo tables found.\")\n",
        "        print(\"=\"*50)"
      ],
      "metadata": {
        "id": "ErtYpb9Qr8O5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "extractor = TableExtractor(base_dir=\"/content/reports\", out_dir=\"/content/tables\")\n",
        "\n",
        "# Process everything\n",
        "extractor.process_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeHTZd6gtq-X",
        "outputId": "05f44049-64e1-4872-f629-06944ff2d02e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine continuation tables\n",
        "combined = extractor.combine_continuation_tables()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1vmHozAnCNP",
        "outputId": "df843214-d20b-4484-fa3d-f930b51e3fc3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found 50 table(s) with continuations to combine...\n",
            "\n",
            "Combining 3_01_2001 with 1 continuation(s)...\n",
            "  Removed: 4_01_2001.csv\n",
            "  Combined table saved as: 3_01_2001.csv (74 rows)\n",
            "\n",
            "Combining 7_01_2001 with 4 continuation(s)...\n",
            "  Removed: 8_01_2001.csv\n",
            "  Removed: 9_01_2001.csv\n",
            "  Removed: 10_01_2001.csv\n",
            "  Removed: 11_01_2001.csv\n",
            "  Combined table saved as: 7_01_2001.csv (169 rows)\n",
            "\n",
            "Combining 15_01_2001 with 4 continuation(s)...\n",
            "  Removed: 16_01_2001.csv\n",
            "  Removed: 17_01_2001.csv\n",
            "  Removed: 18_01_2001.csv\n",
            "  Removed: 19_01_2001.csv\n",
            "  Combined table saved as: 15_01_2001.csv (167 rows)\n",
            "\n",
            "Combining 22_01_2001 with 1 continuation(s)...\n",
            "  Removed: 23_01_2001.csv\n",
            "  Combined table saved as: 22_01_2001.csv (63 rows)\n",
            "\n",
            "Combining 9_02_2001 with 1 continuation(s)...\n",
            "  Removed: 10_02_2001.csv\n",
            "  Combined table saved as: 9_02_2001.csv (111 rows)\n",
            "\n",
            "Combining 4_01_2002 with 2 continuation(s)...\n",
            "  Removed: 5_01_2002.csv\n",
            "  Removed: 6_01_2002.csv\n",
            "  Combined table saved as: 4_01_2002.csv (83 rows)\n",
            "\n",
            "Combining 8_01_2002 with 4 continuation(s)...\n",
            "  Removed: 9_01_2002.csv\n",
            "  Removed: 10_01_2002.csv\n",
            "  Removed: 11_01_2002.csv\n",
            "  Removed: 12_01_2002.csv\n",
            "  Combined table saved as: 8_01_2002.csv (179 rows)\n",
            "\n",
            "Combining 17_01_2002 with 2 continuation(s)...\n",
            "  Removed: 18_01_2002.csv\n",
            "  Removed: 19_01_2002.csv\n",
            "  Combined table saved as: 17_01_2002.csv (67 rows)\n",
            "\n",
            "Combining 5_02_2002 with 2 continuation(s)...\n",
            "  Removed: 6_02_2002.csv\n",
            "  Removed: 7_02_2002.csv\n",
            "  Combined table saved as: 5_02_2002.csv (114 rows)\n",
            "\n",
            "Combining 6_01_2003 with 4 continuation(s)...\n",
            "  Removed: 7_01_2003.csv\n",
            "  Removed: 8_01_2003.csv\n",
            "  Removed: 9_01_2003.csv\n",
            "  Removed: 10_01_2003.csv\n",
            "  Combined table saved as: 6_01_2003.csv (180 rows)\n",
            "\n",
            "Combining 12_01_2003 with 2 continuation(s)...\n",
            "  Removed: 13_01_2003.csv\n",
            "  Removed: 14_01_2003.csv\n",
            "  Combined table saved as: 12_01_2003.csv (67 rows)\n",
            "\n",
            "Combining 8_02_2003 with 2 continuation(s)...\n",
            "  Removed: 9_02_2003.csv\n",
            "  Removed: 10_02_2003.csv\n",
            "  Combined table saved as: 8_02_2003.csv (115 rows)\n",
            "\n",
            "Combining 6_01_2004 with 4 continuation(s)...\n",
            "  Removed: 7_01_2004.csv\n",
            "  Removed: 8_01_2004.csv\n",
            "  Removed: 9_01_2004.csv\n",
            "  Removed: 10_01_2004.csv\n",
            "  Combined table saved as: 6_01_2004.csv (177 rows)\n",
            "\n",
            "Combining 13_01_2004 with 3 continuation(s)...\n",
            "  Removed: 14_01_2004.csv\n",
            "  Removed: 15_01_2004.csv\n",
            "  Removed: 16_01_2004.csv\n",
            "  Combined table saved as: 13_01_2004.csv (108 rows)\n",
            "\n",
            "Combining 9_01_2005 with 3 continuation(s)...\n",
            "  Removed: 10_01_2005.csv\n",
            "  Removed: 11_01_2005.csv\n",
            "  Removed: 12_01_2005.csv\n",
            "  Combined table saved as: 9_01_2005.csv (108 rows)\n",
            "\n",
            "Combining 6_01_2006 with 1 continuation(s)...\n",
            "  Removed: 7_01_2006.csv\n",
            "  Combined table saved as: 6_01_2006.csv (189 rows)\n",
            "\n",
            "Combining 10_01_2006 with 3 continuation(s)...\n",
            "  Removed: 11_01_2006.csv\n",
            "  Removed: 12_01_2006.csv\n",
            "  Removed: 13_01_2006.csv\n",
            "  Combined table saved as: 10_01_2006.csv (108 rows)\n",
            "\n",
            "Combining 6_01_2007 with 1 continuation(s)...\n",
            "  Removed: 7_01_2007.csv\n",
            "  Combined table saved as: 6_01_2007.csv (192 rows)\n",
            "\n",
            "Combining 10_01_2007 with 2 continuation(s)...\n",
            "  Removed: 11_01_2007.csv\n",
            "  Removed: 12_01_2007.csv\n",
            "  Combined table saved as: 10_01_2007.csv (109 rows)\n",
            "\n",
            "Combining 7_02_2007 with 1 continuation(s)...\n",
            "  Removed: 8_02_2007.csv\n",
            "  Combined table saved as: 7_02_2007.csv (54 rows)\n",
            "\n",
            "Combining 6_01_2008 with 5 continuation(s)...\n",
            "  Removed: 7_01_2008.csv\n",
            "  Removed: 8_01_2008.csv\n",
            "  Removed: 9_01_2008.csv\n",
            "  Removed: 10_01_2008.csv\n",
            "  Removed: 11_01_2008.csv\n",
            "  Combined table saved as: 6_01_2008.csv (189 rows)\n",
            "\n",
            "Combining 14_01_2008 with 3 continuation(s)...\n",
            "  Removed: 15_01_2008.csv\n",
            "  Removed: 16_01_2008.csv\n",
            "  Removed: 17_01_2008.csv\n",
            "  Combined table saved as: 14_01_2008.csv (122 rows)\n",
            "\n",
            "Combining 2_02_2008 with 1 continuation(s)...\n",
            "  Removed: 3_02_2008.csv\n",
            "  Combined table saved as: 2_02_2008.csv (54 rows)\n",
            "\n",
            "Combining 10_02_2008 with 2 continuation(s)...\n",
            "  Removed: 11_02_2008.csv\n",
            "  Removed: 12_02_2008.csv\n",
            "  Combined table saved as: 10_02_2008.csv (122 rows)\n",
            "\n",
            "Combining 6_01_2009 with 3 continuation(s)...\n",
            "  Removed: 7_01_2009.csv\n",
            "  Removed: 8_01_2009.csv\n",
            "  Removed: 9_01_2009.csv\n",
            "  Combined table saved as: 6_01_2009.csv (191 rows)\n",
            "\n",
            "Combining 2_02_2009 with 1 continuation(s)...\n",
            "  Removed: 3_02_2009.csv\n",
            "  Combined table saved as: 2_02_2009.csv (53 rows)\n",
            "\n",
            "Combining 4_01_2010 with 1 continuation(s)...\n",
            "  Removed: 5_01_2010.csv\n",
            "  Combined table saved as: 4_01_2010.csv (33 rows)\n",
            "\n",
            "Combining 6_01_2010 with 2 continuation(s)...\n",
            "  Removed: 7_01_2010.csv\n",
            "  Removed: 8_01_2010.csv\n",
            "  Combined table saved as: 6_01_2010.csv (193 rows)\n",
            "\n",
            "Combining 2_02_2010 with 2 continuation(s)...\n",
            "  Removed: 3_02_2010.csv\n",
            "  Removed: 4_02_2010.csv\n",
            "  Combined table saved as: 2_02_2010.csv (85 rows)\n",
            "\n",
            "Combining 4_01_2011 with 1 continuation(s)...\n",
            "  Removed: 5_01_2011.csv\n",
            "  Combined table saved as: 4_01_2011.csv (33 rows)\n",
            "\n",
            "Combining 6_01_2011 with 2 continuation(s)...\n",
            "  Removed: 7_01_2011.csv\n",
            "  Removed: 8_01_2011.csv\n",
            "  Combined table saved as: 6_01_2011.csv (194 rows)\n",
            "\n",
            "Combining 10_01_2011 with 1 continuation(s)...\n",
            "  Removed: 11_01_2011.csv\n",
            "  Combined table saved as: 10_01_2011.csv (129 rows)\n",
            "\n",
            "Combining 2_02_2011 with 1 continuation(s)...\n",
            "  Removed: 3_02_2011.csv\n",
            "  Combined table saved as: 2_02_2011.csv (53 rows)\n",
            "\n",
            "Combining 9_02_2011 with 1 continuation(s)...\n",
            "  Removed: 10_02_2011.csv\n",
            "  Combined table saved as: 9_02_2011.csv (123 rows)\n",
            "\n",
            "Combining 4_01_2012 with 1 continuation(s)...\n",
            "  Removed: 5_01_2012.csv\n",
            "  Combined table saved as: 4_01_2012.csv (32 rows)\n",
            "\n",
            "Combining 6_01_2012 with 2 continuation(s)...\n",
            "  Removed: 7_01_2012.csv\n",
            "  Removed: 8_01_2012.csv\n",
            "  Combined table saved as: 6_01_2012.csv (87 rows)\n",
            "\n",
            "Combining 10_01_2012 with 1 continuation(s)...\n",
            "  Removed: 11_01_2012.csv\n",
            "  Combined table saved as: 10_01_2012.csv (70 rows)\n",
            "\n",
            "Combining 7_02_2012 with 1 continuation(s)...\n",
            "  Removed: 8_02_2012.csv\n",
            "  Combined table saved as: 7_02_2012.csv (121 rows)\n",
            "\n",
            "Combining 7_02_2013 with 1 continuation(s)...\n",
            "  Removed: 8_02_2013.csv\n",
            "  Combined table saved as: 7_02_2013.csv (122 rows)\n",
            "\n",
            "Combining 4_01_2014 with 1 continuation(s)...\n",
            "  Removed: 5_01_2014.csv\n",
            "  Combined table saved as: 4_01_2014.csv (32 rows)\n",
            "\n",
            "Combining 6_01_2014 with 3 continuation(s)...\n",
            "  Removed: 7_01_2014.csv\n",
            "  Removed: 8_01_2014.csv\n",
            "  Removed: 9_01_2014.csv\n",
            "  Combined table saved as: 6_01_2014.csv (197 rows)\n",
            "\n",
            "Combining 7_02_2014 with 2 continuation(s)...\n",
            "  Removed: 8_02_2014.csv\n",
            "  Removed: 9_02_2014.csv\n",
            "  Combined table saved as: 7_02_2014.csv (122 rows)\n",
            "\n",
            "Combining 4_01_2015 with 1 continuation(s)...\n",
            "  Removed: 5_01_2015.csv\n",
            "  Combined table saved as: 4_01_2015.csv (32 rows)\n",
            "\n",
            "Combining 6_01_2015 with 2 continuation(s)...\n",
            "  Removed: 7_01_2015.csv\n",
            "  Removed: 8_01_2015.csv\n",
            "  Combined table saved as: 6_01_2015.csv (198 rows)\n",
            "\n",
            "Combining 7_02_2015 with 2 continuation(s)...\n",
            "  Removed: 8_02_2015.csv\n",
            "  Removed: 9_02_2015.csv\n",
            "  Combined table saved as: 7_02_2015.csv (131 rows)\n",
            "\n",
            "Combining 6_01_2019 with 1 continuation(s)...\n",
            "  Removed: 7_01_2019.csv\n",
            "  Combined table saved as: 6_01_2019.csv (136 rows)\n",
            "\n",
            "Combining 5_02_2019 with 3 continuation(s)...\n",
            "  Removed: 6_02_2019.csv\n",
            "  Removed: 7_02_2019.csv\n",
            "  Removed: 8_02_2019.csv\n",
            "  Combined table saved as: 5_02_2019.csv (134 rows)\n",
            "\n",
            "Combining 4_01_2022 with 4 continuation(s)...\n",
            "  Removed: 5_01_2022.csv\n",
            "  Removed: 6_01_2022.csv\n",
            "  Removed: 7_01_2022.csv\n",
            "  Removed: 8_01_2022.csv\n",
            "  Combined table saved as: 4_01_2022.csv (210 rows)\n",
            "\n",
            "Combining 3_02_2022 with 3 continuation(s)...\n",
            "  Removed: 4_02_2022.csv\n",
            "  Removed: 5_02_2022.csv\n",
            "  Removed: 6_02_2022.csv\n",
            "  Combined table saved as: 3_02_2022.csv (203 rows)\n",
            "\n",
            "Combining 3_02_2023 with 2 continuation(s)...\n",
            "  Removed: 4_02_2023.csv\n",
            "  Removed: 5_02_2023.csv\n",
            "  Combined table saved as: 3_02_2023.csv (205 rows)\n",
            "\n",
            "Renumbering tables sequentially...\n",
            "  Renamed: 5_01_2001.csv -> 4_01_2001.csv\n",
            "  Renamed: 6_01_2001.csv -> 5_01_2001.csv\n",
            "  Renamed: 7_01_2001.csv -> 6_01_2001.csv\n",
            "  Renamed: 12_01_2001.csv -> 7_01_2001.csv\n",
            "  Renamed: 13_01_2001.csv -> 8_01_2001.csv\n",
            "  Renamed: 14_01_2001.csv -> 9_01_2001.csv\n",
            "  Renamed: 15_01_2001.csv -> 10_01_2001.csv\n",
            "  Renamed: 20_01_2001.csv -> 11_01_2001.csv\n",
            "  Renamed: 21_01_2001.csv -> 12_01_2001.csv\n",
            "  Renamed: 22_01_2001.csv -> 13_01_2001.csv\n",
            "  Renamed: 11_02_2001.csv -> 10_02_2001.csv\n",
            "  Renamed: 7_01_2002.csv -> 5_01_2002.csv\n",
            "  Renamed: 8_01_2002.csv -> 6_01_2002.csv\n",
            "  Renamed: 13_01_2002.csv -> 7_01_2002.csv\n",
            "  Renamed: 14_01_2002.csv -> 8_01_2002.csv\n",
            "  Renamed: 15_01_2002.csv -> 9_01_2002.csv\n",
            "  Renamed: 16_01_2002.csv -> 10_01_2002.csv\n",
            "  Renamed: 17_01_2002.csv -> 11_01_2002.csv\n",
            "  Renamed: 8_02_2002.csv -> 6_02_2002.csv\n",
            "  Renamed: 9_02_2002.csv -> 7_02_2002.csv\n",
            "  Renamed: 10_02_2002.csv -> 8_02_2002.csv\n",
            "  Renamed: 11_02_2002.csv -> 9_02_2002.csv\n",
            "  Renamed: 12_02_2002.csv -> 10_02_2002.csv\n",
            "  Renamed: 13_02_2002.csv -> 11_02_2002.csv\n",
            "  Renamed: 14_02_2002.csv -> 12_02_2002.csv\n",
            "  Renamed: 15_02_2002.csv -> 13_02_2002.csv\n",
            "  Renamed: 11_01_2003.csv -> 7_01_2003.csv\n",
            "  Renamed: 12_01_2003.csv -> 8_01_2003.csv\n",
            "  Renamed: 11_02_2003.csv -> 9_02_2003.csv\n",
            "  Renamed: 12_02_2003.csv -> 10_02_2003.csv\n",
            "  Renamed: 13_02_2003.csv -> 11_02_2003.csv\n",
            "  Renamed: 14_02_2003.csv -> 12_02_2003.csv\n",
            "  Renamed: 15_02_2003.csv -> 13_02_2003.csv\n",
            "  Renamed: 11_01_2004.csv -> 7_01_2004.csv\n",
            "  Renamed: 12_01_2004.csv -> 8_01_2004.csv\n",
            "  Renamed: 13_01_2004.csv -> 9_01_2004.csv\n",
            "  Renamed: 8_01_2006.csv -> 7_01_2006.csv\n",
            "  Renamed: 9_01_2006.csv -> 8_01_2006.csv\n",
            "  Renamed: 10_01_2006.csv -> 9_01_2006.csv\n",
            "  Renamed: 14_01_2006.csv -> 10_01_2006.csv\n",
            "  Renamed: 8_01_2007.csv -> 7_01_2007.csv\n",
            "  Renamed: 9_01_2007.csv -> 8_01_2007.csv\n",
            "  Renamed: 10_01_2007.csv -> 9_01_2007.csv\n",
            "  Renamed: 13_01_2007.csv -> 10_01_2007.csv\n",
            "  Renamed: 9_02_2007.csv -> 8_02_2007.csv\n",
            "  Renamed: 10_02_2007.csv -> 9_02_2007.csv\n",
            "  Renamed: 11_02_2007.csv -> 10_02_2007.csv\n",
            "  Renamed: 12_02_2007.csv -> 11_02_2007.csv\n",
            "  Renamed: 13_02_2007.csv -> 12_02_2007.csv\n",
            "  Renamed: 12_01_2008.csv -> 7_01_2008.csv\n",
            "  Renamed: 13_01_2008.csv -> 8_01_2008.csv\n",
            "  Renamed: 14_01_2008.csv -> 9_01_2008.csv\n",
            "  Renamed: 18_01_2008.csv -> 10_01_2008.csv\n",
            "  Renamed: 4_02_2008.csv -> 3_02_2008.csv\n",
            "  Renamed: 5_02_2008.csv -> 4_02_2008.csv\n",
            "  Renamed: 6_02_2008.csv -> 5_02_2008.csv\n",
            "  Renamed: 7_02_2008.csv -> 6_02_2008.csv\n",
            "  Renamed: 8_02_2008.csv -> 7_02_2008.csv\n",
            "  Renamed: 9_02_2008.csv -> 8_02_2008.csv\n",
            "  Renamed: 10_02_2008.csv -> 9_02_2008.csv\n",
            "  Renamed: 13_02_2008.csv -> 10_02_2008.csv\n",
            "  Renamed: 14_02_2008.csv -> 11_02_2008.csv\n",
            "  Renamed: 15_02_2008.csv -> 12_02_2008.csv\n",
            "  Renamed: 10_01_2009.csv -> 7_01_2009.csv\n",
            "  Renamed: 11_01_2009.csv -> 8_01_2009.csv\n",
            "  Renamed: 12_01_2009.csv -> 9_01_2009.csv\n",
            "  Renamed: 13_01_2009.csv -> 10_01_2009.csv\n",
            "  Renamed: 4_02_2009.csv -> 3_02_2009.csv\n",
            "  Renamed: 5_02_2009.csv -> 4_02_2009.csv\n",
            "  Renamed: 6_02_2009.csv -> 5_02_2009.csv\n",
            "  Renamed: 7_02_2009.csv -> 6_02_2009.csv\n",
            "  Renamed: 8_02_2009.csv -> 7_02_2009.csv\n",
            "  Renamed: 9_02_2009.csv -> 8_02_2009.csv\n",
            "  Renamed: 10_02_2009.csv -> 9_02_2009.csv\n",
            "  Renamed: 11_02_2009.csv -> 10_02_2009.csv\n",
            "  Renamed: 12_02_2009.csv -> 11_02_2009.csv\n",
            "  Renamed: 13_02_2009.csv -> 12_02_2009.csv\n",
            "  Renamed: 14_02_2009.csv -> 13_02_2009.csv\n",
            "  Renamed: 15_02_2009.csv -> 14_02_2009.csv\n",
            "  Renamed: 6_01_2010.csv -> 5_01_2010.csv\n",
            "  Renamed: 9_01_2010.csv -> 6_01_2010.csv\n",
            "  Renamed: 10_01_2010.csv -> 7_01_2010.csv\n",
            "  Renamed: 11_01_2010.csv -> 8_01_2010.csv\n",
            "  Renamed: 12_01_2010.csv -> 9_01_2010.csv\n",
            "  Renamed: 5_02_2010.csv -> 3_02_2010.csv\n",
            "  Renamed: 6_02_2010.csv -> 4_02_2010.csv\n",
            "  Renamed: 7_02_2010.csv -> 5_02_2010.csv\n",
            "  Renamed: 8_02_2010.csv -> 6_02_2010.csv\n",
            "  Renamed: 9_02_2010.csv -> 7_02_2010.csv\n",
            "  Renamed: 10_02_2010.csv -> 8_02_2010.csv\n",
            "  Renamed: 11_02_2010.csv -> 9_02_2010.csv\n",
            "  Renamed: 12_02_2010.csv -> 10_02_2010.csv\n",
            "  Renamed: 13_02_2010.csv -> 11_02_2010.csv\n",
            "  Renamed: 6_01_2011.csv -> 5_01_2011.csv\n",
            "  Renamed: 9_01_2011.csv -> 6_01_2011.csv\n",
            "  Renamed: 10_01_2011.csv -> 7_01_2011.csv\n",
            "  Renamed: 12_01_2011.csv -> 8_01_2011.csv\n",
            "  Renamed: 4_02_2011.csv -> 3_02_2011.csv\n",
            "  Renamed: 5_02_2011.csv -> 4_02_2011.csv\n",
            "  Renamed: 6_02_2011.csv -> 5_02_2011.csv\n",
            "  Renamed: 7_02_2011.csv -> 6_02_2011.csv\n",
            "  Renamed: 8_02_2011.csv -> 7_02_2011.csv\n",
            "  Renamed: 9_02_2011.csv -> 8_02_2011.csv\n",
            "  Renamed: 11_02_2011.csv -> 9_02_2011.csv\n",
            "  Renamed: 12_02_2011.csv -> 10_02_2011.csv\n",
            "  Renamed: 13_02_2011.csv -> 11_02_2011.csv\n",
            "  Renamed: 14_02_2011.csv -> 12_02_2011.csv\n",
            "  Renamed: 15_02_2011.csv -> 13_02_2011.csv\n",
            "  Renamed: 16_02_2011.csv -> 14_02_2011.csv\n",
            "  Renamed: 6_01_2012.csv -> 5_01_2012.csv\n",
            "  Renamed: 9_01_2012.csv -> 6_01_2012.csv\n",
            "  Renamed: 10_01_2012.csv -> 7_01_2012.csv\n",
            "  Renamed: 12_01_2012.csv -> 8_01_2012.csv\n",
            "  Renamed: 9_02_2012.csv -> 8_02_2012.csv\n",
            "  Renamed: 10_02_2012.csv -> 9_02_2012.csv\n",
            "  Renamed: 11_02_2012.csv -> 10_02_2012.csv\n",
            "  Renamed: 12_02_2012.csv -> 11_02_2012.csv\n",
            "  Renamed: 13_02_2012.csv -> 12_02_2012.csv\n",
            "  Renamed: 14_02_2012.csv -> 13_02_2012.csv\n",
            "  Renamed: 15_02_2012.csv -> 14_02_2012.csv\n",
            "  Renamed: 9_02_2013.csv -> 8_02_2013.csv\n",
            "  Renamed: 10_02_2013.csv -> 9_02_2013.csv\n",
            "  Renamed: 11_02_2013.csv -> 10_02_2013.csv\n",
            "  Renamed: 12_02_2013.csv -> 11_02_2013.csv\n",
            "  Renamed: 13_02_2013.csv -> 12_02_2013.csv\n",
            "  Renamed: 14_02_2013.csv -> 13_02_2013.csv\n",
            "  Renamed: 15_02_2013.csv -> 14_02_2013.csv\n",
            "  Renamed: 6_01_2014.csv -> 5_01_2014.csv\n",
            "  Renamed: 10_01_2014.csv -> 6_01_2014.csv\n",
            "  Renamed: 11_01_2014.csv -> 7_01_2014.csv\n",
            "  Renamed: 12_01_2014.csv -> 8_01_2014.csv\n",
            "  Renamed: 10_02_2014.csv -> 8_02_2014.csv\n",
            "  Renamed: 11_02_2014.csv -> 9_02_2014.csv\n",
            "  Renamed: 12_02_2014.csv -> 10_02_2014.csv\n",
            "  Renamed: 13_02_2014.csv -> 11_02_2014.csv\n",
            "  Renamed: 14_02_2014.csv -> 12_02_2014.csv\n",
            "  Renamed: 15_02_2014.csv -> 13_02_2014.csv\n",
            "  Renamed: 16_02_2014.csv -> 14_02_2014.csv\n",
            "  Renamed: 17_02_2014.csv -> 15_02_2014.csv\n",
            "  Renamed: 6_01_2015.csv -> 5_01_2015.csv\n",
            "  Renamed: 9_01_2015.csv -> 6_01_2015.csv\n",
            "  Renamed: 10_01_2015.csv -> 7_01_2015.csv\n",
            "  Renamed: 11_01_2015.csv -> 8_01_2015.csv\n",
            "  Renamed: 10_02_2015.csv -> 8_02_2015.csv\n",
            "  Renamed: 11_02_2015.csv -> 9_02_2015.csv\n",
            "  Renamed: 12_02_2015.csv -> 10_02_2015.csv\n",
            "  Renamed: 13_02_2015.csv -> 11_02_2015.csv\n",
            "  Renamed: 14_02_2015.csv -> 12_02_2015.csv\n",
            "  Renamed: 15_02_2015.csv -> 13_02_2015.csv\n",
            "  Renamed: 16_02_2015.csv -> 14_02_2015.csv\n",
            "  Renamed: 8_01_2019.csv -> 7_01_2019.csv\n",
            "  Renamed: 9_01_2019.csv -> 8_01_2019.csv\n",
            "  Renamed: 10_01_2019.csv -> 9_01_2019.csv\n",
            "  Renamed: 11_01_2019.csv -> 10_01_2019.csv\n",
            "  Renamed: 9_02_2019.csv -> 6_02_2019.csv\n",
            "  Renamed: 10_02_2019.csv -> 7_02_2019.csv\n",
            "  Renamed: 11_02_2019.csv -> 8_02_2019.csv\n",
            "  Renamed: 12_02_2019.csv -> 9_02_2019.csv\n",
            "  Renamed: 13_02_2019.csv -> 10_02_2019.csv\n",
            "  Renamed: 9_01_2022.csv -> 5_01_2022.csv\n",
            "  Renamed: 10_01_2022.csv -> 6_01_2022.csv\n",
            "  Renamed: 11_01_2022.csv -> 7_01_2022.csv\n",
            "  Renamed: 12_01_2022.csv -> 8_01_2022.csv\n",
            "  Renamed: 13_01_2022.csv -> 9_01_2022.csv\n",
            "  Renamed: 7_02_2022.csv -> 4_02_2022.csv\n",
            "  Renamed: 8_02_2022.csv -> 5_02_2022.csv\n",
            "  Renamed: 6_02_2023.csv -> 4_02_2023.csv\n",
            "  Renamed: 7_02_2023.csv -> 5_02_2023.csv\n",
            "\n",
            "‚úì Combination complete! Combined 50 table(s)\n",
            "  Tables renumbered sequentially per chapter-year\n",
            "  Combination details saved to: combined_tables_info.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print summary\n",
        "extractor.print_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GePB3e9-v4UW",
        "outputId": "6793e064-c484-474e-ad49-d452b070778c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "EXTRACTION SUMMARY\n",
            "==================================================\n",
            "Total tables extracted: 402\n",
            "\n",
            "Tables per chapter per year:\n",
            "\n",
            "Chapter 01:\n",
            "  2001: 13\n",
            "  2002: 11\n",
            "  2003: 8\n",
            "  2004: 9\n",
            "  2005: 9\n",
            "  2006: 10\n",
            "  2007: 10\n",
            "  2008: 10\n",
            "  2009: 10\n",
            "  2010: 9\n",
            "  2011: 8\n",
            "  2012: 8\n",
            "  2014: 8\n",
            "  2015: 8\n",
            "  2017: 2\n",
            "  2018: 2\n",
            "  2019: 10\n",
            "  2020: 1\n",
            "  2021: 1\n",
            "  2022: 9\n",
            "  2023: 1\n",
            "  2024: 9\n",
            "\n",
            "Chapter 02:\n",
            "  2001: 10\n",
            "  2002: 13\n",
            "  2003: 13\n",
            "  2004: 13\n",
            "  2005: 13\n",
            "  2006: 12\n",
            "  2007: 12\n",
            "  2008: 12\n",
            "  2009: 14\n",
            "  2010: 11\n",
            "  2011: 14\n",
            "  2012: 14\n",
            "  2013: 14\n",
            "  2014: 15\n",
            "  2015: 14\n",
            "  2016: 14\n",
            "  2017: 1\n",
            "  2018: 1\n",
            "  2019: 10\n",
            "  2022: 5\n",
            "  2023: 5\n",
            "  2024: 6\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLZt8BtV0jrz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}