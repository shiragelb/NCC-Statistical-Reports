{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiragelb/NCC-Statistical-Reports/blob/main/Table_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install pandoc\n",
        "!pip install pypandoc\n",
        "!pip install python-docx\n",
        "!pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMvSuUElnUQ7",
        "outputId": "791996cb-1f21-4ffc-a3ad-1a15714b25e5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc-data\n",
            "Suggested packages:\n",
            "  texlive-latex-recommended texlive-xetex texlive-luatex pandoc-citeproc\n",
            "  texlive-latex-extra context wkhtmltopdf librsvg2-bin groff ghc nodejs php\n",
            "  python ruby libjs-mathjax libjs-katex citation-style-language-styles\n",
            "The following NEW packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc\n",
            "  pandoc-data\n",
            "0 upgraded, 4 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 20.6 MB of archives.\n",
            "After this operation, 156 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [115 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm-extensions0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [25.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc-data all 2.9.2.1-3ubuntu2 [81.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc amd64 2.9.2.1-3ubuntu2 [20.3 MB]\n",
            "Fetched 20.6 MB in 3s (6,501 kB/s)\n",
            "Selecting previously unselected package libcmark-gfm0.29.0.gfm.3:amd64.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../libcmark-gfm0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcmark-gfm-extensions0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../libcmark-gfm-extensions0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package pandoc-data.\n",
            "Preparing to unpack .../pandoc-data_2.9.2.1-3ubuntu2_all.deb ...\n",
            "Unpacking pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package pandoc.\n",
            "Preparing to unpack .../pandoc_2.9.2.1-3ubuntu2_amd64.deb ...\n",
            "Unpacking pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Setting up libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Setting up pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.15\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
            "Downloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting folders id from shared memory\n",
        "\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "folder_id = \"1e0eA-AIsz_BSwVHOppJMXECX42hBfG4J\"\n",
        "\n",
        "def list_all_files_in_folder_recursive(parent_id, parent_path=\"\"):\n",
        "    \"\"\"Recursively list all files in a folder and subfolders\"\"\"\n",
        "    all_files = []\n",
        "\n",
        "    query = f\"'{parent_id}' in parents and trashed=false\"\n",
        "    page_token = None\n",
        "\n",
        "    while True:\n",
        "        response = drive_service.files().list(\n",
        "            q=query,\n",
        "            spaces='drive',\n",
        "            fields='nextPageToken, files(id, name, mimeType)',\n",
        "            pageToken=page_token\n",
        "        ).execute()\n",
        "\n",
        "        for item in response.get('files', []):\n",
        "            item_path = f\"{parent_path}/{item['name']}\" if parent_path else item['name']\n",
        "\n",
        "            if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "                # Recurse into subfolder\n",
        "                all_files.extend(list_all_files_in_folder_recursive(item['id'], item_path))\n",
        "            else:\n",
        "                all_files.append({\n",
        "                    \"file_name\": item['name'],\n",
        "                    \"file_path\": item_path,\n",
        "                    \"file_id\": item['id'],\n",
        "                    \"file_url\": f\"https://drive.google.com/file/d/{item['id']}/view?usp=sharing\"\n",
        "                })\n",
        "\n",
        "        page_token = response.get('nextPageToken', None)\n",
        "        if page_token is None:\n",
        "            break\n",
        "\n",
        "    return all_files\n",
        "\n",
        "# Run the recursive function\n",
        "files_list = list_all_files_in_folder_recursive(folder_id)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_files = pd.DataFrame(files_list)\n",
        "##df_files.head()\n",
        "\n",
        "##getting local id of the file\n",
        "!pip install python-docx\n",
        "import os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "import io\n",
        "\n",
        "def get_chapter_file(chapter_num: int, year: str, download_dir=\"/content\") -> str:\n",
        "    \"\"\"\n",
        "    Download a chapter file for a given year from Google Drive and return its local path.\n",
        "\n",
        "    Args:\n",
        "        chapter_num (int): The chapter number (e.g., 1, 2, ..., 14).\n",
        "        year (str): The year folder name.\n",
        "        download_dir (str): Local folder to save the file. Defaults to /content in Colab.\n",
        "\n",
        "    Returns:\n",
        "        str: Local path to the downloaded file, or None if not found.\n",
        "    \"\"\"\n",
        "    # Normalize chapter filename (01, 02, ...)\n",
        "    chapter_str = f\"{chapter_num:02}\"\n",
        "\n",
        "    # Find matching file in df_files\n",
        "    match = df_files[\n",
        "        (df_files[\"file_name\"].str.contains(chapter_str)) &\n",
        "        (df_files[\"file_path\"].str.contains(f\"{year}/\"))\n",
        "    ]\n",
        "\n",
        "    if match.empty:\n",
        "        print(f\"❌ No file found for chapter {chapter_str} in year {year}\")\n",
        "        return None\n",
        "\n",
        "    file_id = match.iloc[0][\"file_id\"]\n",
        "    file_name = match.iloc[0][\"file_name\"]\n",
        "    local_path = os.path.join(download_dir, year, file_name)\n",
        "\n",
        "    # Ensure year directory exists locally\n",
        "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "    # Download the file from Google Drive\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    fh = io.FileIO(local_path, \"wb\")\n",
        "    downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        status, done = downloader.next_chunk()\n",
        "        if status:\n",
        "            print(f\"⬇️ Download {int(status.progress() * 100)}%.\")\n",
        "\n",
        "    print(f\"✅ Saved to {local_path}\")\n",
        "    return local_path\n",
        "\n",
        "from docx import Document\n",
        "\n",
        "# After downloading the file\n",
        "# sanity check\n",
        "path = get_chapter_file(14, \"2015\")\n",
        "print(\"Local file:\", path)\n",
        "\n",
        "if path and path.endswith(\".docx\"):\n",
        "    doc = Document(path)\n",
        "    full_text = \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip() != \"\"])\n",
        "\n",
        "    # Split into sentences (naive split by period/full stop)\n",
        "    sentences = full_text.replace(\"\\n\", \" \").split(\".\")\n",
        "\n",
        "    # Print first few sentences\n",
        "    preview_count = 3\n",
        "    preview_sentences = [s.strip() for s in sentences if s.strip()][:preview_count]\n",
        "    print(\"\\nPreview of content:\")\n",
        "    for i, s in enumerate(preview_sentences, 1):\n",
        "        print(f\"{i}. {s}.\")\n",
        "\n",
        "\n",
        "#a code that downloads the files to a directory called content/reports\n",
        "import os\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "def download_all_chapters(download_dir=\"/content/reports\", years=range(2001, 2025), chapters=range(1, 16)):\n",
        "    \"\"\"\n",
        "    Downloads all chapters for all years to local environment.\n",
        "\n",
        "    Returns:\n",
        "        dict: {year: {chapter_number: local_path}}\n",
        "    \"\"\"\n",
        "    all_paths = {}\n",
        "\n",
        "    for year in years:\n",
        "        year_str = str(year)\n",
        "        all_paths[year_str] = {}\n",
        "\n",
        "        for chapter in chapters:\n",
        "            chapter_str = f\"{chapter:02}\"\n",
        "\n",
        "            # Find matching file in df_files\n",
        "            match = df_files[\n",
        "                (df_files[\"file_name\"].str.contains(chapter_str)) &\n",
        "                (df_files[\"file_path\"].str.contains(f\"{year_str}/\"))\n",
        "            ]\n",
        "\n",
        "            if match.empty:\n",
        "                print(f\"⚠️ Chapter {chapter_str} not found for year {year_str}\")\n",
        "                continue\n",
        "\n",
        "            file_id = match.iloc[0][\"file_id\"]\n",
        "            file_name = match.iloc[0][\"file_name\"]\n",
        "            local_path = os.path.join(download_dir, year_str, f\"{chapter_str}_{file_name}\")\n",
        "\n",
        "            # Ensure folder exists\n",
        "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "            # Download file\n",
        "            request = drive_service.files().get_media(fileId=file_id)\n",
        "            fh = io.FileIO(local_path, \"wb\")\n",
        "            downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "            done = False\n",
        "            while not done:\n",
        "                status, done = downloader.next_chunk()\n",
        "                if status:\n",
        "                    print(f\"⬇️ Download {int(status.progress() * 100)}% for {file_name}\")\n",
        "\n",
        "            print(f\"✅ Saved {file_name} to {local_path}\")\n",
        "            all_paths[year_str][chapter] = local_path\n",
        "\n",
        "    return all_paths\n",
        "download_all_chapters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYsrLuyKpTWb",
        "outputId": "f9585570-fd50-4fc6-857f-7b6dc7936eb0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "⬇️ Download 100%.\n",
            "✅ Saved to /content/2015/14.docx\n",
            "Local file: /content/2015/14.docx\n",
            "\n",
            "Preview of content:\n",
            "1. 14 ילדים במצבי סיכון ומצוקה מבוא פרק זה, המוקדש לילדים במצבי סיכון ומצוקה, הוא אחד הפרקים המורכבים בשנתון.\n",
            "2. ההגדרה מיהו ילד בסיכון או במצוקה תלויה בסביבה, בחברה, בתקופה ובמגדיר.\n",
            "3. לא רק זאת, קשה לאמוד את היקף התופעות הקשורות לילדים במצבי סיכון ומצוקה, שכן לפי טבען לא תמיד הן חשופות או מדווחות, והמקרים הידועים אינם אלא קצה הקרחון.\n",
            "⬇️ Download 100% for 01.docx\n",
            "✅ Saved 01.docx to /content/reports/2020/01_01.docx\n",
            "⬇️ Download 100% for לקט נתוני שנתון 2020.docx\n",
            "✅ Saved לקט נתוני שנתון 2020.docx to /content/reports/2020/02_לקט נתוני שנתון 2020.docx\n",
            "⬇️ Download 100% for 03.docx\n",
            "✅ Saved 03.docx to /content/reports/2020/03_03.docx\n",
            "⬇️ Download 100% for 04.docx\n",
            "✅ Saved 04.docx to /content/reports/2020/04_04.docx\n",
            "⬇️ Download 100% for 05.docx\n",
            "✅ Saved 05.docx to /content/reports/2020/05_05.docx\n",
            "⬇️ Download 100% for 06.docx\n",
            "✅ Saved 06.docx to /content/reports/2020/06_06.docx\n",
            "⬇️ Download 100% for 07.docx\n",
            "✅ Saved 07.docx to /content/reports/2020/07_07.docx\n",
            "⬇️ Download 100% for 08.docx\n",
            "✅ Saved 08.docx to /content/reports/2020/08_08.docx\n",
            "⬇️ Download 100% for 09.docx\n",
            "✅ Saved 09.docx to /content/reports/2020/09_09.docx\n",
            "⬇️ Download 100% for 10.docx\n",
            "✅ Saved 10.docx to /content/reports/2020/10_10.docx\n",
            "⬇️ Download 100% for 11.docx\n",
            "✅ Saved 11.docx to /content/reports/2020/11_11.docx\n",
            "⬇️ Download 100% for 12.docx\n",
            "✅ Saved 12.docx to /content/reports/2020/12_12.docx\n",
            "⬇️ Download 100% for 13.docx\n",
            "✅ Saved 13.docx to /content/reports/2020/13_13.docx\n",
            "⬇️ Download 100% for 14.docx\n",
            "✅ Saved 14.docx to /content/reports/2020/14_14.docx\n",
            "⚠️ Chapter 15 not found for year 2020\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2020': {1: '/content/reports/2020/01_01.docx',\n",
              "  2: '/content/reports/2020/02_לקט נתוני שנתון 2020.docx',\n",
              "  3: '/content/reports/2020/03_03.docx',\n",
              "  4: '/content/reports/2020/04_04.docx',\n",
              "  5: '/content/reports/2020/05_05.docx',\n",
              "  6: '/content/reports/2020/06_06.docx',\n",
              "  7: '/content/reports/2020/07_07.docx',\n",
              "  8: '/content/reports/2020/08_08.docx',\n",
              "  9: '/content/reports/2020/09_09.docx',\n",
              "  10: '/content/reports/2020/10_10.docx',\n",
              "  11: '/content/reports/2020/11_11.docx',\n",
              "  12: '/content/reports/2020/12_12.docx',\n",
              "  13: '/content/reports/2020/13_13.docx',\n",
              "  14: '/content/reports/2020/14_14.docx'}}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "!apt-get install -y libreoffice\n",
        "import subprocess\n",
        "\n",
        "def convert_doc_to_docx(base_dir=\"/content/reports\"):\n",
        "    for root, _, files in os.walk(base_dir):\n",
        "        for fname in files:\n",
        "            if fname.endswith(\".doc\") and not fname.endswith(\".docx\"):\n",
        "                fpath = os.path.join(root, fname)\n",
        "                print(fpath)\n",
        "                subprocess.run([\n",
        "                    \"libreoffice\", \"--headless\", \"--convert-to\", \"docx\", fpath, \"--outdir\", root\n",
        "                ])\n",
        "\n",
        "# convert all docs\n",
        "convert_doc_to_docx(\"/content/reports\")"
      ],
      "metadata": {
        "id": "sNx4GvI-FADG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "!apt-get install -y libreoffice\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def extract_tables_from_reports(base_dir=\"/content/reports\", out_dir=\"/content/tables\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    summary = {}\n",
        "    colnames_map = {}\n",
        "\n",
        "    # loop years\n",
        "    for year in range(2001, 2025):\n",
        "        print(year)\n",
        "        year_path = os.path.join(base_dir, str(year))\n",
        "        if not os.path.isdir(year_path):\n",
        "            continue\n",
        "\n",
        "        for fname in os.listdir(year_path):\n",
        "            if not (fname.endswith(\".docx\") or fname.endswith(\".doc\")):\n",
        "                continue\n",
        "            if fname.endswith(\".doc\"):\n",
        "              continue\n",
        "\n",
        "            chapter = fname.split(\"_\")[0]\n",
        "            fpath = os.path.join(year_path, fname)\n",
        "\n",
        "            try:\n",
        "                doc = Document(fpath)\n",
        "            except Exception as e:\n",
        "                print(f\"skip {fpath}: {e}\")\n",
        "                continue\n",
        "\n",
        "            serial = 1\n",
        "            # find all tables with names containing \"לוח\"\n",
        "            for i, table in enumerate(doc.tables):\n",
        "                # find nearest paragraph before this table\n",
        "                table_name = \"\"\n",
        "                # default: empty name\n",
        "                table_name = \"\"\n",
        "\n",
        "                # check the first row cells for \"לוח\"\n",
        "                if len(table.rows) > 0:\n",
        "                    for cell in table.rows[0].cells:\n",
        "                        if \"לוח\" in cell.text:\n",
        "                            table_name = cell.text.strip()\n",
        "                            break\n",
        "\n",
        "                    # convert table to dataframe\n",
        "                    data = [[cell.text.strip() for cell in row.cells] for row in table.rows]\n",
        "                    df = pd.DataFrame(data)\n",
        "\n",
        "                    # build identifier\n",
        "                    identifier = f\"{serial}_{chapter}_{year}\"\n",
        "\n",
        "                    # save CSV\n",
        "                    save_dir = os.path.join(out_dir, str(year), chapter)\n",
        "                    os.makedirs(save_dir, exist_ok=True)\n",
        "                    save_path = os.path.join(save_dir, f\"{identifier}.csv\")\n",
        "                    df.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "                    # record mapping\n",
        "                    summary[identifier] = table_name\n",
        "\n",
        "                    if len(df) > 0:\n",
        "                        colnames_map[identifier] = df.iloc[0].tolist()\n",
        "                    else:\n",
        "                        colnames_map[identifier] = []\n",
        "\n",
        "                    serial += 1\n",
        "\n",
        "    # write summary JSON (after all years done)\n",
        "    with open(os.path.join(out_dir, \"tables_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    with open(os.path.join(out_dir, \"tables_columns.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(colnames_map, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "extract_tables_from_reports()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkBcC-xkwcbD",
        "outputId": "cb2f9e34-4ce7-4f07-add9-c76aa47e58ab"
      },
      "execution_count": 32,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libreoffice is already the newest version (1:7.3.7-0ubuntu0.22.04.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "2001\n",
            "2002\n",
            "2003\n",
            "2004\n",
            "2005\n",
            "2006\n",
            "2007\n",
            "2008\n",
            "2009\n",
            "2010\n",
            "2011\n",
            "2012\n",
            "2013\n",
            "2014\n",
            "2015\n",
            "2016\n",
            "2017\n",
            "2018\n",
            "2019\n",
            "2020\n",
            "2021\n",
            "2022\n",
            "2023\n",
            "2024\n"
          ]
        }
      ]
    }
  ]
}