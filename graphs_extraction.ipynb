{
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def process_file_from_url(url, year):\n",
        "    \"\"\"\n",
        "    Downloads a file from a URL, processes it, extracts tables from DOCX,\n",
        "    saves them as CSVs, and stores metadata.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the file.\n",
        "        year (int): The year to include in the CSV filenames.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping original table topics to their new filenames.\n",
        "    \"\"\"\n",
        "    # Step 2: Download the file\n",
        "    local_filename = url.split('/')[-1]\n",
        "    print(f\"Downloading {url} to {local_filename}\")\n",
        "    try:\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(local_filename, 'wb') as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "        print(\"Download complete.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading file: {e}\")\n",
        "        return {}\n",
        "\n",
        "\n",
        "    # Step 3: Determine file type and convert if necessary\n",
        "    file_extension = os.path.splitext(local_filename)[1].lower()\n",
        "    docx_path = local_filename\n",
        "\n",
        "    if file_extension == '.pdf':\n",
        "        print(f\"Detected PDF file. Attempting to convert {local_filename} to DOCX.\")\n",
        "        try:\n",
        "            # Basic PDF to DOCX conversion using pdfplumber\n",
        "            # This will extract text but may not preserve formatting or tables accurately.\n",
        "            # For better results, consider using libraries like 'pdf2docx' or external services.\n",
        "            docx_path = local_filename + \".docx\"\n",
        "            document = Document()\n",
        "            with pdfplumber.open(local_filename) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    text = page.extract_text()\n",
        "                    if text:\n",
        "                        document.add_paragraph(text)\n",
        "            document.save(docx_path)\n",
        "            print(f\"Conversion to DOCX complete: {docx_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting PDF to DOCX: {e}\")\n",
        "            print(\"Proceeding with the original file assuming it might be parsable by a DOCX reader or skipping table extraction.\")\n",
        "            # If conversion fails, we might not be able to extract tables reliably.\n",
        "            # Depending on requirements, you might want to return here or handle this case differently.\n",
        "\n",
        "\n",
        "    elif file_extension == '.docx':\n",
        "        print(f\"Detected DOCX file: {local_filename}\")\n",
        "        pass # File is already DOCX, no conversion needed\n",
        "    else:\n",
        "        print(f\"Unsupported file type: {file_extension}. Skipping table extraction.\")\n",
        "        return {} # Return empty dictionary for unsupported types\n",
        "\n",
        "\n",
        "    # Step 4: Extract tables from DOCX\n",
        "    tables = []\n",
        "    if os.path.exists(docx_path):\n",
        "      try:\n",
        "          tables = extract_tables_from_docx(docx_path)\n",
        "          print(f\"Extracted {len(tables)} tables from {docx_path}\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error extracting tables from DOCX: {e}\")\n",
        "          return {}\n",
        "    else:\n",
        "      print(f\"DOCX file not found at {docx_path}. Skipping table extraction.\")\n",
        "      return {}\n",
        "\n",
        "\n",
        "    # Step 5: Save tables as CSV and store metadata\n",
        "    table_metadata = {}\n",
        "    for i, table in enumerate(tables, 1):\n",
        "        # Placeholder for extracting original table topic\n",
        "        # This is highly dependent on the document structure and might require\n",
        "        # more advanced parsing or heuristics.\n",
        "        original_topic = f\"Table {i}\" # Using a placeholder for now\n",
        "\n",
        "        csv_filename = f\"{i}-{year}.csv\"\n",
        "        table.to_csv(csv_filename, index=False)\n",
        "        table_metadata[original_topic] = csv_filename\n",
        "        print(f\"Saved table {i} to {csv_filename}\")\n",
        "\n",
        "    # Step 6: Return the dictionary\n",
        "    return table_metadata\n",
        "\n",
        "def extract_tables_from_docx(docx_path):\n",
        "    \"\"\"\n",
        "    Extracts tables from a DOCX file.\n",
        "\n",
        "    Args:\n",
        "        docx_path (str): The path to the DOCX file.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of pandas DataFrames, where each DataFrame represents a table.\n",
        "    \"\"\"\n",
        "    doc = Document(docx_path)\n",
        "    tables = []\n",
        "\n",
        "    for table in doc.tables:\n",
        "        data = []\n",
        "        for row in table.rows:\n",
        "            data.append([cell.text.strip() for cell in row.cells])\n",
        "        if data: # Ensure table is not empty\n",
        "            df = pd.DataFrame(data)\n",
        "            tables.append(df)\n",
        "    return tables\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "os.chdir('content/drive/Shareddrives/')\n",
        "\n",
        "# Example usage (You can uncomment and modify this to test)\n",
        "url = pdf_path # Replace with the actual file URL\n",
        "year = 2024 # Replace with the desired year\n",
        "extracted_info = process_file_from_url(url, year)\n",
        "print(\"\\nExtracted Information:\")\n",
        "print(extracted_info)"
      ],
      "metadata": {
        "id": "1Q5nm6WiPzlr"
      },
      "id": "1Q5nm6WiPzlr",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
