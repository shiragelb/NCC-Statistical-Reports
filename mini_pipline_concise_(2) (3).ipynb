{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbUF4VOQqNds",
        "outputId": "9139e26f-4fba-4bd6-b489-17f486ea1fc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n",
            "Collecting camelot-py[cv]\n",
            "  Downloading camelot_py-1.0.9-py3-none-any.whl.metadata (9.8 kB)\n",
            "\u001b[33mWARNING: camelot-py 1.0.9 does not provide the extra 'cv'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (8.2.1)\n",
            "Requirement already satisfied: chardet>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (5.2.0)\n",
            "Requirement already satisfied: numpy>=1.26.1 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (2.0.2)\n",
            "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (3.1.5)\n",
            "Requirement already satisfied: pdfminer-six>=20240706 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (20250506)\n",
            "Collecting pypdf<6.0,>=4.0 (from camelot-py[cv])\n",
            "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (2.2.2)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (0.9.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (4.12.0.88)\n",
            "Requirement already satisfied: pypdfium2>=4 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (4.30.0)\n",
            "Requirement already satisfied: pillow>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (11.3.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl>=3.1.0->camelot-py[cv]) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py[cv]) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (2.22)\n",
            "Downloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading camelot_py-1.0.9-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, camelot-py\n",
            "Successfully installed camelot-py-1.0.9 pypdf-5.9.0\n",
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.12/dist-packages (from tabula-py) (2.2.2)\n",
            "Requirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.12/dist-packages (from tabula-py) (2.0.2)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.12/dist-packages (from tabula-py) (1.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.17.0)\n",
            "Downloading tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tabula-py\n",
            "Successfully installed tabula-py-2.10.0\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n",
            "Collecting anthropic\n",
            "  Downloading anthropic-0.66.0-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.1)\n",
            "Downloading anthropic-0.66.0-py3-none-any.whl (308 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.0/308.0 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: anthropic\n",
            "Successfully installed anthropic-0.66.0\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.1)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber\n",
        "!pip install camelot-py[cv]\n",
        "!pip install tabula-py\n",
        "!pip install python-docx\n",
        "!pip install anthropic\n",
        "!pip install plotly -q\n",
        "!pip install networkx\n",
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VP87dKBWqURk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "from docx import Document\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import camelot\n",
        "import tabula\n",
        "import pdfplumber\n",
        "from docx.shared import Inches # Import Inches for setting image size\n",
        "import json\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_XZazA2sziY"
      },
      "source": [
        "What This System Does:\n",
        "1. Initial Data Extraction (First Part)\n",
        "\n",
        "Takes uploaded DOCX files (chapters from 2001 and 2002)\n",
        "Extracts tables from these documents\n",
        "Saves them as CSV files with a naming pattern: table{i}{chapter}{year}.csv\n",
        "Creates a reference dictionary mapping Hebrew headers to file paths\n",
        "\n",
        "2. Core Functionality - Table Evolution Tracking\n",
        "The system solves this problem: \"Given tables from different years, which tables in Year 2 are continuations of tables from Year 1?\"\n",
        "This is challenging because:\n",
        "\n",
        "Tables may change headers slightly year-to-year\n",
        "Hebrew text requires special processing\n",
        "Tables might split (1→N) or merge (N→1)\n",
        "Some tables may disappear and reappear (gaps)\n",
        "\n",
        "3. Processing Pipeline\n",
        "DOCX Files → Table Extraction → Hebrew Processing → Embeddings → Similarity Matching → Chain Building → Validation → Visualization\n",
        "Key Steps:\n",
        "\n",
        "Hebrew Text Processing: Normalizes Hebrew text, removes year references, handles special characters\n",
        "Embedding Generation: Creates semantic vectors for each table using multilingual models\n",
        "Similarity Computation: Builds matrices comparing all tables between years\n",
        "Hungarian Algorithm: Finds optimal 1-to-1 matches between years\n",
        "Special Cases Detection: Identifies splits, merges, and gaps\n",
        "Claude API Validation: For uncertain matches (0.85-0.97 similarity), asks Claude to validate\n",
        "Chain Building: Creates continuous chains showing table evolution\n",
        "\n",
        "How It Relies on Initial Data:\n",
        "The initial DOCX uploads provide:\n",
        "\n",
        "Table Content: The actual data and headers\n",
        "Structure: How many tables exist each year\n",
        "Hebrew Headers: Critical for matching - the system must understand that \"מספר הילדים לפי גיל 2001\" and \"מספר הילדים לפי גיל 2002\" are the same table\n",
        "\n",
        "The process_documents() function creates:\n",
        "\n",
        "CSV files in tables/ directory\n",
        "table_references.json with Hebrew header → filename mappings\n",
        "\n",
        "Key Thresholds:\n",
        "\n",
        ">0.97: Automatic match (high confidence)\n",
        "0.85-0.97: Edge case (needs API validation)\n",
        "<0.85: No match\n",
        "Splits/Merges: Detected at 0.80+ similarity\n",
        "\n",
        "Output:\n",
        "\n",
        "JSON graph structure showing table evolution\n",
        "HTML/Markdown reports with statistics\n",
        "Interactive Sankey diagrams\n",
        "Validation reports\n",
        "\n",
        "The code is essentially building a temporal knowledge graph of how statistical tables evolve, handling the complexities of real-world data where tables don't always continue cleanly from year to year."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# This opens a file dialog to select files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "3s0K0a5EMiOC",
        "outputId": "eb3ef716-bc61-4462-e5cb-3634bb008bec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-884d15fe-3258-4ecc-8e76-5e6912a0f8d6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-884d15fe-3258-4ecc-8e76-5e6912a0f8d6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving tables_columns (1).json to tables_columns (1).json\n",
            "Saving tables_summary (2).json to tables_summary (2).json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAsAp65Ss6y5"
      },
      "source": [
        "#  config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9mU-I3gqe6M",
        "outputId": "94523aea-16dd-46ca-d491-70e353f85e04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.py\n",
        "import json\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MatchingConfig:\n",
        "    tables_dir: str = \"/content/tables\"  # Updated from \"tables\"\n",
        "    reference_json: str = \"tables_summary.json\"  # Updated from \"table_references.json\"\n",
        "    columns_json: str = \"tables_columns.json\"  # New field for column headers\n",
        "    output_dir: str = \"output\"\n",
        "\n",
        "    similarity_threshold: float = 0.85\n",
        "    confident_threshold: float = 0.97\n",
        "    split_threshold: float = 0.80\n",
        "    merge_threshold: float = 0.80\n",
        "    max_gap_years: int = 2\n",
        "\n",
        "    use_api_validation: bool = False\n",
        "    api_key: Optional[str] = None\n",
        "\n",
        "    def save(self, path=\"config.json\"):\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(self.__dict__, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVj85cbms8jJ"
      },
      "source": [
        "# hebrew_processor.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5T9wCrLs84g",
        "outputId": "3ce4ca22-ef14-4c48-e02e-26278b3989fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hebrew_processor.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile hebrew_processor.py\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "class HebrewProcessor:\n",
        "    def __init__(self):\n",
        "        self.year_patterns = [\n",
        "            r'ממוצע \\d{4}', r'סוף \\d{4}', r'\\d{4}'\n",
        "        ]\n",
        "\n",
        "    def process_header(self, text):\n",
        "        text = unicodedata.normalize('NFC', text)\n",
        "        text = re.sub(r'[\\u0591-\\u05C7]', '', text)\n",
        "        for pattern in self.year_patterns:\n",
        "            text = re.sub(pattern, '', text)\n",
        "        text = re.sub(r'\\(המשך\\)', '', text)\n",
        "        text = re.sub(r'לוח:\\s*\\d+\\.\\d+', '', text)\n",
        "        return ' '.join(text.split()).strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39jo1cPOtCYs"
      },
      "source": [
        "# table_loader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVKZtywatCK5",
        "outputId": "714d9b85-9ea8-4bdc-ba79-2a956d0e39ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing table_loader.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile table_loader.py\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "class TableLoader:\n",
        "    def __init__(self, tables_dir=\"/content/tables\",\n",
        "                 reference_json=\"tables_summary.json\",\n",
        "                 columns_json=\"tables_columns.json\"):\n",
        "        self.tables_dir = tables_dir\n",
        "        self.reference_json = reference_json\n",
        "        self.columns_json = columns_json\n",
        "        self.tables_metadata = {}\n",
        "        self.tables_by_year = {}\n",
        "        self.column_names = {}  # New: store column names\n",
        "\n",
        "    def load_metadata(self):\n",
        "        # Load the main tables summary (identifier → header)\n",
        "        with open(self.reference_json, 'r', encoding='utf-8') as f:\n",
        "            identifier_to_header = json.load(f)\n",
        "\n",
        "        # Load column names if available\n",
        "        if os.path.exists(self.columns_json):\n",
        "            with open(self.columns_json, 'r', encoding='utf-8') as f:\n",
        "                self.column_names = json.load(f)\n",
        "\n",
        "        # Process each identifier\n",
        "        for identifier, header in identifier_to_header.items():\n",
        "            # Parse new format: serial_chapter_year (e.g., \"1_03_2021\")\n",
        "            match = re.match(r'(\\d+)_(\\d+)_(\\d{4})', identifier)\n",
        "            if match:\n",
        "                serial, chapter, year = match.groups()\n",
        "                serial = int(serial)\n",
        "                chapter_str = chapter  # Keep as string with leading zeros\n",
        "                chapter_int = int(chapter)\n",
        "                year = int(year)\n",
        "\n",
        "                # Build filepath (not used but kept for metadata)\n",
        "                filepath = os.path.join(self.tables_dir, str(year),\n",
        "                                    chapter_str, f\"{identifier}.csv\")\n",
        "\n",
        "                # Store metadata with identifier as key\n",
        "                self.tables_metadata[identifier] = {\n",
        "                    'id': identifier,\n",
        "                    'file': filepath,\n",
        "                    'header': header,\n",
        "                    'year': year,\n",
        "                    'chapter': chapter_int,\n",
        "                    'serial': serial,\n",
        "                    'columns': self.column_names.get(identifier, [])  # Add column names\n",
        "                }\n",
        "\n",
        "                # Group by year\n",
        "                if year not in self.tables_by_year:\n",
        "                    self.tables_by_year[year] = []\n",
        "                self.tables_by_year[year].append(identifier)\n",
        "\n",
        "        return len(self.tables_metadata)\n",
        "\n",
        "    def load_table_data(self, table_id):\n",
        "        \"\"\"Load actual CSV data for a table\"\"\"\n",
        "        metadata = self.tables_metadata.get(table_id)\n",
        "        if metadata and os.path.exists(metadata['file']):\n",
        "            return pd.read_csv(metadata['file'], header=None)\n",
        "        return None\n",
        "\n",
        "    def get_header_for_identifier(self, identifier):\n",
        "        \"\"\"Get header text for an identifier\"\"\"\n",
        "        metadata = self.tables_metadata.get(identifier)\n",
        "        return metadata['header'] if metadata else None\n",
        "\n",
        "    def get_columns_for_identifier(self, identifier):\n",
        "        \"\"\"Get column names for an identifier\"\"\"\n",
        "        metadata = self.tables_metadata.get(identifier)\n",
        "        return metadata['columns'] if metadata else []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A06xwZZgtRek"
      },
      "source": [
        "# Similarity Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9hgjkW8tOji",
        "outputId": "749d2598-a92a-4658-faf0-d179286c5ac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing similarity.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile similarity.py\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "class SimilarityBuilder:\n",
        "    def compute_similarity_matrix(self, chain_embeddings, table_embeddings):\n",
        "        chain_ids = list(chain_embeddings.keys())\n",
        "        table_ids = list(table_embeddings.keys())\n",
        "\n",
        "        n_chains = len(chain_ids)\n",
        "        n_tables = len(table_ids)\n",
        "\n",
        "        matrix = np.zeros((n_chains, n_tables))\n",
        "\n",
        "        for i, chain_id in enumerate(chain_ids):\n",
        "            for j, table_id in enumerate(table_ids):\n",
        "                # Cosine similarity\n",
        "                sim = 1 - cosine(chain_embeddings[chain_id],\n",
        "                                 table_embeddings[table_id])\n",
        "                matrix[i, j] = (sim + 1) / 2  # Normalize to [0,1]\n",
        "\n",
        "        return {\n",
        "            'matrix': matrix,\n",
        "            'chain_ids': chain_ids,\n",
        "            'table_ids': table_ids\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUeR4LPhtVC4"
      },
      "source": [
        "# Hungarian Matching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbEM9YZstX4k",
        "outputId": "b4cc7a39-b4bc-421d-e6ca-45f2cac3197e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing hungarian.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile hungarian.py\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "class HungarianMatcher:\n",
        "    def __init__(self, threshold=0.85):\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def find_optimal_matching(self, sim_matrix):\n",
        "        matrix = sim_matrix['matrix']\n",
        "        chain_ids = sim_matrix['chain_ids']\n",
        "        table_ids = sim_matrix['table_ids']\n",
        "\n",
        "        # Convert to cost matrix\n",
        "        cost = 1 - matrix\n",
        "        row_ind, col_ind = linear_sum_assignment(cost)\n",
        "\n",
        "        matches = []\n",
        "        for i, j in zip(row_ind, col_ind):\n",
        "            if i < len(chain_ids) and j < len(table_ids):\n",
        "                similarity = matrix[i, j]\n",
        "                if similarity >= self.threshold:\n",
        "                    matches.append((chain_ids[i], table_ids[j], similarity))\n",
        "\n",
        "        unmatched_chains = [c for i, c in enumerate(chain_ids)\n",
        "                           if i not in row_ind]\n",
        "        unmatched_tables = [t for j, t in enumerate(table_ids)\n",
        "                           if j not in col_ind]\n",
        "\n",
        "        return {\n",
        "            'matches': matches,\n",
        "            'unmatched_chains': unmatched_chains,\n",
        "            'unmatched_tables': unmatched_tables\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5HZ2W_OtanW"
      },
      "source": [
        "# Split/Merge Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZvYuIaEtaW7",
        "outputId": "d19ec2ba-765f-42ff-fc0b-eec24ac3d897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing split_merge.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile split_merge.py\n",
        "class SplitMergeDetector:\n",
        "    def __init__(self, split_threshold=0.80, merge_threshold=0.80):\n",
        "        self.split_threshold = split_threshold\n",
        "        self.merge_threshold = merge_threshold\n",
        "\n",
        "    def detect_splits(self, sim_matrix):\n",
        "        splits = []\n",
        "        matrix = sim_matrix['matrix']\n",
        "        chain_ids = sim_matrix['chain_ids']\n",
        "        table_ids = sim_matrix['table_ids']\n",
        "\n",
        "        for i, chain_id in enumerate(chain_ids):\n",
        "            high_sim_tables = []\n",
        "            for j, table_id in enumerate(table_ids):\n",
        "                if matrix[i, j] >= self.split_threshold:\n",
        "                    high_sim_tables.append((table_id, matrix[i, j]))\n",
        "\n",
        "            if len(high_sim_tables) >= 2:\n",
        "                splits.append({\n",
        "                    'chain': chain_id,\n",
        "                    'targets': high_sim_tables\n",
        "                })\n",
        "\n",
        "        return splits\n",
        "\n",
        "    def detect_merges(self, sim_matrix):\n",
        "        merges = []\n",
        "        matrix = sim_matrix['matrix']\n",
        "        chain_ids = sim_matrix['chain_ids']\n",
        "        table_ids = sim_matrix['table_ids']\n",
        "\n",
        "        for j, table_id in enumerate(table_ids):\n",
        "            high_sim_chains = []\n",
        "            for i, chain_id in enumerate(chain_ids):\n",
        "                if matrix[i, j] >= self.merge_threshold:\n",
        "                    high_sim_chains.append((chain_id, matrix[i, j]))\n",
        "\n",
        "            if len(high_sim_chains) >= 2:\n",
        "                merges.append({\n",
        "                    'table': table_id,\n",
        "                    'sources': high_sim_chains\n",
        "                })\n",
        "\n",
        "        return merges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBYG1xpqthcU"
      },
      "source": [
        "# Chain Manager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2AA23qetir6",
        "outputId": "a960a69e-8b80-430a-be5b-965d0f64ebc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing chains.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile chains.py\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class ChainManager:\n",
        "    def __init__(self):\n",
        "        self.chains = {}\n",
        "        self.match_details = {}  # Store similarity scores and API usage\n",
        "\n",
        "    def initialize_from_first_year(self, tables):\n",
        "        for table_id, metadata in tables.items():\n",
        "            chain_id = f\"chain_{table_id}\"\n",
        "            self.chains[chain_id] = {\n",
        "                'id': chain_id,\n",
        "                'tables': [table_id],\n",
        "                'years': [metadata['year']],\n",
        "                'headers': [metadata['header']],\n",
        "                'columns': [metadata.get('columns', [])],  # New: track columns\n",
        "                'status': 'active',\n",
        "                'gaps': [],\n",
        "                'similarities': [],  # Store similarity scores\n",
        "                'api_validated': []  # Track API validation usage\n",
        "            }\n",
        "        return len(self.chains)\n",
        "\n",
        "    def update_chains(self, matches, year, table_metadata, api_validations=None):\n",
        "        matched_chains = set()\n",
        "        for match_info in matches:\n",
        "            # Handle both tuple and dict formats\n",
        "            if isinstance(match_info, tuple):\n",
        "                chain_id, table_id, similarity = match_info\n",
        "                api_used = False\n",
        "            else:\n",
        "                chain_id = match_info['chain_id']\n",
        "                table_id = match_info['table_id']\n",
        "                similarity = match_info['similarity']\n",
        "                api_used = match_info.get('api_validated', False)\n",
        "\n",
        "            if chain_id in self.chains:\n",
        "                self.chains[chain_id]['tables'].append(table_id)\n",
        "                self.chains[chain_id]['years'].append(year)\n",
        "                self.chains[chain_id]['similarities'].append(similarity)\n",
        "                self.chains[chain_id]['api_validated'].append(api_used)\n",
        "\n",
        "                if table_id in table_metadata:\n",
        "                    self.chains[chain_id]['headers'].append(table_metadata[table_id]['header'])\n",
        "                    # New: Add column information\n",
        "                    self.chains[chain_id]['columns'].append(table_metadata[table_id].get('columns', []))\n",
        "\n",
        "                # Store match details for visualization\n",
        "                edge_key = f\"{self.chains[chain_id]['tables'][-2]}_{table_id}\"\n",
        "                self.match_details[edge_key] = {\n",
        "                    'similarity': similarity,\n",
        "                    'api_validated': api_used\n",
        "                }\n",
        "\n",
        "                matched_chains.add(chain_id)\n",
        "\n",
        "        # Mark unmatched as dormant\n",
        "        for chain_id, chain in self.chains.items():\n",
        "            if chain['status'] == 'active' and chain_id not in matched_chains:\n",
        "                chain['status'] = 'dormant'\n",
        "                chain['gaps'].append(year)\n",
        "\n",
        "    def get_chain_embeddings(self, embeddings_dict):\n",
        "        chain_embeddings = {}\n",
        "        for chain_id, chain in self.chains.items():\n",
        "            if chain['status'] == 'active' and chain['tables']:\n",
        "                last_table = chain['tables'][-1]\n",
        "                if last_table in embeddings_dict:\n",
        "                    chain_embeddings[chain_id] = embeddings_dict[last_table]\n",
        "        return chain_embeddings\n",
        "\n",
        "    def get_columns_for_chain(self, chain_id):\n",
        "        \"\"\"Get all column sets in a chain\"\"\"\n",
        "        if chain_id in self.chains:\n",
        "            return self.chains[chain_id].get('columns', [])\n",
        "        return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5qRKCzItlOn"
      },
      "source": [
        "# Report Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKg-rxC4tnuu",
        "outputId": "b123e927-af63-44ae-b1cf-cdb2778c8c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing report_gen.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile report_gen.py\n",
        "import json\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "class ReportGenerator:\n",
        "    def __init__(self):\n",
        "        self.timestamp = datetime.now()\n",
        "\n",
        "    def generate_summary(self, chains, statistics):\n",
        "        \"\"\"Generate summary with column statistics\"\"\"\n",
        "        # Calculate column statistics\n",
        "        total_column_sets = sum(len(chain.get('columns', [])) for chain in chains.values())\n",
        "        avg_columns_per_table = 0\n",
        "        if total_column_sets > 0:\n",
        "            all_column_counts = []\n",
        "            for chain in chains.values():\n",
        "                for cols in chain.get('columns', []):\n",
        "                    if cols:\n",
        "                        all_column_counts.append(len(cols))\n",
        "            if all_column_counts:\n",
        "                avg_columns_per_table = sum(all_column_counts) / len(all_column_counts)\n",
        "\n",
        "        summary = {\n",
        "            'timestamp': self.timestamp.isoformat(),\n",
        "            'total_chains': len(chains),\n",
        "            'active_chains': sum(1 for c in chains.values()\n",
        "                               if c['status'] == 'active'),\n",
        "            'statistics': statistics,\n",
        "            'column_info': {\n",
        "                'total_tables_with_columns': total_column_sets,\n",
        "                'average_columns_per_table': round(avg_columns_per_table, 2)\n",
        "            }\n",
        "        }\n",
        "        return summary\n",
        "\n",
        "    def save_chains_json(self, chains, filepath=\"chains.json\"):\n",
        "        \"\"\"Save chains to JSON with proper type conversion\"\"\"\n",
        "\n",
        "        def convert_to_native(obj):\n",
        "            \"\"\"Convert numpy types to native Python types\"\"\"\n",
        "            if isinstance(obj, np.bool_):\n",
        "                return bool(obj)\n",
        "            elif isinstance(obj, (np.integer, np.int32, np.int64)):\n",
        "                return int(obj)\n",
        "            elif isinstance(obj, (np.floating, np.float32, np.float64)):\n",
        "                return float(obj)\n",
        "            elif isinstance(obj, np.ndarray):\n",
        "                return obj.tolist()\n",
        "            elif isinstance(obj, list):\n",
        "                return [convert_to_native(item) for item in obj]\n",
        "            elif isinstance(obj, dict):\n",
        "                return {key: convert_to_native(value) for key, value in obj.items()}\n",
        "            else:\n",
        "                return obj\n",
        "\n",
        "        # Convert all chains\n",
        "        chains_export = convert_to_native(chains)\n",
        "\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(chains_export, f, indent=2, ensure_ascii=False)\n",
        "        return filepath\n",
        "\n",
        "    def generate_html_report(self, chains, statistics):\n",
        "        \"\"\"Generate HTML report with column information\"\"\"\n",
        "        html = f\"\"\"<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Chain Matching Report</title>\n",
        "    <meta charset=\"utf-8\">\n",
        "    <style>\n",
        "        body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
        "        h1, h2, h3 {{ color: #333; }}\n",
        "        .summary {{ background: #f0f0f0; padding: 15px; border-radius: 5px; margin: 20px 0; }}\n",
        "        .chain {{ margin: 15px 0; padding: 10px; border-left: 3px solid #4CAF50; }}\n",
        "        .active {{ background: #e8f5e9; }}\n",
        "        .dormant {{ background: #fff3e0; border-color: #FF9800; }}\n",
        "        .ended {{ background: #ffebee; border-color: #f44336; }}\n",
        "        .table-info {{ margin-left: 20px; font-size: 0.9em; color: #666; }}\n",
        "        .columns {{ font-size: 0.85em; color: #888; margin-left: 30px; }}\n",
        "        .statistics {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; }}\n",
        "        .stat-card {{ background: white; padding: 10px; border: 1px solid #ddd; border-radius: 5px; }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Table Chain Matching Report</h1>\n",
        "    <p><strong>Generated:</strong> {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
        "\n",
        "    <div class=\"summary\">\n",
        "        <h2>Summary</h2>\n",
        "        <div class=\"statistics\">\n",
        "            <div class=\"stat-card\">\n",
        "                <strong>Total Chains:</strong> {len(chains)}\n",
        "            </div>\n",
        "            <div class=\"stat-card\">\n",
        "                <strong>Active Chains:</strong> {sum(1 for c in chains.values() if c['status'] == 'active')}\n",
        "            </div>\n",
        "            <div class=\"stat-card\">\n",
        "                <strong>Dormant Chains:</strong> {sum(1 for c in chains.values() if c['status'] == 'dormant')}\n",
        "            </div>\n",
        "            <div class=\"stat-card\">\n",
        "                <strong>Ended Chains:</strong> {sum(1 for c in chains.values() if c['status'] == 'ended')}\n",
        "            </div>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <h2>Chain Details</h2>\"\"\"\n",
        "\n",
        "        # Group chains by status\n",
        "        active_chains = {k: v for k, v in chains.items() if v['status'] == 'active'}\n",
        "        dormant_chains = {k: v for k, v in chains.items() if v['status'] == 'dormant'}\n",
        "        ended_chains = {k: v for k, v in chains.items() if v['status'] == 'ended'}\n",
        "\n",
        "        # Active Chains\n",
        "        if active_chains:\n",
        "            html += \"<h3>Active Chains</h3>\"\n",
        "            for chain_id, chain in sorted(active_chains.items()):\n",
        "                html += self._format_chain_html(chain_id, chain)\n",
        "\n",
        "        # Dormant Chains\n",
        "        if dormant_chains:\n",
        "            html += \"<h3>Dormant Chains</h3>\"\n",
        "            for chain_id, chain in sorted(dormant_chains.items()):\n",
        "                html += self._format_chain_html(chain_id, chain)\n",
        "\n",
        "        # Ended Chains (show only summary)\n",
        "        if ended_chains:\n",
        "            html += f\"<h3>Ended Chains ({len(ended_chains)})</h3>\"\n",
        "            html += \"<p>Chains that have not matched for multiple years and are considered ended.</p>\"\n",
        "\n",
        "        # Add statistics if available\n",
        "        if statistics and 'year_by_year' in statistics:\n",
        "            html += \"<h2>Year-by-Year Statistics</h2>\"\n",
        "            html += \"<table border='1' style='border-collapse: collapse; width: 100%;'>\"\n",
        "            html += \"<tr><th>Year</th><th>Tables</th><th>Matches</th><th>Match Rate</th><th>Processing Time</th></tr>\"\n",
        "\n",
        "            for year, stats in sorted(statistics['year_by_year'].items()):\n",
        "                html += f\"\"\"<tr>\n",
        "                    <td>{year}</td>\n",
        "                    <td>{stats.get('tables', 'N/A')}</td>\n",
        "                    <td>{stats.get('matches', 'N/A')}</td>\n",
        "                    <td>{stats.get('match_rate', 'N/A')}</td>\n",
        "                    <td>{stats.get('processing_time', 'N/A')}</td>\n",
        "                </tr>\"\"\"\n",
        "            html += \"</table>\"\n",
        "\n",
        "        html += \"\"\"\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "        with open(\"report.html\", \"w\", encoding='utf-8') as f:\n",
        "            f.write(html)\n",
        "\n",
        "        return \"report.html\"\n",
        "\n",
        "    def _format_chain_html(self, chain_id, chain):\n",
        "        \"\"\"Format a single chain for HTML display\"\"\"\n",
        "        status_class = chain['status']\n",
        "        years_range = f\"{min(chain['years'])}-{max(chain['years'])}\" if chain['years'] else \"N/A\"\n",
        "\n",
        "        html = f\"\"\"<div class=\"chain {status_class}\">\n",
        "            <strong>{chain_id}</strong>\n",
        "            <span style=\"color: #666;\">({len(chain['tables'])} tables, Years: {years_range})</span>\n",
        "        \"\"\"\n",
        "\n",
        "        # Show first few tables with column info\n",
        "        tables_to_show = min(3, len(chain['tables']))\n",
        "        for i in range(tables_to_show):\n",
        "            table = chain['tables'][i]\n",
        "            year = chain['years'][i] if i < len(chain['years']) else 'N/A'\n",
        "            header = chain['headers'][i] if i < len(chain['headers']) else 'No header'\n",
        "            columns = chain.get('columns', [])[i] if i < len(chain.get('columns', [])) else []\n",
        "\n",
        "            # Clean header for display\n",
        "            clean_header = header.replace('\\n', ' ')[:100]\n",
        "            if len(header) > 100:\n",
        "                clean_header += '...'\n",
        "\n",
        "            html += f\"\"\"<div class=\"table-info\">\n",
        "                <strong>{table}</strong> ({year}): {clean_header}\n",
        "            \"\"\"\n",
        "\n",
        "            # Show column information if available\n",
        "            if columns:\n",
        "                col_preview = ', '.join(columns[:5])\n",
        "                if len(columns) > 5:\n",
        "                    col_preview += f', ... ({len(columns)} total)'\n",
        "                else:\n",
        "                    col_preview += f' ({len(columns)} total)'\n",
        "                html += f'<div class=\"columns\">Columns: {col_preview}</div>'\n",
        "\n",
        "            html += \"</div>\"\n",
        "\n",
        "        if len(chain['tables']) > tables_to_show:\n",
        "            html += f\"<div class='table-info'>... and {len(chain['tables']) - tables_to_show} more tables</div>\"\n",
        "\n",
        "        # Show gaps if any\n",
        "        if chain.get('gaps'):\n",
        "            html += f\"<div class='table-info' style='color: #FF9800;'>Gaps in years: {', '.join(map(str, chain['gaps']))}</div>\"\n",
        "\n",
        "        html += \"</div>\"\n",
        "        return html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7GiPDUqtqCF"
      },
      "source": [
        "# Real Embeddings with Sentence Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObMPx30rtsf9",
        "outputId": "97057b4f-bcf1-4083-d554-55a9635b5a85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing real_embeddings.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile real_embeddings.py\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import hashlib\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    TRANSFORMER_AVAILABLE = True\n",
        "except:\n",
        "    TRANSFORMER_AVAILABLE = False\n",
        "    print(\"Install with: !pip install sentence-transformers\")\n",
        "\n",
        "class RealEmbeddingGenerator:\n",
        "    def __init__(self, model_name=\"sentence-transformers/LaBSE\", cache_dir=\"cache\"):\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        self.embedding_cache = {}\n",
        "\n",
        "        if TRANSFORMER_AVAILABLE:\n",
        "            self.model = SentenceTransformer(model_name)\n",
        "            self.dimension = self.model.get_sentence_embedding_dimension()\n",
        "        else:\n",
        "            self.model = None\n",
        "            self.dimension = 768\n",
        "\n",
        "    def get_text_hash(self, text):\n",
        "        return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
        "\n",
        "    def generate_embedding(self, text, use_cache=True):\n",
        "        text_hash = self.get_text_hash(text)\n",
        "\n",
        "        if use_cache and text_hash in self.embedding_cache:\n",
        "            return self.embedding_cache[text_hash]\n",
        "\n",
        "        if self.model:\n",
        "            embedding = self.model.encode(text, convert_to_numpy=True)\n",
        "        else:\n",
        "            # Fallback to deterministic random\n",
        "            np.random.seed(int(text_hash[:8], 16) % 10000)\n",
        "            embedding = np.random.randn(self.dimension)\n",
        "\n",
        "        if use_cache:\n",
        "            self.embedding_cache[text_hash] = embedding\n",
        "\n",
        "        return embedding\n",
        "\n",
        "    def generate_batch(self, texts, show_progress=True):\n",
        "        if self.model:\n",
        "            return self.model.encode(texts,\n",
        "                                    batch_size=32,\n",
        "                                    show_progress_bar=show_progress,\n",
        "                                    convert_to_numpy=True)\n",
        "        else:\n",
        "            return np.array([self.generate_embedding(t) for t in texts])\n",
        "\n",
        "    def save_cache(self):\n",
        "        cache_file = os.path.join(self.cache_dir, \"embedding_cache.pkl\")\n",
        "        with open(cache_file, 'wb') as f:\n",
        "            pickle.dump(self.embedding_cache, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug18NH--uhBB"
      },
      "source": [
        "# Claude API Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXAxHzOFuj2H",
        "outputId": "9403608d-00fd-4a8d-c3ea-f958a3fa94c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing api_validator.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile api_validator.py\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "\n",
        "class ClaudeAPIValidator:\n",
        "    def __init__(self, api_key=None):\n",
        "        self.api_key = api_key or os.getenv('CLAUDE_API_KEY')\n",
        "        self.has_api = bool(self.api_key)\n",
        "        self.validation_count = 0\n",
        "\n",
        "    def validate_edge_case(self, chain_headers, table_header, similarity):\n",
        "        \"\"\"Validate uncertain match (0.85-0.97)\"\"\"\n",
        "        self.validation_count += 1\n",
        "\n",
        "        if self.has_api:\n",
        "            return self._real_api_call(chain_headers, table_header, similarity)\n",
        "        else:\n",
        "            return self._mock_validation(similarity)\n",
        "\n",
        "    def _mock_validation(self, similarity):\n",
        "        \"\"\"Mock validation for testing\"\"\"\n",
        "        if similarity >= 0.92:\n",
        "            return {'decision': 'accept', 'confidence': 0.9, 'reasoning': 'High similarity'}\n",
        "        elif similarity >= 0.88:\n",
        "            return {'decision': 'uncertain', 'confidence': 0.6, 'reasoning': 'Moderate similarity'}\n",
        "        else:\n",
        "            return {'decision': 'reject', 'confidence': 0.8, 'reasoning': 'Low similarity'}\n",
        "\n",
        "    def _real_api_call(self, chain_headers, table_header, similarity):\n",
        "        \"\"\"Real API call (if implemented)\"\"\"\n",
        "        # Placeholder for real Claude API implementation\n",
        "        prompt = f\"\"\"\n",
        "        Chain history: {chain_headers}\n",
        "        New table: {table_header}\n",
        "        Similarity: {similarity}\n",
        "        Should these match?\n",
        "        \"\"\"\n",
        "\n",
        "        # Would make actual API call here\n",
        "        return self._mock_validation(similarity)\n",
        "\n",
        "    def validate_conflict(self, table_header, competing_chains):\n",
        "        \"\"\"Resolve conflicts between multiple chains\"\"\"\n",
        "        if self.has_api:\n",
        "            # Real API logic\n",
        "            pass\n",
        "        else:\n",
        "            # Mock: choose highest similarity\n",
        "            best_chain = max(competing_chains, key=lambda x: x[1])\n",
        "            return {\n",
        "                'winning_chain': best_chain[0],\n",
        "                'confidence': 0.8,\n",
        "                'reasoning': 'Highest similarity score'\n",
        "            }\n",
        "\n",
        "    def validate_split(self, source_chain, target_tables):\n",
        "        \"\"\"Validate potential split\"\"\"\n",
        "        if len(target_tables) >= 2:\n",
        "            return {\n",
        "                'decision': 'accept',\n",
        "                'split_type': 'even_split' if len(target_tables) == 2 else 'fragmentation',\n",
        "                'confidence': 0.7,\n",
        "                'targets': [t[0] for t in target_tables[:3]]\n",
        "            }\n",
        "        return {'decision': 'reject', 'confidence': 0.9}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzTMhFh8umDM"
      },
      "source": [
        "# Gap Handler with Reactivation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgagA_ZTuqAB",
        "outputId": "19dae3f6-768c-49cb-e2c3-3c39020eaad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing gap_handler.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile gap_handler.py\n",
        "import numpy as np\n",
        "\n",
        "class GapHandler:\n",
        "    def __init__(self, max_gap_years=3, reactivation_threshold=0.90):\n",
        "        self.max_gap_years = max_gap_years\n",
        "        self.reactivation_threshold = reactivation_threshold\n",
        "        self.dormant_chains = {}\n",
        "        self.ended_chains = {}\n",
        "\n",
        "    def check_gaps(self, chains, current_year, matched_chains):\n",
        "        \"\"\"Check for gaps and handle dormant chains\"\"\"\n",
        "        gap_report = {\n",
        "            'new_dormant': [],\n",
        "            'reactivated': [],\n",
        "            'ended': [],\n",
        "            'continuing_gaps': []\n",
        "        }\n",
        "\n",
        "        for chain_id, chain in chains.items():\n",
        "            if chain['status'] == 'active' and chain_id not in matched_chains:\n",
        "                # Chain has no match this year\n",
        "                last_year = chain['years'][-1] if chain['years'] else 0\n",
        "                gap_length = current_year - last_year\n",
        "\n",
        "                if gap_length > self.max_gap_years:\n",
        "                    # End chain\n",
        "                    chain['status'] = 'ended'\n",
        "                    self.ended_chains[chain_id] = chain\n",
        "                    gap_report['ended'].append(chain_id)\n",
        "                else:\n",
        "                    # Mark dormant\n",
        "                    chain['status'] = 'dormant'\n",
        "                    chain['dormant_since'] = current_year\n",
        "                    self.dormant_chains[chain_id] = chain\n",
        "                    gap_report['new_dormant'].append(chain_id)\n",
        "\n",
        "        return gap_report\n",
        "\n",
        "    def check_reactivation(self, dormant_chain, new_tables, embeddings):\n",
        "        \"\"\"Check if dormant chain can be reactivated\"\"\"\n",
        "        if dormant_chain['tables']:\n",
        "            last_table = dormant_chain['tables'][-1]\n",
        "            if last_table in embeddings:\n",
        "                chain_emb = embeddings[last_table]\n",
        "\n",
        "                candidates = []\n",
        "                for table_id in new_tables:\n",
        "                    if table_id in embeddings:\n",
        "                        table_emb = embeddings[table_id]\n",
        "                        similarity = self._compute_similarity(chain_emb, table_emb)\n",
        "\n",
        "                        if similarity >= self.reactivation_threshold:\n",
        "                            candidates.append((table_id, similarity))\n",
        "\n",
        "                if candidates:\n",
        "                    return max(candidates, key=lambda x: x[1])\n",
        "        return None\n",
        "\n",
        "    def _compute_similarity(self, emb1, emb2):\n",
        "        \"\"\"Compute cosine similarity\"\"\"\n",
        "        from scipy.spatial.distance import cosine\n",
        "        return (1 - cosine(emb1, emb2) + 1) / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE8feNCYurft"
      },
      "source": [
        "# Storage and Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otP5a6oRuy6K",
        "outputId": "cc6921a4-4bb2-495a-db23-ae38f8c70de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing storage_manager.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile storage_manager.py\n",
        "import json\n",
        "import pickle\n",
        "import gzip\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "class StorageManager:\n",
        "    def __init__(self, storage_dir=\"chain_storage\"):\n",
        "        self.storage_dir = Path(storage_dir)\n",
        "        self.storage_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Create subdirectories\n",
        "        (self.storage_dir / \"checkpoints\").mkdir(exist_ok=True)\n",
        "        (self.storage_dir / \"backups\").mkdir(exist_ok=True)\n",
        "        (self.storage_dir / \"embeddings\").mkdir(exist_ok=True)\n",
        "\n",
        "    def save_checkpoint(self, year, chains, statistics):\n",
        "        \"\"\"Save processing checkpoint\"\"\"\n",
        "        checkpoint = {\n",
        "            'year': year,\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'chains': chains,\n",
        "            'statistics': statistics\n",
        "        }\n",
        "\n",
        "        filename = f\"checkpoint_{year}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "        filepath = self.storage_dir / \"checkpoints\" / filename\n",
        "\n",
        "        with gzip.open(filepath.with_suffix('.json.gz'), 'wt', encoding='utf-8') as f:\n",
        "            json.dump(checkpoint, f, indent=2)\n",
        "\n",
        "        return str(filepath)\n",
        "\n",
        "    def load_checkpoint(self, year):\n",
        "        \"\"\"Load latest checkpoint for a year\"\"\"\n",
        "        checkpoint_dir = self.storage_dir / \"checkpoints\"\n",
        "        pattern = f\"checkpoint_{year}_*.json.gz\"\n",
        "\n",
        "        files = list(checkpoint_dir.glob(pattern))\n",
        "        if files:\n",
        "            latest = max(files, key=lambda f: f.stat().st_mtime)\n",
        "            with gzip.open(latest, 'rt', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return None\n",
        "\n",
        "    def save_embeddings(self, embeddings, year):\n",
        "        \"\"\"Save embeddings for a year\"\"\"\n",
        "        filepath = self.storage_dir / \"embeddings\" / f\"embeddings_{year}.pkl\"\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(embeddings, f)\n",
        "\n",
        "    def load_embeddings(self, year):\n",
        "        \"\"\"Load embeddings for a year\"\"\"\n",
        "        filepath = self.storage_dir / \"embeddings\" / f\"embeddings_{year}.pkl\"\n",
        "        if filepath.exists():\n",
        "            with open(filepath, 'rb') as f:\n",
        "                return pickle.load(f)\n",
        "        return None\n",
        "\n",
        "    def backup_chains(self, chains):\n",
        "        \"\"\"Create backup of chains\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        backup_file = self.storage_dir / \"backups\" / f\"chains_backup_{timestamp}.json.gz\"\n",
        "\n",
        "        with gzip.open(backup_file, 'wt', encoding='utf-8') as f:\n",
        "            json.dump(chains, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkIOfa3Mu1SK"
      },
      "source": [
        "# Comprehensive Statistics Tracker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5ZsVDL6u4rP",
        "outputId": "0a343764-f20c-427e-a53d-b84cccf4a4a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing statistics_tracker.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile statistics_tracker.py\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "\n",
        "class StatisticsTracker:\n",
        "    def __init__(self):\n",
        "        self.match_history = []\n",
        "        self.year_statistics = {}\n",
        "        self.chain_statistics = defaultdict(lambda: {\n",
        "            'length': 0,\n",
        "            'gaps': [],\n",
        "            'similarity_scores': [],\n",
        "            'api_validations': 0\n",
        "        })\n",
        "\n",
        "        self.global_stats = {\n",
        "            'total_years_processed': 0,\n",
        "            'total_matches': 0,\n",
        "            'total_chains': 0,\n",
        "            'total_api_calls': 0,\n",
        "            'total_splits': 0,\n",
        "            'total_merges': 0\n",
        "        }\n",
        "\n",
        "        self.similarity_distributions = defaultdict(list)\n",
        "\n",
        "    def record_match(self, chain_id, table_id, year, similarity, match_type='confident'):\n",
        "        \"\"\"Record a single match\"\"\"\n",
        "        self.match_history.append({\n",
        "            'chain': chain_id,\n",
        "            'table': table_id,\n",
        "            'year': year,\n",
        "            'similarity': similarity,\n",
        "            'type': match_type,\n",
        "            'timestamp': str(datetime.now())\n",
        "        })\n",
        "\n",
        "        self.chain_statistics[chain_id]['length'] += 1\n",
        "        self.chain_statistics[chain_id]['similarity_scores'].append(similarity)\n",
        "        self.similarity_distributions[year].append(similarity)\n",
        "        self.global_stats['total_matches'] += 1\n",
        "\n",
        "    def record_year(self, year, tables_count, matches_count,\n",
        "                    unmatched_tables, unmatched_chains, processing_time):\n",
        "        \"\"\"Record year statistics\"\"\"\n",
        "        self.year_statistics[year] = {\n",
        "            'tables': tables_count,\n",
        "            'matches': matches_count,\n",
        "            'unmatched_tables': len(unmatched_tables),\n",
        "            'unmatched_chains': len(unmatched_chains),\n",
        "            'match_rate': matches_count / tables_count if tables_count > 0 else 0,\n",
        "            'processing_time': processing_time,\n",
        "            'similarity_distribution': {}\n",
        "        }\n",
        "\n",
        "        if year in self.similarity_distributions:\n",
        "            scores = self.similarity_distributions[year]\n",
        "            self.year_statistics[year]['similarity_distribution'] = {\n",
        "                'mean': float(np.mean(scores)),\n",
        "                'median': float(np.median(scores)),\n",
        "                'std': float(np.std(scores)),\n",
        "                'min': float(np.min(scores)),\n",
        "                'max': float(np.max(scores))\n",
        "            }\n",
        "\n",
        "        self.global_stats['total_years_processed'] += 1\n",
        "\n",
        "    def get_summary(self):\n",
        "        \"\"\"Get comprehensive summary\"\"\"\n",
        "        chain_lengths = [s['length'] for s in self.chain_statistics.values()]\n",
        "\n",
        "        return {\n",
        "            'overview': {\n",
        "                'total_years': self.global_stats['total_years_processed'],\n",
        "                'total_matches': self.global_stats['total_matches'],\n",
        "                'total_chains': len(self.chain_statistics),\n",
        "                'match_rate': f\"{np.mean([y['match_rate'] for y in self.year_statistics.values()])*100:.1f}%\" if self.year_statistics else \"0%\"\n",
        "            },\n",
        "            'chain_statistics': {\n",
        "                'average_length': np.mean(chain_lengths) if chain_lengths else 0,\n",
        "                'max_length': max(chain_lengths) if chain_lengths else 0,\n",
        "                'min_length': min(chain_lengths) if chain_lengths else 0,\n",
        "                'chains_with_gaps': sum(1 for c in self.chain_statistics.values() if c['gaps'])\n",
        "            },\n",
        "            'year_by_year': {\n",
        "                year: {\n",
        "                    'tables': stats['tables'],\n",
        "                    'matches': stats['matches'],\n",
        "                    'match_rate': f\"{stats['match_rate']*100:.1f}%\",\n",
        "                    'processing_time': f\"{stats['processing_time']:.2f}s\"\n",
        "                }\n",
        "                for year, stats in self.year_statistics.items()\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ3opGYVu6gH"
      },
      "source": [
        "# Visualization Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l5FumHPu9c8",
        "outputId": "f04271fa-9eba-4e89-acf1-0a461e9f912a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing visualization.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile visualization.py\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "try:\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    PLOTLY_AVAILABLE = True\n",
        "except:\n",
        "    PLOTLY_AVAILABLE = False\n",
        "\n",
        "\n",
        "class VisualizationGenerator:\n",
        "    def __init__(self):\n",
        "        self.colors = ['#4CAF50', '#FF9800', '#9C27B0', '#F44336', '#2196F3']\n",
        "\n",
        "    def create_sankey(self, chains, sim_matrix_data=None):\n",
        "        \"\"\"Create enhanced Sankey diagram with similarity scores and correlation matrix\"\"\"\n",
        "        if not PLOTLY_AVAILABLE:\n",
        "            print(\"Install plotly: !pip install plotly\")\n",
        "            return None\n",
        "\n",
        "        # Create subplots - Sankey on top, heatmap below\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=1,\n",
        "            row_heights=[0.7, 0.3],\n",
        "            specs=[[{\"type\": \"sankey\"}],\n",
        "                   [{\"type\": \"heatmap\"}]],\n",
        "            subplot_titles=(\"Table Chain Evolution\", \"Similarity Matrix\")\n",
        "        )\n",
        "\n",
        "        # Build Sankey data\n",
        "        nodes = []\n",
        "        node_labels = []\n",
        "        sources = []\n",
        "        targets = []\n",
        "        values = []\n",
        "        link_labels = []\n",
        "        link_colors = []\n",
        "\n",
        "        node_map = {}\n",
        "        header_map = {}\n",
        "        columns_map = {}  # New: track columns for hover text\n",
        "        node_idx = 0\n",
        "\n",
        "        for chain_id, chain in chains.items():\n",
        "            for i, table in enumerate(chain['tables']):\n",
        "                if table not in node_map:\n",
        "                    node_map[table] = node_idx\n",
        "                    header = chain['headers'][i] if i < len(chain['headers']) else 'No header'\n",
        "                    header_map[table] = header\n",
        "\n",
        "                    # Get column information\n",
        "                    columns = chain.get('columns', [])[i] if i < len(chain.get('columns', [])) else []\n",
        "                    columns_map[table] = columns\n",
        "\n",
        "                    clean_header = header.replace('\\n', ' ')[:50] + '...' if len(header) > 50 else header.replace('\\n', ' ')\n",
        "\n",
        "                    # Enhanced node label with column count\n",
        "                    col_info = f\"<br>Columns: {len(columns)}\" if columns else \"\"\n",
        "                    node_labels.append(f\"{table}<br>Year: {chain['years'][i]}<br>{clean_header}{col_info}\")\n",
        "                    node_idx += 1\n",
        "\n",
        "                if i > 0:\n",
        "                    prev_table = chain['tables'][i-1]\n",
        "                    sources.append(node_map[prev_table])\n",
        "                    targets.append(node_map[table])\n",
        "                    values.append(1)\n",
        "\n",
        "                    # Get similarity and API info if available\n",
        "                    similarity = chain.get('similarities', [])[i-1] if i-1 < len(chain.get('similarities', [])) else 0.95\n",
        "                    api_used = chain.get('api_validated', [])[i-1] if i-1 < len(chain.get('api_validated', [])) else False\n",
        "\n",
        "                    # Create detailed hover text with column preview\n",
        "                    source_header = header_map.get(prev_table, 'No header')\n",
        "                    target_header = header_map.get(table, 'No header')\n",
        "                    api_text = \"✓ API Validated\" if api_used else \"Auto-matched\"\n",
        "\n",
        "                    # Add column information if available\n",
        "                    source_cols = columns_map.get(prev_table, [])\n",
        "                    target_cols = columns_map.get(table, [])\n",
        "\n",
        "                    col_preview_source = \"\"\n",
        "                    col_preview_target = \"\"\n",
        "\n",
        "                    if source_cols:\n",
        "                        col_preview_source = f\"<br>Columns ({len(source_cols)}): {', '.join(source_cols[:3])}\"\n",
        "                        if len(source_cols) > 3:\n",
        "                            col_preview_source += \"...\"\n",
        "\n",
        "                    if target_cols:\n",
        "                        col_preview_target = f\"<br>Columns ({len(target_cols)}): {', '.join(target_cols[:3])}\"\n",
        "                        if len(target_cols) > 3:\n",
        "                            col_preview_target += \"...\"\n",
        "\n",
        "                    hover_text = (f\"<b>Similarity: {similarity:.3f}</b><br>\"\n",
        "                                f\"{api_text}<br><br>\"\n",
        "                                f\"<b>Source:</b> {prev_table}<br>{source_header}{col_preview_source}<br><br>\"\n",
        "                                f\"<b>Target:</b> {table}<br>{target_header}{col_preview_target}\")\n",
        "                    link_labels.append(hover_text)\n",
        "\n",
        "                    # Color based on similarity\n",
        "                    if similarity >= 0.97:\n",
        "                        color = 'rgba(76, 175, 80, 0.5)'  # Green\n",
        "                    elif similarity >= 0.90:\n",
        "                        color = 'rgba(255, 193, 7, 0.5)'  # Amber\n",
        "                    elif similarity >= 0.85:\n",
        "                        color = 'rgba(255, 152, 0, 0.5)'  # Orange\n",
        "                    else:\n",
        "                        color = 'rgba(244, 67, 54, 0.5)'  # Red\n",
        "\n",
        "                    if api_used:\n",
        "                        color = color.replace('0.5', '0.8')  # Darker if API validated\n",
        "\n",
        "                    link_colors.append(color)\n",
        "\n",
        "        # Add Sankey to subplot\n",
        "        sankey = go.Sankey(\n",
        "            node=dict(\n",
        "                pad=15,\n",
        "                thickness=20,\n",
        "                line=dict(color=\"black\", width=0.5),\n",
        "                label=node_labels,\n",
        "                hovertemplate='%{label}<extra></extra>'\n",
        "            ),\n",
        "            link=dict(\n",
        "                source=sources,\n",
        "                target=targets,\n",
        "                value=values,\n",
        "                label=link_labels,\n",
        "                color=link_colors,\n",
        "                hovertemplate='%{label}<extra></extra>'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        fig.add_trace(sankey, row=1, col=1)\n",
        "\n",
        "        # Add similarity matrix heatmap if available\n",
        "        if sim_matrix_data and 'matrix' in sim_matrix_data:\n",
        "            matrix = sim_matrix_data['matrix']\n",
        "            chain_ids = [c.split('_')[-1] if c.startswith('chain_') else c\n",
        "                        for c in sim_matrix_data.get('chain_ids', [])]\n",
        "            table_ids = sim_matrix_data.get('table_ids', [])\n",
        "\n",
        "            heatmap = go.Heatmap(\n",
        "                z=matrix,\n",
        "                x=table_ids,\n",
        "                y=chain_ids,\n",
        "                colorscale='RdYlGn',\n",
        "                zmin=0,\n",
        "                zmax=1,\n",
        "                text=np.round(matrix, 3),\n",
        "                texttemplate='%{text}',\n",
        "                textfont={\"size\": 8},\n",
        "                hovertemplate='Chain: %{y}<br>Table: %{x}<br>Similarity: %{z:.3f}<extra></extra>',\n",
        "                colorbar=dict(title=\"Similarity\", len=0.3, y=0.15)\n",
        "            )\n",
        "\n",
        "            fig.add_trace(heatmap, row=2, col=1)\n",
        "\n",
        "        # Update layout\n",
        "        fig.update_layout(\n",
        "            title_text=\"Table Chain Analysis with Similarity Metrics and Column Information\",\n",
        "            height=1000,\n",
        "            width=1400,\n",
        "            showlegend=False,\n",
        "            font_size=10\n",
        "        )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def save_graph_json(self, chains, filepath=\"graph.json\"):\n",
        "        \"\"\"Save graph structure as JSON with similarity scores and column information\"\"\"\n",
        "        graph = {\n",
        "            'nodes': [],\n",
        "            'edges': [],\n",
        "            'metadata': {\n",
        "                'created': str(datetime.now()),\n",
        "                'total_chains': len(chains),\n",
        "                'active_chains': sum(1 for c in chains.values() if c['status'] == 'active')\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for chain_id, chain in chains.items():\n",
        "            for i, table in enumerate(chain['tables']):\n",
        "                # Include column information in node data\n",
        "                columns = chain.get('columns', [])[i] if i < len(chain.get('columns', [])) else []\n",
        "\n",
        "                graph['nodes'].append({\n",
        "                    'id': table,\n",
        "                    'chain': chain_id,\n",
        "                    'year': chain['years'][i] if i < len(chain['years']) else 0,\n",
        "                    'header': chain['headers'][i] if i < len(chain['headers']) else '',\n",
        "                    'columns': columns,  # New: include column information\n",
        "                    'column_count': len(columns)  # New: include column count for easy reference\n",
        "                })\n",
        "\n",
        "                if i > 0:\n",
        "                    similarity = chain.get('similarities', [])[i-1] if i-1 < len(chain.get('similarities', [])) else None\n",
        "                    api_used = chain.get('api_validated', [])[i-1] if i-1 < len(chain.get('api_validated', [])) else False\n",
        "\n",
        "                    graph['edges'].append({\n",
        "                        'source': chain['tables'][i-1],\n",
        "                        'target': table,\n",
        "                        'type': 'continuation',\n",
        "                        'similarity': similarity,\n",
        "                        'api_validated': api_used\n",
        "                    })\n",
        "\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(graph, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        return filepath"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frx1uxYwu_PD"
      },
      "source": [
        "# Complex N:N Relationships"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlfTxiOjvChy",
        "outputId": "2d0a738a-b28f-4c92-cb81-ee7425e97081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing complex_relationships.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile complex_relationships.py\n",
        "import numpy as np\n",
        "from enum import Enum\n",
        "from datetime import datetime\n",
        "\n",
        "class RelationshipType(Enum):\n",
        "    ONE_TO_ONE = \"1:1\"\n",
        "    ONE_TO_MANY = \"1:N\"\n",
        "    MANY_TO_ONE = \"N:1\"\n",
        "    MANY_TO_MANY = \"N:N\"\n",
        "\n",
        "class ComplexRelationshipDetector:\n",
        "    def __init__(self):\n",
        "        self.complex_relationships = []\n",
        "\n",
        "    def detect_complex(self, sim_matrix, splits, merges):\n",
        "        \"\"\"Detect N:N complex reorganizations\"\"\"\n",
        "        split_tables = set()\n",
        "        for split in splits:\n",
        "            split_tables.update([t[0] for t in split['targets']])\n",
        "\n",
        "        merge_chains = set()\n",
        "        for merge in merges:\n",
        "            merge_chains.update([c[0] for c in merge['sources']])\n",
        "\n",
        "        # Find overlapping splits and merges (N:N)\n",
        "        for split in splits:\n",
        "            if split['chain'] in merge_chains:\n",
        "                for merge in merges:\n",
        "                    if split['chain'] in [c[0] for c in merge['sources']]:\n",
        "                        self.complex_relationships.append({\n",
        "                            'type': RelationshipType.MANY_TO_MANY,\n",
        "                            'chains': list(set([split['chain']] + [c[0] for c in merge['sources']])),\n",
        "                            'tables': list(set([merge['table']] + [t[0] for t in split['targets']])),\n",
        "                            'confidence': 0.7\n",
        "                        })\n",
        "\n",
        "        return self.complex_relationships"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3fGHHlGvG_M"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTWgkjVfv6Qc"
      },
      "source": [
        "# NetworkX Graph Builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0s2I0c6vH4S",
        "outputId": "a3ff0583-3860-49fe-f1f1-cf33987ef6f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing networkx_builder.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile networkx_builder.py\n",
        "try:\n",
        "    import networkx as nx\n",
        "    NX_AVAILABLE = True\n",
        "except:\n",
        "    NX_AVAILABLE = False\n",
        "\n",
        "class NetworkXGraphBuilder:\n",
        "    def __init__(self):\n",
        "        self.G = None if not NX_AVAILABLE else nx.DiGraph()\n",
        "\n",
        "    def build_graph(self, chains):\n",
        "        \"\"\"Build complete NetworkX graph\"\"\"\n",
        "        if not NX_AVAILABLE:\n",
        "            return None\n",
        "\n",
        "        self.G = nx.DiGraph()\n",
        "\n",
        "        # Add all nodes\n",
        "        for chain_id, chain in chains.items():\n",
        "            for i, table in enumerate(chain['tables']):\n",
        "                self.G.add_node(table,\n",
        "                              chain=chain_id,\n",
        "                              year=chain['years'][i] if i < len(chain['years']) else 0,\n",
        "                              header=chain['headers'][i] if i < len(chain['headers']) else '')\n",
        "\n",
        "        # Add edges\n",
        "        for chain_id, chain in chains.items():\n",
        "            for i in range(1, len(chain['tables'])):\n",
        "                self.G.add_edge(chain['tables'][i-1],\n",
        "                              chain['tables'][i],\n",
        "                              weight=1.0,\n",
        "                              type='continuation')\n",
        "\n",
        "        return self.G\n",
        "\n",
        "    def analyze_graph(self):\n",
        "        \"\"\"Analyze graph properties\"\"\"\n",
        "        if not self.G:\n",
        "            return {}\n",
        "\n",
        "        return {\n",
        "            'nodes': self.G.number_of_nodes(),\n",
        "            'edges': self.G.number_of_edges(),\n",
        "            'connected_components': nx.number_weakly_connected_components(self.G),\n",
        "            'average_degree': sum(dict(self.G.degree()).values()) / self.G.number_of_nodes()\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v16oh1Mv_OF"
      },
      "source": [
        "# Full Conflict Resolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOj4PsAWwGYz",
        "outputId": "ae2ed07c-f802-45f5-9566-473fa3aaa573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing conflict_resolver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile conflict_resolver.py\n",
        "class ConflictResolver:\n",
        "    def __init__(self):\n",
        "        self.conflicts = {}\n",
        "        self.resolutions = {}\n",
        "\n",
        "    def detect_conflicts(self, sim_matrix, threshold=0.85):\n",
        "        \"\"\"Detect all conflicts\"\"\"\n",
        "        matrix = sim_matrix['matrix']\n",
        "        chain_ids = sim_matrix['chain_ids']\n",
        "        table_ids = sim_matrix['table_ids']\n",
        "\n",
        "        for j, table_id in enumerate(table_ids):\n",
        "            claimants = []\n",
        "            for i, chain_id in enumerate(chain_ids):\n",
        "                if matrix[i, j] >= threshold:\n",
        "                    claimants.append((chain_id, matrix[i, j]))\n",
        "\n",
        "            if len(claimants) > 1:\n",
        "                self.conflicts[table_id] = {\n",
        "                    'claimants': claimants,\n",
        "                    'max_similarity': max(c[1] for c in claimants)\n",
        "                }\n",
        "\n",
        "        return self.conflicts\n",
        "\n",
        "    def resolve_conflicts(self, conflicts, api_validator=None):\n",
        "        \"\"\"Resolve all conflicts\"\"\"\n",
        "        for table_id, conflict in conflicts.items():\n",
        "            if api_validator:\n",
        "                resolution = api_validator.validate_conflict(\n",
        "                    table_id, conflict['claimants']\n",
        "                )\n",
        "                self.resolutions[table_id] = resolution\n",
        "            else:\n",
        "                # Default: highest similarity wins\n",
        "                winner = max(conflict['claimants'], key=lambda x: x[1])\n",
        "                self.resolutions[table_id] = {\n",
        "                    'winning_chain': winner[0],\n",
        "                    'confidence': winner[1]\n",
        "                }\n",
        "\n",
        "        return self.resolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0N4-YiWwI5I"
      },
      "source": [
        "# API Response Handler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBzgqdtOwLql",
        "outputId": "26155e9c-3dac-42da-f667-97bb85412ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing response_handler.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile response_handler.py\n",
        "from enum import Enum\n",
        "\n",
        "class DecisionAction(Enum):\n",
        "    CONFIRM = \"confirm\"\n",
        "    REJECT = \"reject\"\n",
        "    SPLIT = \"split\"\n",
        "    MERGE = \"merge\"\n",
        "    MANUAL = \"manual\"\n",
        "\n",
        "class APIResponseHandler:\n",
        "    def __init__(self):\n",
        "        self.decisions = []\n",
        "        self.manual_queue = []\n",
        "\n",
        "    def process_response(self, api_response, match_type):\n",
        "        \"\"\"Process API validation response\"\"\"\n",
        "        decision = api_response.get('decision', 'uncertain')\n",
        "        confidence = api_response.get('confidence', 0.5)\n",
        "\n",
        "        if decision == 'accept' and confidence >= 0.7:\n",
        "            action = DecisionAction.CONFIRM\n",
        "        elif decision == 'reject' and confidence >= 0.7:\n",
        "            action = DecisionAction.REJECT\n",
        "        else:\n",
        "            action = DecisionAction.MANUAL\n",
        "            self.manual_queue.append(api_response)\n",
        "\n",
        "        self.decisions.append({\n",
        "            'action': action,\n",
        "            'confidence': confidence,\n",
        "            'type': match_type,\n",
        "            'response': api_response\n",
        "        })\n",
        "\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d54pESzbwOA_"
      },
      "source": [
        "# Parameter Tuning Suite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7C4qIEhwRHR",
        "outputId": "8c4284ee-e422-4aa3-8e2c-aed7bf91a17c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing parameter_tuner.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile parameter_tuner.py\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "class ParameterTuner:\n",
        "    def __init__(self):\n",
        "        self.param_history = []\n",
        "        self.optimal_params = None\n",
        "\n",
        "    def grid_search(self, param_ranges, validation_data):\n",
        "        \"\"\"Grid search for optimal parameters\"\"\"\n",
        "        best_score = 0\n",
        "        best_params = {}\n",
        "\n",
        "        # Example grid search\n",
        "        for sim_thresh in param_ranges.get('similarity_threshold', [0.85]):\n",
        "            for split_thresh in param_ranges.get('split_threshold', [0.80]):\n",
        "                score = self._evaluate_params({\n",
        "                    'similarity_threshold': sim_thresh,\n",
        "                    'split_threshold': split_thresh\n",
        "                }, validation_data)\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_params = {\n",
        "                        'similarity_threshold': sim_thresh,\n",
        "                        'split_threshold': split_thresh\n",
        "                    }\n",
        "\n",
        "        self.optimal_params = best_params\n",
        "        return best_params\n",
        "\n",
        "    def _evaluate_params(self, params, validation_data):\n",
        "        \"\"\"Evaluate parameter set\"\"\"\n",
        "        # Mock evaluation - in reality would run matching and compare\n",
        "        return np.random.random()\n",
        "\n",
        "    def suggest_adjustments(self, current_stats):\n",
        "        \"\"\"Suggest parameter adjustments based on statistics\"\"\"\n",
        "        suggestions = []\n",
        "\n",
        "        if current_stats.get('match_rate', 0) < 0.7:\n",
        "            suggestions.append(\"Consider lowering similarity_threshold\")\n",
        "\n",
        "        if current_stats.get('false_positives', 0) > 0.1:\n",
        "            suggestions.append(\"Consider raising similarity_threshold\")\n",
        "\n",
        "        return suggestions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XrQ_R71wSto"
      },
      "source": [
        "# Complete Testing Suite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwBF2yr6wVd8",
        "outputId": "3f8361cc-0a84-4228-c11a-3faa00c133ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test_suite.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile test_suite.py\n",
        "import unittest\n",
        "import numpy as np\n",
        "\n",
        "class TestCompleteSystem(unittest.TestCase):\n",
        "    def test_similarity_computation(self):\n",
        "        \"\"\"Test similarity computation\"\"\"\n",
        "        emb1 = np.array([1, 0, 0])\n",
        "        emb2 = np.array([1, 0, 0])\n",
        "        emb3 = np.array([0, 1, 0])\n",
        "\n",
        "        from scipy.spatial.distance import cosine\n",
        "        sim12 = 1 - cosine(emb1, emb2)\n",
        "        sim13 = 1 - cosine(emb1, emb3)\n",
        "\n",
        "        self.assertAlmostEqual(sim12, 1.0)\n",
        "        self.assertAlmostEqual(sim13, 0.0)\n",
        "\n",
        "    def test_hungarian_matching(self):\n",
        "        \"\"\"Test Hungarian algorithm\"\"\"\n",
        "        from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "        cost = np.array([[1, 2], [3, 4]])\n",
        "        row_ind, col_ind = linear_sum_assignment(cost)\n",
        "\n",
        "        self.assertEqual(len(row_ind), 2)\n",
        "        self.assertEqual(len(col_ind), 2)\n",
        "\n",
        "    def test_conflict_detection(self):\n",
        "        \"\"\"Test conflict detection\"\"\"\n",
        "        matrix = np.array([[0.9, 0.3], [0.88, 0.4]])\n",
        "\n",
        "        conflicts = []\n",
        "        for j in range(matrix.shape[1]):\n",
        "            high_sim = []\n",
        "            for i in range(matrix.shape[0]):\n",
        "                if matrix[i, j] >= 0.85:\n",
        "                    high_sim.append(i)\n",
        "            if len(high_sim) > 1:\n",
        "                conflicts.append(j)\n",
        "\n",
        "        self.assertEqual(len(conflicts), 0)  # No conflicts in this example\n",
        "\n",
        "def run_all_tests():\n",
        "    \"\"\"Run complete test suite\"\"\"\n",
        "    loader = unittest.TestLoader()\n",
        "    suite = loader.loadTestsFromTestCase(TestCompleteSystem)\n",
        "    runner = unittest.TextTestRunner(verbosity=2)\n",
        "    result = runner.run(suite)\n",
        "    return result.wasSuccessful()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-LYdbQzxlPG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWi7Be_4wZiB"
      },
      "source": [
        "# Final Complete Orchestrator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Q7_mxo4wciT",
        "outputId": "2d03c6b3-0eb3-4a49-f03e-8edf1456ed28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing final_complete_processor.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile final_complete_processor.py\n",
        "import time\n",
        "import os\n",
        "from datetime import datetime\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Import ALL components\n",
        "from config import MatchingConfig\n",
        "from hebrew_processor import HebrewProcessor\n",
        "from table_loader import TableLoader\n",
        "from real_embeddings import RealEmbeddingGenerator\n",
        "from similarity import SimilarityBuilder\n",
        "from hungarian import HungarianMatcher\n",
        "from split_merge import SplitMergeDetector\n",
        "from complex_relationships import ComplexRelationshipDetector\n",
        "from chains import ChainManager\n",
        "from api_validator import ClaudeAPIValidator\n",
        "from gap_handler import GapHandler\n",
        "from storage_manager import StorageManager\n",
        "from statistics_tracker import StatisticsTracker\n",
        "from visualization import VisualizationGenerator\n",
        "from report_gen import ReportGenerator\n",
        "from networkx_builder import NetworkXGraphBuilder\n",
        "from conflict_resolver import ConflictResolver\n",
        "from response_handler import APIResponseHandler\n",
        "from parameter_tuner import ParameterTuner\n",
        "from test_suite import run_all_tests\n",
        "\n",
        "def process_table_chains_final_complete():\n",
        "    \"\"\"Complete processing with chapter-by-chapter matching and proper validation\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"CHAPTER-BY-CHAPTER TABLE CHAIN MATCHING SYSTEM\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Run tests first\n",
        "    print(\"\\nRunning system tests...\")\n",
        "    tests_passed = run_all_tests()\n",
        "    print(f\"Tests: {'PASSED' if tests_passed else 'FAILED'}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Initialize components\n",
        "    config = MatchingConfig()\n",
        "    config.use_api_validation = True  # Enable API validation\n",
        "    hebrew_proc = HebrewProcessor()\n",
        "\n",
        "    # Initialize loader\n",
        "    loader = TableLoader(\n",
        "        tables_dir=config.tables_dir,\n",
        "        reference_json=config.reference_json,\n",
        "        columns_json=config.columns_json\n",
        "    )\n",
        "\n",
        "    embedder = RealEmbeddingGenerator()\n",
        "    sim_builder = SimilarityBuilder()\n",
        "    matcher = HungarianMatcher(config.similarity_threshold)\n",
        "    split_detector = SplitMergeDetector()\n",
        "    complex_detector = ComplexRelationshipDetector()\n",
        "    api_validator = ClaudeAPIValidator(config.api_key)\n",
        "    gap_handler = GapHandler(config.max_gap_years)\n",
        "    storage_mgr = StorageManager()\n",
        "    stats_tracker = StatisticsTracker()\n",
        "    visualizer = VisualizationGenerator()\n",
        "    reporter = ReportGenerator()\n",
        "    nx_builder = NetworkXGraphBuilder()\n",
        "    conflict_resolver = ConflictResolver()\n",
        "    response_handler = APIResponseHandler()\n",
        "    param_tuner = ParameterTuner()\n",
        "\n",
        "    # Load tables\n",
        "    print(\"\\n1. Loading tables...\")\n",
        "    n_tables = loader.load_metadata()\n",
        "    print(f\"   Loaded {n_tables} tables\")\n",
        "\n",
        "    # Check if reference files exist\n",
        "    if not os.path.exists(config.reference_json):\n",
        "        print(f\"   Warning: {config.reference_json} not found!\")\n",
        "        return None, None\n",
        "\n",
        "    if not os.path.exists(config.columns_json):\n",
        "        print(f\"   Note: {config.columns_json} not found, proceeding without column data\")\n",
        "\n",
        "    # Reorganize tables by chapter and year\n",
        "    tables_by_chapter_year = {}\n",
        "    for tid, metadata in loader.tables_metadata.items():\n",
        "        chapter = metadata['chapter']\n",
        "        year = metadata['year']\n",
        "\n",
        "        if chapter not in tables_by_chapter_year:\n",
        "            tables_by_chapter_year[chapter] = {}\n",
        "        if year not in tables_by_chapter_year[chapter]:\n",
        "            tables_by_chapter_year[chapter][year] = []\n",
        "        tables_by_chapter_year[chapter][year].append(tid)\n",
        "\n",
        "    # TEST MODE: Only process Chapter 1\n",
        "    # Change to list(sorted(tables_by_chapter_year.keys())) for all chapters\n",
        "    chapters_to_process = [1]\n",
        "\n",
        "    print(f\"\\n2. Chapter Organization:\")\n",
        "    print(f\"   Found {len(tables_by_chapter_year)} chapters total\")\n",
        "    print(f\"   Processing chapters: {chapters_to_process}\")\n",
        "\n",
        "    all_chapter_chains = {}\n",
        "    all_chapter_stats = {}\n",
        "\n",
        "    # Process each chapter independently\n",
        "    for chapter in chapters_to_process:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"PROCESSING CHAPTER {chapter}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        if chapter not in tables_by_chapter_year:\n",
        "            print(f\"   No tables found for chapter {chapter}\")\n",
        "            continue\n",
        "\n",
        "        chapter_years = sorted(tables_by_chapter_year[chapter].keys())\n",
        "        print(f\"   Years available: {min(chapter_years)} to {max(chapter_years)}\")\n",
        "\n",
        "        # Initialize fresh components for this chapter\n",
        "        chain_mgr = ChainManager()\n",
        "        chapter_stats = StatisticsTracker()\n",
        "\n",
        "        # Initialize chains with first year\n",
        "        first_year = chapter_years[0]\n",
        "        first_year_tables = {tid: loader.tables_metadata[tid]\n",
        "                            for tid in tables_by_chapter_year[chapter][first_year]}\n",
        "\n",
        "        chain_mgr.initialize_from_first_year(first_year_tables)\n",
        "        print(f\"   Initialized {len(chain_mgr.chains)} chains for Year {first_year}\")\n",
        "\n",
        "        # Generate embeddings for all tables in this chapter\n",
        "        print(f\"\\n   Generating embeddings for chapter {chapter}...\")\n",
        "        chapter_embeddings = {}\n",
        "\n",
        "        for year in chapter_years:\n",
        "            year_count = 0\n",
        "            for tid in tables_by_chapter_year[chapter][year]:\n",
        "                text = hebrew_proc.process_header(loader.tables_metadata[tid]['header'])\n",
        "                embedding = embedder.generate_embedding(text)\n",
        "                chapter_embeddings[tid] = embedding\n",
        "                year_count += 1\n",
        "            print(f\"      Year {year}: {year_count} embeddings\")\n",
        "\n",
        "        last_sim_matrix = None\n",
        "\n",
        "        # Process each subsequent year for this chapter\n",
        "        for year in chapter_years[1:]:\n",
        "            print(f\"\\n   Processing Chapter {chapter}, Year {year}...\")\n",
        "            year_start = time.time()\n",
        "\n",
        "            # Get embeddings for matching\n",
        "            chain_embeddings = chain_mgr.get_chain_embeddings(chapter_embeddings)\n",
        "            table_embeddings = {tid: chapter_embeddings[tid]\n",
        "                              for tid in tables_by_chapter_year[chapter][year]\n",
        "                              if tid in chapter_embeddings}\n",
        "\n",
        "            print(f\"      Active chains: {len(chain_embeddings)}, Tables: {len(table_embeddings)}\")\n",
        "\n",
        "            if not table_embeddings:\n",
        "                print(f\"      No tables to match for year {year}\")\n",
        "                continue\n",
        "\n",
        "            # Build similarity matrix\n",
        "            sim_matrix = sim_builder.compute_similarity_matrix(\n",
        "                chain_embeddings, table_embeddings\n",
        "            )\n",
        "            last_sim_matrix = sim_matrix\n",
        "\n",
        "            # Detect conflicts\n",
        "            conflicts = conflict_resolver.detect_conflicts(sim_matrix)\n",
        "            if conflicts:\n",
        "                print(f\"      Conflicts detected: {len(conflicts)}\")\n",
        "                resolutions = conflict_resolver.resolve_conflicts(conflicts, api_validator)\n",
        "\n",
        "            # Hungarian matching\n",
        "            matching_result = matcher.find_optimal_matching(sim_matrix)\n",
        "            print(f\"      Initial matches found: {len(matching_result['matches'])}\")\n",
        "\n",
        "            # Detect splits and merges\n",
        "            splits = split_detector.detect_splits(sim_matrix)\n",
        "            merges = split_detector.detect_merges(sim_matrix)\n",
        "            complex_rels = complex_detector.detect_complex(sim_matrix, splits, merges)\n",
        "\n",
        "            if splits:\n",
        "                print(f\"      Splits detected: {len(splits)}\")\n",
        "            if merges:\n",
        "                print(f\"      Merges detected: {len(merges)}\")\n",
        "            if complex_rels:\n",
        "                print(f\"      Complex N:N relationships: {len(complex_rels)}\")\n",
        "\n",
        "            # FIX 1: Proper API validation with rejection of low-confidence matches\n",
        "            validated_matches = []\n",
        "            for chain_id, table_id, similarity in matching_result['matches']:\n",
        "                # Reject low confidence matches unless API confirms\n",
        "                if similarity < 0.97:\n",
        "                    if config.use_api_validation and 0.85 <= similarity:\n",
        "                        validation = api_validator.validate_edge_case(\n",
        "                            chain_mgr.chains[chain_id]['headers'],\n",
        "                            loader.tables_metadata[table_id]['header'],\n",
        "                            similarity\n",
        "                        )\n",
        "                        action = response_handler.process_response(validation, 'edge_case')\n",
        "                        if action.value == 'confirm':\n",
        "                            validated_matches.append({\n",
        "                                'chain_id': chain_id,\n",
        "                                'table_id': table_id,\n",
        "                                'similarity': similarity,\n",
        "                                'api_validated': True\n",
        "                            })\n",
        "                            print(f\"      API confirmed: {chain_id} -> {table_id} (sim={similarity:.3f})\")\n",
        "                        else:\n",
        "                            print(f\"      API rejected: {chain_id} -> {table_id} (sim={similarity:.3f})\")\n",
        "                            # Mark table as unmatched since it was rejected\n",
        "                            if table_id not in matching_result['unmatched_tables']:\n",
        "                                matching_result['unmatched_tables'].append(table_id)\n",
        "                    else:\n",
        "                        print(f\"      Rejected low confidence: {chain_id} -> {table_id} (sim={similarity:.3f})\")\n",
        "                        # Mark table as unmatched\n",
        "                        if table_id not in matching_result['unmatched_tables']:\n",
        "                            matching_result['unmatched_tables'].append(table_id)\n",
        "                else:\n",
        "                    # High confidence match, accept\n",
        "                    validated_matches.append({\n",
        "                        'chain_id': chain_id,\n",
        "                        'table_id': table_id,\n",
        "                        'similarity': similarity,\n",
        "                        'api_validated': False\n",
        "                    })\n",
        "\n",
        "            print(f\"      Validated matches: {len(validated_matches)}\")\n",
        "\n",
        "            # Update chains\n",
        "            chain_mgr.update_chains(validated_matches, year, loader.tables_metadata)\n",
        "\n",
        "            # Handle gaps\n",
        "            matched_chains = {m['chain_id'] for m in validated_matches}\n",
        "            gap_report = gap_handler.check_gaps(chain_mgr.chains, year, matched_chains)\n",
        "\n",
        "            if gap_report['new_dormant']:\n",
        "                print(f\"      New dormant chains: {len(gap_report['new_dormant'])}\")\n",
        "            if gap_report['ended']:\n",
        "                print(f\"      Ended chains: {len(gap_report['ended'])}\")\n",
        "\n",
        "            # FIX 2: Try to reactivate dormant chains\n",
        "            reactivated_count = 0\n",
        "            for chain_id, chain in chain_mgr.chains.items():\n",
        "                if chain['status'] == 'dormant' and chain_id not in matched_chains:\n",
        "                    # Try matching this dormant chain to unmatched tables\n",
        "                    if chain['tables']:\n",
        "                        last_table = chain['tables'][-1]\n",
        "                        if last_table in chapter_embeddings:\n",
        "                            chain_emb = chapter_embeddings[last_table]\n",
        "                            for table_id in list(matching_result['unmatched_tables']):  # Use list() to avoid modification during iteration\n",
        "                                if table_id in chapter_embeddings:\n",
        "                                    table_emb = chapter_embeddings[table_id]\n",
        "                                    similarity = (1 - cosine(chain_emb, table_emb) + 1) / 2\n",
        "\n",
        "                                    # Check if high confidence or API validates\n",
        "                                    should_reactivate = False\n",
        "                                    if similarity >= 0.97:\n",
        "                                        should_reactivate = True\n",
        "                                    elif config.use_api_validation and 0.85 <= similarity:\n",
        "                                        validation = api_validator.validate_edge_case(\n",
        "                                            chain['headers'],\n",
        "                                            loader.tables_metadata[table_id]['header'],\n",
        "                                            similarity\n",
        "                                        )\n",
        "                                        action = response_handler.process_response(validation, 'edge_case')\n",
        "                                        if action.value == 'confirm':\n",
        "                                            should_reactivate = True\n",
        "\n",
        "                                    if should_reactivate:\n",
        "                                        chain['status'] = 'active'\n",
        "                                        chain['tables'].append(table_id)\n",
        "                                        chain['years'].append(year)\n",
        "                                        chain['headers'].append(loader.tables_metadata[table_id]['header'])\n",
        "                                        chain['columns'].append(loader.tables_metadata[table_id].get('columns', []))\n",
        "                                        chain['similarities'].append(similarity)\n",
        "                                        chain['api_validated'].append(similarity < 0.97)\n",
        "                                        matching_result['unmatched_tables'].remove(table_id)\n",
        "                                        reactivated_count += 1\n",
        "                                        print(f\"      Reactivated chain {chain_id} with {table_id} (sim={similarity:.3f})\")\n",
        "                                        break\n",
        "\n",
        "            if reactivated_count > 0:\n",
        "                print(f\"      Total chains reactivated: {reactivated_count}\")\n",
        "\n",
        "            # FIX 3: Create new chains for unmatched tables\n",
        "            new_chains_count = 0\n",
        "            for table_id in matching_result['unmatched_tables']:\n",
        "                if table_id in loader.tables_metadata:\n",
        "                    new_chain_id = f\"chain_{table_id}\"\n",
        "                    chain_mgr.chains[new_chain_id] = {\n",
        "                        'id': new_chain_id,\n",
        "                        'tables': [table_id],\n",
        "                        'years': [year],\n",
        "                        'headers': [loader.tables_metadata[table_id]['header']],\n",
        "                        'columns': [loader.tables_metadata[table_id].get('columns', [])],\n",
        "                        'status': 'active',\n",
        "                        'gaps': [],\n",
        "                        'similarities': [],\n",
        "                        'api_validated': []\n",
        "                    }\n",
        "                    new_chains_count += 1\n",
        "\n",
        "            if new_chains_count > 0:\n",
        "                print(f\"      Created {new_chains_count} new chains for unmatched tables\")\n",
        "\n",
        "            # Record statistics for this chapter\n",
        "            for match in validated_matches:\n",
        "                chapter_stats.record_match(\n",
        "                    match['chain_id'],\n",
        "                    match['table_id'],\n",
        "                    year,\n",
        "                    match['similarity'],\n",
        "                    'confident' if match['similarity'] >= 0.97 else 'uncertain'\n",
        "                )\n",
        "\n",
        "            year_time = time.time() - year_start\n",
        "            chapter_stats.record_year(\n",
        "                year, len(tables_by_chapter_year[chapter][year]),\n",
        "                len(validated_matches),\n",
        "                matching_result['unmatched_tables'],\n",
        "                matching_result['unmatched_chains'],\n",
        "                year_time\n",
        "            )\n",
        "\n",
        "        # Store results for this chapter\n",
        "        all_chapter_chains[chapter] = chain_mgr.chains\n",
        "        all_chapter_stats[chapter] = chapter_stats.get_summary()\n",
        "\n",
        "        # Generate outputs for this chapter\n",
        "        print(f\"\\n   Generating outputs for Chapter {chapter}...\")\n",
        "\n",
        "        # Create chapter-specific output files\n",
        "        chapter_dir = f\"output_chapter_{chapter}\"\n",
        "        os.makedirs(chapter_dir, exist_ok=True)\n",
        "\n",
        "        # Visualizations\n",
        "        sankey = visualizer.create_sankey(chain_mgr.chains, last_sim_matrix)\n",
        "        if sankey:\n",
        "            sankey_file = f\"{chapter_dir}/sankey_chapter_{chapter}.html\"\n",
        "            sankey.write_html(sankey_file)\n",
        "            print(f\"      Created {sankey_file}\")\n",
        "\n",
        "        # Reports\n",
        "        chains_file = f\"{chapter_dir}/chains_chapter_{chapter}.json\"\n",
        "        reporter.save_chains_json(chain_mgr.chains, chains_file)\n",
        "        print(f\"      Created {chains_file}\")\n",
        "\n",
        "        html_file = f\"{chapter_dir}/report_chapter_{chapter}.html\"\n",
        "        with open(html_file, 'w', encoding='utf-8') as f:\n",
        "            html = f\"\"\"<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Chapter {chapter} Chain Report</title>\n",
        "    <meta charset=\"utf-8\">\n",
        "    <style>\n",
        "        body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
        "        .summary {{ background: #f0f0f0; padding: 15px; margin: 20px 0; }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Chapter {chapter} Chain Matching Report</h1>\n",
        "    <div class=\"summary\">\n",
        "        <h2>Summary</h2>\n",
        "        <p>Total chains: {len(chain_mgr.chains)}</p>\n",
        "        <p>Active chains: {sum(1 for c in chain_mgr.chains.values() if c['status'] == 'active')}</p>\n",
        "        <p>Dormant chains: {sum(1 for c in chain_mgr.chains.values() if c['status'] == 'dormant')}</p>\n",
        "        <p>Year range: {min(chapter_years)} - {max(chapter_years)}</p>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "            f.write(html)\n",
        "        print(f\"      Created {html_file}\")\n",
        "\n",
        "        # Chapter summary\n",
        "        print(f\"\\n   Chapter {chapter} Summary:\")\n",
        "        print(f\"      Total chains: {len(chain_mgr.chains)}\")\n",
        "        print(f\"      Active chains: {sum(1 for c in chain_mgr.chains.values() if c['status'] == 'active')}\")\n",
        "        print(f\"      Dormant chains: {sum(1 for c in chain_mgr.chains.values() if c['status'] == 'dormant')}\")\n",
        "        print(f\"      Total matches: {chapter_stats.global_stats['total_matches']}\")\n",
        "\n",
        "    # Final summary\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"✅ COMPLETE Processing finished in {total_time:.2f} seconds\")\n",
        "    print(f\"   Chapters processed: {len(all_chapter_chains)}\")\n",
        "\n",
        "    for chapter in chapters_to_process:\n",
        "        if chapter in all_chapter_chains:\n",
        "            print(f\"\\n   Chapter {chapter}:\")\n",
        "            print(f\"      Chains: {len(all_chapter_chains[chapter])}\")\n",
        "            print(f\"      Active: {sum(1 for c in all_chapter_chains[chapter].values() if c['status'] == 'active')}\")\n",
        "            print(f\"      Dormant: {sum(1 for c in all_chapter_chains[chapter].values() if c['status'] == 'dormant')}\")\n",
        "\n",
        "    if config.use_api_validation:\n",
        "        print(f\"\\n   Total API validations: {api_validator.validation_count}\")\n",
        "\n",
        "    return all_chapter_chains, all_chapter_stats\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chains, statistics = process_table_chains_final_complete()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To process ALL chapters instead of just Chapter 1, change line 87:\n",
        "chapters_to_process = list(sorted(tables_by_chapter_year.keys()))"
      ],
      "metadata": {
        "id": "VetFLLbGUrrJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohnI5SzKwfEm"
      },
      "source": [
        "# Final Execution Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yiMHCfhnwlkC"
      },
      "outputs": [],
      "source": [
        "# Install ALL required packages\n",
        "!pip install scipy sentence-transformers plotly networkx -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EFQSldfkp_zZ"
      },
      "outputs": [],
      "source": [
        "!rm -rf chain_storage/embeddings/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS26m4N_wlzh",
        "outputId": "977755ef-5ed2-4500-eaff-b5624bee1d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_conflict_detection (test_suite.TestCompleteSystem.test_conflict_detection)\n",
            "Test conflict detection ... FAIL\n",
            "test_hungarian_matching (test_suite.TestCompleteSystem.test_hungarian_matching)\n",
            "Test Hungarian algorithm ... ok\n",
            "test_similarity_computation (test_suite.TestCompleteSystem.test_similarity_computation)\n",
            "Test similarity computation ... ok\n",
            "\n",
            "======================================================================\n",
            "FAIL: test_conflict_detection (test_suite.TestCompleteSystem.test_conflict_detection)\n",
            "Test conflict detection\n",
            "----------------------------------------------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/test_suite.py\", line 41, in test_conflict_detection\n",
            "    self.assertEqual(len(conflicts), 0)  # No conflicts in this example\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: 1 != 0\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 3 tests in 0.008s\n",
            "\n",
            "FAILED (failures=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CHAPTER-BY-CHAPTER TABLE CHAIN MATCHING SYSTEM\n",
            "============================================================\n",
            "\n",
            "Running system tests...\n",
            "Tests: FAILED\n",
            "\n",
            "1. Loading tables...\n",
            "   Loaded 6496 tables\n",
            "\n",
            "2. Chapter Organization:\n",
            "   Found 15 chapters total\n",
            "   Processing chapters: [1]\n",
            "\n",
            "============================================================\n",
            "PROCESSING CHAPTER 1\n",
            "============================================================\n",
            "   Years available: 2001 to 2024\n",
            "   Initialized 23 chains for Year 2001\n",
            "\n",
            "   Generating embeddings for chapter 1...\n",
            "      Year 2001: 23 embeddings\n",
            "      Year 2002: 19 embeddings\n",
            "      Year 2003: 14 embeddings\n",
            "      Year 2004: 16 embeddings\n",
            "      Year 2005: 12 embeddings\n",
            "      Year 2006: 14 embeddings\n",
            "      Year 2007: 15 embeddings\n",
            "      Year 2008: 20 embeddings\n",
            "      Year 2009: 15 embeddings\n",
            "      Year 2010: 14 embeddings\n",
            "      Year 2011: 14 embeddings\n",
            "      Year 2012: 31 embeddings\n",
            "      Year 2014: 14 embeddings\n",
            "      Year 2015: 13 embeddings\n",
            "      Year 2016: 45 embeddings\n",
            "      Year 2017: 2 embeddings\n",
            "      Year 2018: 2 embeddings\n",
            "      Year 2019: 11 embeddings\n",
            "      Year 2020: 1 embeddings\n",
            "      Year 2021: 1 embeddings\n",
            "      Year 2022: 13 embeddings\n",
            "      Year 2023: 1 embeddings\n",
            "      Year 2024: 1 embeddings\n",
            "\n",
            "   Processing Chapter 1, Year 2002...\n",
            "      Active chains: 23, Tables: 19\n",
            "      Conflicts detected: 19\n",
            "      Initial matches found: 18\n",
            "      Splits detected: 23\n",
            "      Merges detected: 19\n",
            "      Complex N:N relationships: 372\n",
            "      API rejected: chain_6_01_2001 -> 6_01_2002 (sim=0.867)\n",
            "      Validated matches: 17\n",
            "      Created 1 new chains for unmatched tables\n",
            "\n",
            "   Processing Chapter 1, Year 2003...\n",
            "      Active chains: 18, Tables: 14\n",
            "      Conflicts detected: 33\n",
            "      Initial matches found: 13\n",
            "      Splits detected: 18\n",
            "      Merges detected: 14\n",
            "      Complex N:N relationships: 590\n",
            "      API confirmed: chain_22_01_2001 -> 13_01_2003 (sim=0.963)\n",
            "      Validated matches: 13\n",
            "\n",
            "   Processing Chapter 1, Year 2004...\n",
            "      Active chains: 13, Tables: 16\n",
            "      Conflicts detected: 49\n",
            "      Initial matches found: 13\n",
            "      Splits detected: 13\n",
            "      Merges detected: 16\n",
            "      Complex N:N relationships: 770\n",
            "      Validated matches: 13\n",
            "      Created 3 new chains for unmatched tables\n",
            "\n",
            "   Processing Chapter 1, Year 2005...\n",
            "      Active chains: 16, Tables: 12\n",
            "      Conflicts detected: 61\n",
            "      Initial matches found: 12\n",
            "      Splits detected: 16\n",
            "      Merges detected: 12\n",
            "      Complex N:N relationships: 924\n",
            "      API confirmed: chain_3_01_2001 -> 4_01_2005 (sim=0.969)\n",
            "      API confirmed: chain_5_01_2001 -> 5_01_2005 (sim=0.965)\n",
            "      Validated matches: 12\n",
            "\n",
            "   Processing Chapter 1, Year 2006...\n",
            "      Active chains: 12, Tables: 14\n",
            "      Conflicts detected: 74\n",
            "      Initial matches found: 12\n",
            "      Splits detected: 12\n",
            "      Merges detected: 14\n",
            "      Complex N:N relationships: 1053\n",
            "      API confirmed: chain_2_01_2001 -> 2_01_2006 (sim=0.963)\n",
            "      Validated matches: 12\n",
            "      Reactivated chain chain_7_01_2001 with 6_01_2006 (sim=0.994)\n",
            "      Total chains reactivated: 1\n",
            "      Created 1 new chains for unmatched tables\n",
            "\n",
            "   Processing Chapter 1, Year 2007...\n",
            "      Active chains: 14, Tables: 15\n",
            "      Conflicts detected: 86\n",
            "      Initial matches found: 13\n",
            "      Splits detected: 14\n",
            "      Merges detected: 13\n",
            "      Complex N:N relationships: 1207\n",
            "      API confirmed: chain_1_01_2001 -> 2_01_2007 (sim=0.962)\n",
            "      API confirmed: chain_2_01_2001 -> 3_01_2007 (sim=0.955)\n",
            "      API confirmed: chain_3_01_2001 -> 6_01_2007 (sim=0.969)\n",
            "      API confirmed: chain_5_01_2001 -> 7_01_2007 (sim=0.932)\n",
            "      API confirmed: chain_7_01_2001 -> 8_01_2007 (sim=0.936)\n",
            "      API confirmed: chain_10_01_2001 -> 9_01_2007 (sim=0.955)\n",
            "      API confirmed: chain_14_01_2001 -> 4_01_2007 (sim=0.965)\n",
            "      Validated matches: 13\n",
            "      Created 1 new chains for unmatched tables\n",
            "\n",
            "   Processing Chapter 1, Year 2008...\n",
            "      Active chains: 14, Tables: 20\n",
            "      Conflicts detected: 104\n",
            "      Initial matches found: 14\n",
            "      Splits detected: 13\n",
            "      Merges detected: 18\n",
            "      Complex N:N relationships: 1414\n",
            "      API rejected: chain_22_01_2001 -> 16_01_2008 (sim=0.903)\n",
            "      API rejected: chain_23_01_2001 -> 17_01_2008 (sim=0.903)\n",
            "      API rejected: chain_15_01_2004 -> 18_01_2008 (sim=0.903)\n",
            "      Validated matches: 11\n",
            "      Reactivated chain chain_6_01_2001 with 9_01_2008 (sim=0.926)\n",
            "      Reactivated chain chain_8_01_2001 with 10_01_2008 (sim=0.947)\n",
            "      Reactivated chain chain_9_01_2001 with 11_01_2008 (sim=0.950)\n",
            "      Reactivated chain chain_11_01_2001 with 13_01_2008 (sim=0.950)\n",
            "      Total chains reactivated: 4\n",
            "      Created 5 new chains for unmatched tables\n",
            "\n",
            "   Processing Chapter 1, Year 2009...\n",
            "      Active chains: 20, Tables: 15\n",
            "      Conflicts detected: 117\n",
            "      Initial matches found: 15\n",
            "      Splits detected: 19\n",
            "      Merges detected: 14\n",
            "      Complex N:N relationships: 1643\n",
            "      Validated matches: 15\n",
            "\n",
            "   Processing Chapter 1, Year 2010...\n",
            "      Active chains: 15, Tables: 14\n",
            "      Conflicts detected: 129\n",
            "      Initial matches found: 14\n",
            "      Splits detected: 14\n",
            "      Merges detected: 13\n",
            "      Complex N:N relationships: 1801\n",
            "      API rejected: chain_5_01_2001 -> 7_01_2010 (sim=0.901)\n",
            "      API confirmed: chain_14_01_2001 -> 4_01_2010 (sim=0.964)\n",
            "      Validated matches: 13\n",
            "      Reactivated chain chain_4_01_2001 with 7_01_2010 (sim=0.982)\n",
            "      Total chains reactivated: 1\n",
            "\n",
            "   Processing Chapter 1, Year 2011...\n",
            "      Active chains: 14, Tables: 14\n",
            "      Conflicts detected: 141\n",
            "      Initial matches found: 13\n",
            "      Splits detected: 13\n",
            "      Merges detected: 13\n",
            "      Complex N:N relationships: 1941\n",
            "      Validated matches: 13\n",
            "\n",
            "   Processing Chapter 1, Year 2012...\n",
            "      Active chains: 13, Tables: 31\n",
            "      Conflicts detected: 159\n",
            "      Initial matches found: 13\n",
            "      Splits detected: 12\n",
            "      Merges detected: 25\n",
            "      Complex N:N relationships: 2171\n",
            "      API confirmed: chain_9_01_2001 -> 8_01_2012 (sim=0.969)\n",
            "      Validated matches: 13\n",
            "      Reactivated chain chain_16_01_2008 with 13_01_2012 (sim=0.937)\n",
            "      Total chains reactivated: 1\n",
            "      Created 17 new chains for unmatched tables\n",
            "\n",
            "   Processing Chapter 1, Year 2014...\n",
            "      Active chains: 31, Tables: 14\n",
            "      Conflicts detected: 171\n",
            "      Initial matches found: 14\n",
            "      Splits detected: 25\n",
            "      Merges detected: 13\n",
            "      Complex N:N relationships: 2431\n",
            "      API confirmed: chain_9_01_2001 -> 9_01_2014 (sim=0.960)\n",
            "      API confirmed: chain_23_01_2012 -> 11_01_2014 (sim=0.922)\n",
            "      Validated matches: 14\n",
            "\n",
            "   Processing Chapter 1, Year 2015...\n",
            "      Active chains: 14, Tables: 13\n",
            "      Conflicts detected: 182\n",
            "      Initial matches found: 13\n",
            "      Splits detected: 13\n",
            "      Merges detected: 12\n",
            "      Complex N:N relationships: 2564\n",
            "      Validated matches: 13\n",
            "\n",
            "   Processing Chapter 1, Year 2016...\n",
            "      Active chains: 13, Tables: 45\n",
            "      Conflicts detected: 189\n",
            "      Initial matches found: 6\n",
            "      Splits detected: 10\n",
            "      Merges detected: 27\n",
            "      Complex N:N relationships: 2754\n",
            "      API rejected: chain_3_01_2001 -> 11_01_2016 (sim=0.851)\n",
            "      API rejected: chain_4_01_2001 -> 34_01_2016 (sim=0.859)\n",
            "      API rejected: chain_6_01_2001 -> 2_01_2016 (sim=0.857)\n",
            "      API rejected: chain_8_01_2001 -> 35_01_2016 (sim=0.852)\n",
            "      API rejected: chain_21_01_2001 -> 40_01_2016 (sim=0.856)\n",
            "      API rejected: chain_11_01_2004 -> 33_01_2016 (sim=0.852)\n",
            "      Validated matches: 0\n",
            "      Created 38 new chains for unmatched tables\n",
            "\n",
            "   Processing Chapter 1, Year 2017...\n",
            "      Active chains: 38, Tables: 2\n",
            "      Conflicts detected: 189\n",
            "      Initial matches found: 0\n",
            "      Splits detected: 3\n",
            "      Merges detected: 2\n",
            "      Complex N:N relationships: 2760\n",
            "      Validated matches: 0\n",
            "\n",
            "   Processing Chapter 1, Year 2018...\n",
            "      Active chains: 0, Tables: 2\n",
            "      Conflicts detected: 189\n",
            "      Initial matches found: 0\n",
            "      Complex N:N relationships: 2760\n",
            "      Validated matches: 0\n",
            "      Reactivated chain chain_1_01_2001 with 1_01_2018 (sim=0.931)\n",
            "      Reactivated chain chain_21_01_2001 with 2_01_2018 (sim=0.924)\n",
            "      Total chains reactivated: 2\n",
            "\n",
            "   Processing Chapter 1, Year 2019...\n",
            "      Active chains: 2, Tables: 11\n",
            "      Conflicts detected: 197\n",
            "      Initial matches found: 2\n",
            "      Splits detected: 2\n",
            "      Merges detected: 11\n",
            "      Complex N:N relationships: 2782\n",
            "      Validated matches: 2\n",
            "      Reactivated chain chain_14_01_2001 with 3_01_2019 (sim=0.955)\n",
            "      Reactivated chain chain_19_01_2008 with 6_01_2019 (sim=0.934)\n",
            "      Reactivated chain chain_17_01_2008 with 7_01_2019 (sim=0.924)\n",
            "      Total chains reactivated: 3\n",
            "      Created 6 new chains for unmatched tables\n",
            "\n",
            "   Processing Chapter 1, Year 2020...\n",
            "      Active chains: 11, Tables: 1\n",
            "      Conflicts detected: 198\n",
            "      Initial matches found: 1\n",
            "      Merges detected: 1\n",
            "      Complex N:N relationships: 2782\n",
            "      Validated matches: 1\n",
            "\n",
            "   Processing Chapter 1, Year 2021...\n",
            "      Active chains: 1, Tables: 1\n",
            "      Conflicts detected: 198\n",
            "      Initial matches found: 1\n",
            "      Complex N:N relationships: 2782\n",
            "      Validated matches: 1\n",
            "\n",
            "   Processing Chapter 1, Year 2022...\n",
            "      Active chains: 1, Tables: 13\n",
            "      Conflicts detected: 198\n",
            "      Initial matches found: 1\n",
            "      Splits detected: 1\n",
            "      Complex N:N relationships: 2782\n",
            "      Validated matches: 1\n",
            "      Reactivated chain chain_14_01_2001 with 3_01_2022 (sim=1.000)\n",
            "      Reactivated chain chain_21_01_2001 with 9_01_2022 (sim=1.000)\n",
            "      Reactivated chain chain_2_01_2019 with 2_01_2022 (sim=1.000)\n",
            "      Reactivated chain chain_4_01_2019 with 4_01_2022 (sim=0.979)\n",
            "      Reactivated chain chain_8_01_2019 with 10_01_2022 (sim=0.962)\n",
            "      Reactivated chain chain_9_01_2019 with 11_01_2022 (sim=0.964)\n",
            "      Reactivated chain chain_10_01_2019 with 12_01_2022 (sim=0.942)\n",
            "      Reactivated chain chain_11_01_2019 with 13_01_2022 (sim=0.962)\n",
            "      Total chains reactivated: 8\n",
            "      Created 4 new chains for unmatched tables\n",
            "\n",
            "   Processing Chapter 1, Year 2023...\n",
            "      Active chains: 13, Tables: 1\n",
            "      Conflicts detected: 199\n",
            "      Initial matches found: 1\n",
            "      Merges detected: 1\n",
            "      Complex N:N relationships: 2782\n",
            "      Validated matches: 1\n",
            "\n",
            "   Processing Chapter 1, Year 2024...\n",
            "      Active chains: 1, Tables: 1\n",
            "      Conflicts detected: 199\n",
            "      Initial matches found: 1\n",
            "      Complex N:N relationships: 2782\n",
            "      Validated matches: 1\n",
            "\n",
            "   Generating outputs for Chapter 1...\n",
            "      Created output_chapter_1/sankey_chapter_1.html\n",
            "      Created output_chapter_1/chains_chapter_1.json\n",
            "      Created output_chapter_1/report_chapter_1.html\n",
            "\n",
            "   Chapter 1 Summary:\n",
            "      Total chains: 99\n",
            "      Active chains: 1\n",
            "      Dormant chains: 98\n",
            "      Total matches: 179\n",
            "\n",
            "============================================================\n",
            "✅ COMPLETE Processing finished in 122.17 seconds\n",
            "   Chapters processed: 1\n",
            "\n",
            "   Chapter 1:\n",
            "      Chains: 99\n",
            "      Active: 1\n",
            "      Dormant: 98\n",
            "\n",
            "   Total API validations: 327\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import importlib\n",
        "import report_gen\n",
        "importlib.reload(report_gen)\n",
        "\n",
        "os.environ['CLAUDE_API_KEY'] = \"put key here\"\n",
        "\n",
        "from final_complete_processor import process_table_chains_final_complete\n",
        "chains, statistics = process_table_chains_final_complete()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5SiQtcQCcmgz",
        "outputId": "ce885afa-f90e-4279-eaa7-12ae6976ee8f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_68bc30de-058b-42df-ba21-ee432912c75e\", \"sankey_chapter_1.html\", 4952285)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Display in Colab\n",
        "from IPython.display import IFrame\n",
        "IFrame('output_chapter_1/sankey_chapter_1.html', width=800, height=600)\n",
        "\n",
        "# Or download it\n",
        "from google.colab import files\n",
        "files.download('output_chapter_1/sankey_chapter_1.html')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('output_chapter_1/chains_chapter_1.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "IQqRlB5qX9kh",
        "outputId": "55b8dcc0-7f75-4a42-e09c-d213b736e9cd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c7c8df5f-5a95-4545-b3b8-1bade019ed4d\", \"chains_chapter_1.json\", 420999)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}