{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXfp1J9Wuz0DRBhBBlOPJq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiragelb/NCC-Statistical-Reports/blob/main/Table_Extraction_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulO1BwoAvu4j",
        "outputId": "9d2b17e9-bd27-4107-b8da-2037bf0d9ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc-data\n",
            "Suggested packages:\n",
            "  texlive-latex-recommended texlive-xetex texlive-luatex pandoc-citeproc\n",
            "  texlive-latex-extra context wkhtmltopdf librsvg2-bin groff ghc nodejs php\n",
            "  python ruby libjs-mathjax libjs-katex citation-style-language-styles\n",
            "The following NEW packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc\n",
            "  pandoc-data\n",
            "0 upgraded, 4 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 20.6 MB of archives.\n",
            "After this operation, 156 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [115 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm-extensions0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [25.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc-data all 2.9.2.1-3ubuntu2 [81.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc amd64 2.9.2.1-3ubuntu2 [20.3 MB]\n",
            "31% [4 pandoc 3,929 kB/20.3 MB 19%]                           123 kB/s 2min 13s"
          ]
        }
      ],
      "source": [
        "!apt-get install pandoc\n",
        "!pip install pypandoc\n",
        "!pip install python-docx\n",
        "!pip install docx2txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting folders id from shared memory\n",
        "\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "folder_id = \"1e0eA-AIsz_BSwVHOppJMXECX42hBfG4J\"\n",
        "\n",
        "def list_all_files_in_folder_recursive(parent_id, parent_path=\"\"):\n",
        "    \"\"\"Recursively list all docx files in a folder and subfolders\"\"\"\n",
        "    all_files = []\n",
        "\n",
        "    query = f\"'{parent_id}' in parents and trashed=false\"\n",
        "    page_token = None\n",
        "\n",
        "    while True:\n",
        "        response = drive_service.files().list(\n",
        "            q=query,\n",
        "            spaces='drive',\n",
        "            fields='nextPageToken, files(id, name, mimeType)',\n",
        "            pageToken=page_token\n",
        "        ).execute()\n",
        "\n",
        "        for item in response.get('files', []):\n",
        "            item_path = f\"{parent_path}/{item['name']}\" if parent_path else item['name']\n",
        "\n",
        "            if item['mimeType'] == 'application/vnd.google-apps.folder':\n",
        "                # Recurse into subfolder\n",
        "                all_files.extend(list_all_files_in_folder_recursive(item['id'], item_path))\n",
        "            elif item['name'].endswith('.docx'):\n",
        "                # Only add docx files\n",
        "                all_files.append({\n",
        "                    \"file_name\": item['name'],\n",
        "                    \"file_path\": item_path,\n",
        "                    \"file_id\": item['id'],\n",
        "                    \"file_url\": f\"https://drive.google.com/file/d/{item['id']}/view?usp=sharing\"\n",
        "                })\n",
        "\n",
        "        page_token = response.get('nextPageToken', None)\n",
        "        if page_token is None:\n",
        "            break\n",
        "\n",
        "    return all_files\n",
        "\n",
        "# Run the recursive function\n",
        "files_list = list_all_files_in_folder_recursive(folder_id)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_files = pd.DataFrame(files_list)\n",
        "##df_files.head()\n",
        "\n",
        "##getting local id of the file\n",
        "!pip install python-docx\n",
        "import os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "import io\n",
        "\n",
        "def get_chapter_file(chapter_num: int, year: str, download_dir=\"/content\") -> str:\n",
        "    \"\"\"\n",
        "    Download a chapter file for a given year from Google Drive and return its local path.\n",
        "\n",
        "    Args:\n",
        "        chapter_num (int): The chapter number (e.g., 1, 2, ..., 14).\n",
        "        year (str): The year folder name.\n",
        "        download_dir (str): Local folder to save the file. Defaults to /content in Colab.\n",
        "\n",
        "    Returns:\n",
        "        str: Local path to the downloaded file, or None if not found.\n",
        "    \"\"\"\n",
        "    # Normalize chapter filename (01, 02, ...)\n",
        "    chapter_str = f\"{chapter_num:02}\"\n",
        "\n",
        "    # Find matching file in df_files\n",
        "    match = df_files[\n",
        "        (df_files[\"file_name\"].str.contains(chapter_str)) &\n",
        "        (df_files[\"file_path\"].str.contains(f\"{year}/\"))\n",
        "    ]\n",
        "\n",
        "    if match.empty:\n",
        "        print(f\"❌ No file found for chapter {chapter_str} in year {year}\")\n",
        "        return None\n",
        "\n",
        "    file_id = match.iloc[0][\"file_id\"]\n",
        "    file_name = match.iloc[0][\"file_name\"]\n",
        "    local_path = os.path.join(download_dir, year, file_name)\n",
        "\n",
        "    # Ensure year directory exists locally\n",
        "    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "    # Download the file from Google Drive\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    fh = io.FileIO(local_path, \"wb\")\n",
        "    downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "    done = False\n",
        "    while not done:\n",
        "        status, done = downloader.next_chunk()\n",
        "        if status:\n",
        "            print(f\"⬇️ Download {int(status.progress() * 100)}%.\")\n",
        "\n",
        "    print(f\"✅ Saved to {local_path}\")\n",
        "    return local_path\n",
        "\n",
        "from docx import Document\n",
        "\n",
        "# After downloading the file\n",
        "# sanity check\n",
        "path = get_chapter_file(14, \"2015\")\n",
        "print(\"Local file:\", path)\n",
        "\n",
        "if path and path.endswith(\".docx\"):\n",
        "    doc = Document(path)\n",
        "    full_text = \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip() != \"\"])\n",
        "\n",
        "    # Split into sentences (naive split by period/full stop)\n",
        "    sentences = full_text.replace(\"\\n\", \" \").split(\".\")\n",
        "\n",
        "    # Print first few sentences\n",
        "    preview_count = 3\n",
        "    preview_sentences = [s.strip() for s in sentences if s.strip()][:preview_count]\n",
        "    print(\"\\nPreview of content:\")\n",
        "    for i, s in enumerate(preview_sentences, 1):\n",
        "        print(f\"{i}. {s}.\")\n",
        "\n",
        "\n",
        "#a code that downloads the files to a directory called content/reports\n",
        "import os\n",
        "import io\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "def download_all_chapters(download_dir=\"/content/reports\", years=range(2001, 2025), chapters=range(1, 16)):\n",
        "    \"\"\"\n",
        "    Downloads all chapters for all years to local environment.\n",
        "\n",
        "    Returns:\n",
        "        dict: {year: {chapter_number: local_path}}\n",
        "    \"\"\"\n",
        "    all_paths = {}\n",
        "\n",
        "    for year in years:\n",
        "        year_str = str(year)\n",
        "        all_paths[year_str] = {}\n",
        "\n",
        "        for chapter in chapters:\n",
        "            chapter_str = f\"{chapter:02}\"\n",
        "\n",
        "            # Find matching file in df_files\n",
        "            match = df_files[\n",
        "                (df_files[\"file_name\"].str.contains(chapter_str)) &\n",
        "                (df_files[\"file_path\"].str.contains(f\"{year_str}/\"))\n",
        "            ]\n",
        "\n",
        "            if match.empty:\n",
        "                print(f\"⚠️ Chapter {chapter_str} not found for year {year_str}\")\n",
        "                continue\n",
        "\n",
        "            file_id = match.iloc[0][\"file_id\"]\n",
        "            file_name = match.iloc[0][\"file_name\"]\n",
        "            local_path = os.path.join(download_dir, year_str, f\"{chapter_str}_{file_name}\")\n",
        "\n",
        "            # Ensure folder exists\n",
        "            os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
        "\n",
        "            # Download file\n",
        "            request = drive_service.files().get_media(fileId=file_id)\n",
        "            fh = io.FileIO(local_path, \"wb\")\n",
        "            downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "            done = False\n",
        "            while not done:\n",
        "                status, done = downloader.next_chunk()\n",
        "                if status:\n",
        "                    print(f\"⬇️ Download {int(status.progress() * 100)}% for {file_name}\")\n",
        "\n",
        "            print(f\"✅ Saved {file_name} to {local_path}\")\n",
        "            all_paths[year_str][chapter] = local_path\n",
        "\n",
        "    return all_paths\n",
        "\n",
        "# Donwloads all docx files - ignores doc or pdf\n",
        "download_all_chapters()"
      ],
      "metadata": {
        "id": "12HzQvP_vz9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "\n",
        "def extract_tables_from_reports(base_dir=\"/content/reports\", out_dir=\"/content/tables\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    summary = {}\n",
        "    colnames_map = {}\n",
        "\n",
        "    for year in range(2001, 2025):  # adjust later\n",
        "        print(year)\n",
        "        year_path = os.path.join(base_dir, str(year))\n",
        "        if not os.path.isdir(year_path):\n",
        "            continue\n",
        "\n",
        "        for fname in os.listdir(year_path):\n",
        "            if not (fname.endswith(\".docx\") or fname.endswith(\".doc\")):\n",
        "                continue\n",
        "            if fname.endswith(\".doc\"):\n",
        "                continue\n",
        "\n",
        "            chapter = fname.split(\"_\")[0]\n",
        "            fpath = os.path.join(year_path, fname)\n",
        "\n",
        "            try:\n",
        "                doc = Document(fpath)\n",
        "            except Exception as e:\n",
        "                print(f\"skip {fpath}: {e}\")\n",
        "                continue\n",
        "\n",
        "            serial = 1\n",
        "            # find all tables with names containing \"לוח\"\n",
        "            for i, table in enumerate(doc.tables):\n",
        "                table_name = \"\"  # default\n",
        "\n",
        "                # check the first row cells for \"לוח\"\n",
        "                if len(table.rows) > 0:\n",
        "                    for cell in table.rows[0].cells:\n",
        "                        if \"לוח\" in cell.text and \"תרשים\" not in cell.text:  # extra check\n",
        "                            table_name = cell.text.strip()\n",
        "                            break\n",
        "\n",
        "                    if not table_name:  # skip if not a valid table\n",
        "                        continue\n",
        "\n",
        "                    # convert table to dataframe\n",
        "                    data = [[cell.text.strip() for cell in row.cells] for row in table.rows]\n",
        "                    df = pd.DataFrame(data)\n",
        "\n",
        "                    identifier = f\"{serial}_{chapter}_{year}\"\n",
        "\n",
        "                    # record mapping for JSON\n",
        "                    if len(df) > 0:\n",
        "                        summary[identifier] = \" \".join(df.iloc[0].astype(str).tolist())\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                    if len(df) > 1:\n",
        "                        colnames_map[identifier] = df.iloc[1].astype(str).tolist()\n",
        "                    else:\n",
        "                        colnames_map[identifier] = []\n",
        "\n",
        "                    # save CSV\n",
        "                    save_dir = os.path.join(out_dir, str(year), chapter)\n",
        "                    os.makedirs(save_dir, exist_ok=True)\n",
        "                    save_path = os.path.join(save_dir, f\"{identifier}.csv\")\n",
        "                    df.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "                    serial += 1\n",
        "\n",
        "    # write summary JSON\n",
        "    with open(os.path.join(out_dir, \"tables_summary.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    with open(os.path.join(out_dir, \"tables_columns.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(colnames_map, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "extract_tables_from_reports()"
      ],
      "metadata": {
        "id": "3DDdJHqaxW5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table Extraction Validation Plan\n",
        "\n",
        "## Project Overview\n",
        "Validation strategy for Hebrew table extraction workflow processing ~24 years × 14-15 chapters × 10-20 tables per chapter. Focus: ensuring complete extraction of all \"לוח\" tables with proper data integrity.\n",
        "\n",
        "## Approved Validation Strategy\n",
        "\n",
        "### Phase 1: Document-Level Extraction Validation (Automated)\n",
        "\n",
        "#### Step 1: Table Position Mapping\n",
        "- **Purpose**: Track document position of each extracted table to detect gaps\n",
        "- **Method**: Record paragraph/table index in original document for each extraction\n",
        "- **Output**: Position mapping file showing extraction sequence\n",
        "- **Rationale**: Identifies if tables were skipped during document traversal\n",
        "\n",
        "#### Step 2: Serial Number Sequence Validation  \n",
        "- **Purpose**: Verify consecutive serial numbering within each document\n",
        "- **Method**: Check CSV filenames against `tables_summary.json` for gaps in serial sequence\n",
        "- **Expected Pattern**: 1, 2, 3, 4... (no missing numbers per document)\n",
        "- **Output**: List of documents with missing serial numbers\n",
        "- **Rationale**: Missing serials indicate extraction failures\n",
        "\n",
        "### Phase 2: CSV Quality & Integrity Checks (Automated)\n",
        "\n",
        "#### Step 1: File Completeness Validator\n",
        "- **Purpose**: Ensure all expected files exist with correct naming\n",
        "- **Method**: Validate `serial_chapter_year.csv` naming convention\n",
        "- **Cross-check**: Verify files exist for all entries in `tables_summary.json`\n",
        "- **Output**: List of missing or misnamed files\n",
        "\n",
        "#### Step 2: Data Integrity Checker\n",
        "- **Truncation Detection**: Flag tables with suspiciously low row/column counts\n",
        "- **Empty File Detection**: Identify completely empty CSV files\n",
        "- **Encoding Validation**: Check for encoding issues or malformed data\n",
        "- **Header Validation**: Verify first row contains \"לוח\" as expected\n",
        "- **Output**: Integrity issues report with specific file problems\n",
        "\n",
        "#### Step 3: Content Pattern Validator\n",
        "- **Purpose**: Ensure extracted data maintains expected structure\n",
        "- **Method**: Validate Hebrew text presence and basic table patterns\n",
        "- **Checks**: Proper CSV format, readable Hebrew characters\n",
        "- **Output**: Pattern validation report\n",
        "\n",
        "### Phase 3: Statistical Anomaly Detection (Automated)\n",
        "\n",
        "#### Step 1: Conservative Cross-Chapter Analysis\n",
        "- **Purpose**: Flag extreme deviations in table counts\n",
        "- **Method**:\n",
        "  - Track historical patterns for each chapter name\n",
        "  - Flag only deviations >50% from chapter's historical range\n",
        "  - Avoid false positives from legitimate year-to-year variations\n",
        "- **Output**: Flagged chapters list for manual TOC verification\n",
        "- **Rationale**: Focus manual effort on most suspicious cases\n",
        "\n",
        "#### Step 2: Table Size Distribution Analysis\n",
        "- **Purpose**: Detect extraction errors through size anomalies\n",
        "- **Method**: Identify unusually small (1-2 rows) or extremely large tables\n",
        "- **Thresholds**: Statistical outliers in row/column distributions\n",
        "- **Output**: Size anomaly report for investigation\n",
        "\n",
        "#### Step 3: Chapter Completeness Overview\n",
        "- **Purpose**: Visual pattern recognition for missing extractions\n",
        "- **Method**: Generate summary matrix showing table counts per chapter-year\n",
        "- **Output**: Heat map or tabular view for visual inspection\n",
        "- **Usage**: Quick identification of suspicious patterns\n",
        "\n",
        "### Phase 4: Targeted Manual Verification (Semi-Automated)\n",
        "\n",
        "#### Step 1: High-Risk Sample Generator\n",
        "- **Purpose**: Prioritize manual verification effort\n",
        "- **Method**: Generate ranked list of documents flagged by automated checks\n",
        "- **Priority Factors**: Multiple flags, extreme deviations, historical anomalies\n",
        "- **Output**: Prioritized verification queue for manual TOC checking\n",
        "\n",
        "#### Step 2: Stratified Random Sampling\n",
        "- **Purpose**: Quality assurance through representative sampling\n",
        "- **Method**: Random selection across years/chapters for spot-checking\n",
        "- **Sample Size**: 5-10% of total documents, stratified by year\n",
        "- **Process**: Manual comparison of extraction results against original documents\n",
        "- **Output**: Sampling validation report\n",
        "\n",
        "## Implementation Sequence\n",
        "\n",
        "1. **Phase 2** (CSV validation) - Quick automated quality checks\n",
        "2. **Phase 1** (document-level validation) - Core extraction completeness\n",
        "3. **Phase 3** (statistical analysis) - Pattern-based anomaly detection  \n",
        "4. **Phase 4** (manual verification) - Targeted human validation\n",
        "\n",
        "## Success Criteria\n",
        "\n",
        "- **Completeness**: No missing serial numbers within documents\n",
        "- **Integrity**: All CSV files readable with expected Hebrew content\n",
        "- **Quality**: <5% of extractions flagged as anomalous\n",
        "- **Validation Coverage**: 100% automated checking + 5-10% manual sampling\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Execute Phase 2 first to establish baseline data quality, then proceed through phases sequentially. Each phase generates specific action items for investigation or correction."
      ],
      "metadata": {
        "id": "9p18obaIFxhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def validate_file_completeness(tables_dir=\"/content/tables\"):\n",
        "    \"\"\"\n",
        "    Validate that all CSV files referenced in tables_summary.json exist\n",
        "    and follow the expected naming convention.\n",
        "    \"\"\"\n",
        "    summary_path = os.path.join(tables_dir, \"tables_summary.json\")\n",
        "\n",
        "    # Check if summary file exists\n",
        "    if not os.path.exists(summary_path):\n",
        "        print(f\"❌ ERROR: tables_summary.json not found at {summary_path}\")\n",
        "        return False\n",
        "\n",
        "    # Load summary data\n",
        "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        summary = json.load(f)\n",
        "\n",
        "    print(f\"📊 Checking {len(summary)} table entries...\")\n",
        "\n",
        "    missing_files = []\n",
        "    naming_issues = []\n",
        "    found_files = []\n",
        "\n",
        "    for identifier, table_name in summary.items():\n",
        "        # Parse identifier: serial_chapter_year\n",
        "        try:\n",
        "            parts = identifier.split(\"_\")\n",
        "            if len(parts) != 3:\n",
        "                naming_issues.append(f\"Invalid identifier format: {identifier}\")\n",
        "                continue\n",
        "\n",
        "            serial, chapter, year = parts\n",
        "\n",
        "            # Construct expected file path\n",
        "            expected_path = os.path.join(tables_dir, year, chapter, f\"{identifier}.csv\")\n",
        "\n",
        "            if os.path.exists(expected_path):\n",
        "                found_files.append(expected_path)\n",
        "            else:\n",
        "                missing_files.append({\n",
        "                    \"identifier\": identifier,\n",
        "                    \"expected_path\": expected_path,\n",
        "                    \"table_name\": table_name\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            naming_issues.append(f\"Error parsing identifier {identifier}: {e}\")\n",
        "\n",
        "    # Generate report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"📋 FILE COMPLETENESS VALIDATION REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"✅ Files found: {len(found_files)}\")\n",
        "    print(f\"❌ Files missing: {len(missing_files)}\")\n",
        "    print(f\"⚠️  Naming issues: {len(naming_issues)}\")\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"\\n🔍 MISSING FILES ({len(missing_files)}):\")\n",
        "        for item in missing_files[:10]:  # Show first 10\n",
        "            print(f\"  • {item['identifier']} → {item['expected_path']}\")\n",
        "            print(f\"    Table: {item['table_name'][:50]}...\")\n",
        "        if len(missing_files) > 10:\n",
        "            print(f\"  ... and {len(missing_files) - 10} more\")\n",
        "\n",
        "    if naming_issues:\n",
        "        print(f\"\\n⚠️  NAMING ISSUES ({len(naming_issues)}):\")\n",
        "        for issue in naming_issues[:5]:  # Show first 5\n",
        "            print(f\"  • {issue}\")\n",
        "        if len(naming_issues) > 5:\n",
        "            print(f\"  ... and {len(naming_issues) - 5} more\")\n",
        "\n",
        "    # Calculate success rate\n",
        "    total_expected = len(summary)\n",
        "    success_rate = (len(found_files) / total_expected) * 100 if total_expected > 0 else 0\n",
        "\n",
        "    print(f\"\\n📈 SUCCESS RATE: {success_rate:.1f}% ({len(found_files)}/{total_expected})\")\n",
        "\n",
        "    # Save detailed report\n",
        "    if not os.path.exists(tables_dir + \"/validations/\"):\n",
        "      os.mkdir(tables_dir + \"/validations/\")\n",
        "    report_path = os.path.join(tables_dir, \"validations\", \"validation_file_completeness.json\")\n",
        "    report = {\n",
        "        \"summary\": {\n",
        "            \"total_expected\": total_expected,\n",
        "            \"files_found\": len(found_files),\n",
        "            \"files_missing\": len(missing_files),\n",
        "            \"naming_issues\": len(naming_issues),\n",
        "            \"success_rate\": success_rate\n",
        "        },\n",
        "        \"missing_files\": missing_files,\n",
        "        \"naming_issues\": naming_issues,\n",
        "        \"found_files\": found_files\n",
        "    }\n",
        "\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\n💾 Detailed report saved to: {report_path}\")\n",
        "\n",
        "    return success_rate > 95  # Consider success if >95% files found\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run validation\n",
        "    is_valid = validate_file_completeness()\n",
        "\n",
        "    if is_valid:\n",
        "        print(\"\\n✅ File completeness validation PASSED\")\n",
        "    else:\n",
        "        print(\"\\n❌ File completeness validation FAILED - review missing files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhpXo-SjJ0To",
        "outputId": "28abbedc-b87a-44a0-9f23-0dda070ed5a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Checking 6732 table entries...\n",
            "\n",
            "============================================================\n",
            "📋 FILE COMPLETENESS VALIDATION REPORT\n",
            "============================================================\n",
            "✅ Files found: 6732\n",
            "❌ Files missing: 0\n",
            "⚠️  Naming issues: 0\n",
            "\n",
            "📈 SUCCESS RATE: 100.0% (6732/6732)\n",
            "\n",
            "💾 Detailed report saved to: /content/tables/validations/validation_file_completeness.json\n",
            "\n",
            "✅ File completeness validation PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import csv\n",
        "\n",
        "def check_data_integrity(tables_dir=\"/content/tables\"):\n",
        "    \"\"\"\n",
        "    Check CSV files for truncation, empty files, encoding issues,\n",
        "    and validate that first row contains \"לוח\" as expected.\n",
        "    \"\"\"\n",
        "    summary_path = os.path.join(tables_dir, \"tables_summary.json\")\n",
        "\n",
        "    if not os.path.exists(summary_path):\n",
        "        print(f\"❌ ERROR: tables_summary.json not found at {summary_path}\")\n",
        "        return False\n",
        "\n",
        "    # Load summary data\n",
        "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        summary = json.load(f)\n",
        "\n",
        "    print(f\"🔍 Checking data integrity for {len(summary)} CSV files...\")\n",
        "\n",
        "    issues = {\n",
        "        \"empty_files\": [],\n",
        "        \"truncated_tables\": [],\n",
        "        \"encoding_errors\": [],\n",
        "        \"missing_luach\": [],\n",
        "        \"malformed_csv\": [],\n",
        "        \"suspicious_size\": []\n",
        "    }\n",
        "\n",
        "    processed = 0\n",
        "    valid_files = 0\n",
        "\n",
        "    for identifier, table_name in summary.items():\n",
        "        try:\n",
        "            # Parse identifier and construct path\n",
        "            parts = identifier.split(\"_\")\n",
        "            if len(parts) != 3:\n",
        "                continue\n",
        "\n",
        "            serial, chapter, year = parts\n",
        "            csv_path = os.path.join(tables_dir, year, chapter, f\"{identifier}.csv\")\n",
        "\n",
        "            if not os.path.exists(csv_path):\n",
        "                continue  # Already handled by completeness validator\n",
        "\n",
        "            processed += 1\n",
        "            file_issues = []\n",
        "\n",
        "            # Check file size\n",
        "            file_size = os.path.getsize(csv_path)\n",
        "            if file_size == 0:\n",
        "                issues[\"empty_files\"].append({\n",
        "                    \"identifier\": identifier,\n",
        "                    \"path\": csv_path,\n",
        "                    \"table_name\": table_name\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Try to read the CSV\n",
        "            try:\n",
        "                # First try with utf-8-sig (Excel format)\n",
        "                df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
        "\n",
        "                # Check if completely empty\n",
        "                if df.empty or len(df) == 0:\n",
        "                    issues[\"empty_files\"].append({\n",
        "                        \"identifier\": identifier,\n",
        "                        \"path\": csv_path,\n",
        "                        \"table_name\": table_name,\n",
        "                        \"reason\": \"DataFrame is empty\"\n",
        "                    })\n",
        "                    continue\n",
        "\n",
        "                # Check for suspicious dimensions\n",
        "                rows, cols = df.shape\n",
        "                if rows < 2:  # Should have at least header + 1 data row\n",
        "                    issues[\"truncated_tables\"].append({\n",
        "                        \"identifier\": identifier,\n",
        "                        \"path\": csv_path,\n",
        "                        \"rows\": rows,\n",
        "                        \"cols\": cols,\n",
        "                        \"reason\": \"Too few rows (possible truncation)\"\n",
        "                    })\n",
        "\n",
        "                if cols < 2:  # Tables should have at least 2 columns\n",
        "                    issues[\"suspicious_size\"].append({\n",
        "                        \"identifier\": identifier,\n",
        "                        \"path\": csv_path,\n",
        "                        \"rows\": rows,\n",
        "                        \"cols\": cols,\n",
        "                        \"reason\": \"Too few columns\"\n",
        "                    })\n",
        "\n",
        "                # Check for extremely large tables (possible extraction error)\n",
        "                if rows > 1000 or cols > 50:\n",
        "                    issues[\"suspicious_size\"].append({\n",
        "                        \"identifier\": identifier,\n",
        "                        \"path\": csv_path,\n",
        "                        \"rows\": rows,\n",
        "                        \"cols\": cols,\n",
        "                        \"reason\": \"Unusually large table\"\n",
        "                    })\n",
        "\n",
        "                # Check first row for \"לוח\"\n",
        "                has_luach = False\n",
        "                if len(df) > 0:\n",
        "                    first_row = df.iloc[0]\n",
        "                    for cell in first_row:\n",
        "                        if isinstance(cell, str) and \"לוח\" in cell:\n",
        "                            has_luach = True\n",
        "                            break\n",
        "\n",
        "                if not has_luach:\n",
        "                    issues[\"missing_luach\"].append({\n",
        "                        \"identifier\": identifier,\n",
        "                        \"path\": csv_path,\n",
        "                        \"first_row_sample\": str(first_row.iloc[0])[:50] if len(first_row) > 0 else \"N/A\",\n",
        "                        \"table_name\": table_name\n",
        "                    })\n",
        "\n",
        "                # If we got here, file is basically valid\n",
        "                if not file_issues:\n",
        "                    valid_files += 1\n",
        "\n",
        "            except UnicodeDecodeError as e:\n",
        "                issues[\"encoding_errors\"].append({\n",
        "                    \"identifier\": identifier,\n",
        "                    \"path\": csv_path,\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "            except pd.errors.EmptyDataError:\n",
        "                issues[\"empty_files\"].append({\n",
        "                    \"identifier\": identifier,\n",
        "                    \"path\": csv_path,\n",
        "                    \"reason\": \"Empty CSV file\"\n",
        "                })\n",
        "            except Exception as e:\n",
        "                issues[\"malformed_csv\"].append({\n",
        "                    \"identifier\": identifier,\n",
        "                    \"path\": csv_path,\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Error processing {identifier}: {e}\")\n",
        "\n",
        "    # Generate report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🔍 DATA INTEGRITY VALIDATION REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    total_issues = sum(len(issue_list) for issue_list in issues.values())\n",
        "\n",
        "    print(f\"📊 Files processed: {processed}\")\n",
        "    print(f\"✅ Valid files: {valid_files}\")\n",
        "    print(f\"❌ Files with issues: {total_issues}\")\n",
        "\n",
        "    for issue_type, issue_list in issues.items():\n",
        "        if issue_list:\n",
        "            print(f\"\\n🚨 {issue_type.replace('_', ' ').upper()} ({len(issue_list)}):\")\n",
        "            for item in issue_list[:3]:  # Show first 3\n",
        "                if issue_type == \"empty_files\":\n",
        "                    print(f\"  • {item['identifier']}: {item.get('reason', 'Empty file')}\")\n",
        "                elif issue_type == \"truncated_tables\":\n",
        "                    print(f\"  • {item['identifier']}: {item['rows']}x{item['cols']} - {item['reason']}\")\n",
        "                elif issue_type == \"suspicious_size\":\n",
        "                    print(f\"  • {item['identifier']}: {item['rows']}x{item['cols']} - {item['reason']}\")\n",
        "                elif issue_type == \"missing_luach\":\n",
        "                    print(f\"  • {item['identifier']}: First cell: '{item['first_row_sample']}'\")\n",
        "                elif issue_type == \"encoding_errors\":\n",
        "                    print(f\"  • {item['identifier']}: {item['error']}\")\n",
        "                elif issue_type == \"malformed_csv\":\n",
        "                    print(f\"  • {item['identifier']}: {item['error']}\")\n",
        "\n",
        "            if len(issue_list) > 3:\n",
        "                print(f\"  ... and {len(issue_list) - 3} more\")\n",
        "\n",
        "    # Calculate integrity score\n",
        "    integrity_score = (valid_files / processed) * 100 if processed > 0 else 0\n",
        "\n",
        "    print(f\"\\n📈 INTEGRITY SCORE: {integrity_score:.1f}% ({valid_files}/{processed})\")\n",
        "\n",
        "    # Save detailed report\n",
        "    if not os.path.exists(tables_dir + \"/validations/\"):\n",
        "      os.mkdir(tables_dir + \"/validations/\")\n",
        "    report_path = os.path.join(tables_dir, \"validations\", \"validation_data_integrity.json\")\n",
        "    report = {\n",
        "        \"summary\": {\n",
        "            \"files_processed\": processed,\n",
        "            \"valid_files\": valid_files,\n",
        "            \"total_issues\": total_issues,\n",
        "            \"integrity_score\": integrity_score\n",
        "        },\n",
        "        \"issues\": issues\n",
        "    }\n",
        "\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\n💾 Detailed report saved to: {report_path}\")\n",
        "\n",
        "    return integrity_score > 90  # Consider success if >90% files are valid\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run integrity check\n",
        "    is_valid = check_data_integrity()\n",
        "\n",
        "    if is_valid:\n",
        "        print(\"\\n✅ Data integrity validation PASSED\")\n",
        "    else:\n",
        "        print(\"\\n❌ Data integrity validation FAILED - review integrity issues\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efy-iNkgKWkI",
        "outputId": "4b3a72ea-7c20-47e6-ec5d-ea04b96dc19e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Checking data integrity for 6732 CSV files...\n",
            "\n",
            "============================================================\n",
            "🔍 DATA INTEGRITY VALIDATION REPORT\n",
            "============================================================\n",
            "📊 Files processed: 6732\n",
            "✅ Valid files: 6732\n",
            "❌ Files with issues: 451\n",
            "\n",
            "🚨 TRUNCATED TABLES (255):\n",
            "  • 1_07_2001: 1x1 - Too few rows (possible truncation)\n",
            "  • 8_14_2001: 1x1 - Too few rows (possible truncation)\n",
            "  • 1_07_2002: 1x1 - Too few rows (possible truncation)\n",
            "  ... and 252 more\n",
            "\n",
            "🚨 SUSPICIOUS SIZE (196):\n",
            "  • 1_07_2001: 1x1 - Too few columns\n",
            "  • 8_14_2001: 1x1 - Too few columns\n",
            "  • 1_07_2002: 1x1 - Too few columns\n",
            "  ... and 193 more\n",
            "\n",
            "📈 INTEGRITY SCORE: 100.0% (6732/6732)\n",
            "\n",
            "💾 Detailed report saved to: /content/tables/validations/validation_data_integrity.json\n",
            "\n",
            "✅ Data integrity validation PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "def validate_content_patterns(tables_dir=\"/content/tables\"):\n",
        "    \"\"\"\n",
        "    Validate that extracted tables contain Hebrew text and proper table structure patterns.\n",
        "    \"\"\"\n",
        "    summary_path = os.path.join(tables_dir, \"tables_summary.json\")\n",
        "\n",
        "    if not os.path.exists(summary_path):\n",
        "        print(f\"❌ ERROR: tables_summary.json not found at {summary_path}\")\n",
        "        return False\n",
        "\n",
        "    # Load summary data\n",
        "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        summary = json.load(f)\n",
        "\n",
        "    print(f\"🔤 Validating content patterns for {len(summary)} CSV files...\")\n",
        "\n",
        "    # Hebrew character range\n",
        "    hebrew_pattern = re.compile(r'[\\u0590-\\u05FF]')\n",
        "\n",
        "    issues = {\n",
        "        \"no_hebrew_content\": [],\n",
        "        \"all_empty_cells\": [],\n",
        "        \"no_table_structure\": [],\n",
        "        \"suspicious_content\": []\n",
        "    }\n",
        "\n",
        "    processed = 0\n",
        "    valid_files = 0\n",
        "    hebrew_stats = {\"files_with_hebrew\": 0, \"total_hebrew_chars\": 0}\n",
        "\n",
        "    for identifier, table_name in summary.items():\n",
        "        try:\n",
        "            # Parse identifier and construct path\n",
        "            parts = identifier.split(\"_\")\n",
        "            if len(parts) != 3:\n",
        "                continue\n",
        "\n",
        "            serial, chapter, year = parts\n",
        "            csv_path = os.path.join(tables_dir, year, chapter, f\"{identifier}.csv\")\n",
        "\n",
        "            if not os.path.exists(csv_path):\n",
        "                continue  # Already handled by previous validators\n",
        "\n",
        "            processed += 1\n",
        "            file_valid = True\n",
        "\n",
        "            # Read CSV\n",
        "            try:\n",
        "                df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
        "\n",
        "                if df.empty:\n",
        "                    continue  # Already handled by integrity checker\n",
        "\n",
        "                # Convert all data to string for analysis\n",
        "                all_text = \"\"\n",
        "                non_empty_cells = 0\n",
        "                total_cells = df.shape[0] * df.shape[1]\n",
        "\n",
        "                for col in df.columns:\n",
        "                    for value in df[col]:\n",
        "                        if pd.notna(value) and str(value).strip():\n",
        "                            all_text += str(value) + \" \"\n",
        "                            non_empty_cells += 1\n",
        "\n",
        "                # Check 1: Hebrew content\n",
        "                hebrew_matches = hebrew_pattern.findall(all_text)\n",
        "                has_hebrew = len(hebrew_matches) > 0\n",
        "\n",
        "                if not has_hebrew:\n",
        "                    issues[\"no_hebrew_content\"].append({\n",
        "                        \"identifier\": identifier,\n",
        "                        \"path\": csv_path,\n",
        "                        \"sample_content\": all_text[:100].strip(),\n",
        "                        \"table_name\": table_name\n",
        "                    })\n",
        "                    file_valid = False\n",
        "                else:\n",
        "                    hebrew_stats[\"files_with_hebrew\"] += 1\n",
        "                    hebrew_stats[\"total_hebrew_chars\"] += len(hebrew_matches)\n",
        "\n",
        "                # Check 2: Empty table (all cells empty or whitespace)\n",
        "                if non_empty_cells == 0:\n",
        "                    issues[\"all_empty_cells\"].append({\n",
        "                        \"identifier\": identifier,\n",
        "                        \"path\": csv_path,\n",
        "                        \"total_cells\": total_cells\n",
        "                    })\n",
        "                    file_valid = False\n",
        "\n",
        "                # Check 3: Table structure (reasonable content distribution)\n",
        "                rows, cols = df.shape\n",
        "                content_density = non_empty_cells / total_cells if total_cells > 0 else 0\n",
        "\n",
        "                if content_density < 0.1:  # Less than 10% of cells have content\n",
        "                    issues[\"no_table_structure\"].append({\n",
        "                        \"identifier\": identifier,\n",
        "                        \"path\": csv_path,\n",
        "                        \"rows\": rows,\n",
        "                        \"cols\": cols,\n",
        "                        \"content_density\": f\"{content_density:.2%}\",\n",
        "                        \"non_empty_cells\": non_empty_cells\n",
        "                    })\n",
        "                    file_valid = False\n",
        "\n",
        "                # Check 4: Suspicious content patterns\n",
        "                suspicious_patterns = []\n",
        "\n",
        "                # Check for tables that are mostly numbers (might be extracted incorrectly)\n",
        "                digit_chars = len(re.findall(r'\\d', all_text))\n",
        "                total_chars = len(all_text.replace(' ', ''))\n",
        "\n",
        "                if total_chars > 0 and (digit_chars / total_chars) > 0.8:\n",
        "                    suspicious_patterns.append(\"mostly_numeric\")\n",
        "\n",
        "                # Check for repeated identical content (possible extraction duplication)\n",
        "                words = all_text.split()\n",
        "                if len(words) > 10:\n",
        "                    unique_words = set(words)\n",
        "                    if len(unique_words) / len(words) < 0.3:  # Less than 30% unique words\n",
        "                        suspicious_patterns.append(\"repetitive_content\")\n",
        "\n",
        "                # Check for extremely long single cells (possible formatting issue)\n",
        "                max_cell_length = 0\n",
        "                for col in df.columns:\n",
        "                    for value in df[col]:\n",
        "                        if pd.notna(value):\n",
        "                            cell_length = len(str(value))\n",
        "                            max_cell_length = max(max_cell_length, cell_length)\n",
        "\n",
        "                if max_cell_length > 500:  # Single cell with >500 characters\n",
        "                    suspicious_patterns.append(\"oversized_cells\")\n",
        "\n",
        "                if suspicious_patterns:\n",
        "                    issues[\"suspicious_content\"].append({\n",
        "                        \"identifier\": identifier,\n",
        "                        \"path\": csv_path,\n",
        "                        \"patterns\": suspicious_patterns,\n",
        "                        \"details\": {\n",
        "                            \"digit_ratio\": f\"{digit_chars/total_chars:.2%}\" if total_chars > 0 else \"N/A\",\n",
        "                            \"max_cell_length\": max_cell_length,\n",
        "                            \"content_density\": f\"{content_density:.2%}\"\n",
        "                        }\n",
        "                    })\n",
        "\n",
        "                if file_valid:\n",
        "                    valid_files += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                issues[\"suspicious_content\"].append({\n",
        "                    \"identifier\": identifier,\n",
        "                    \"path\": csv_path,\n",
        "                    \"patterns\": [\"read_error\"],\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Error processing {identifier}: {e}\")\n",
        "\n",
        "    # Generate report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🔤 CONTENT PATTERN VALIDATION REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    total_issues = sum(len(issue_list) for issue_list in issues.values())\n",
        "\n",
        "    print(f\"📊 Files processed: {processed}\")\n",
        "    print(f\"✅ Valid patterns: {valid_files}\")\n",
        "    print(f\"❌ Pattern issues: {total_issues}\")\n",
        "    print(f\"🔤 Files with Hebrew: {hebrew_stats['files_with_hebrew']} ({hebrew_stats['files_with_hebrew']/processed:.1%})\")\n",
        "    print(f\"🔤 Total Hebrew characters: {hebrew_stats['total_hebrew_chars']:,}\")\n",
        "\n",
        "    for issue_type, issue_list in issues.items():\n",
        "        if issue_list:\n",
        "            print(f\"\\n🚨 {issue_type.replace('_', ' ').upper()} ({len(issue_list)}):\")\n",
        "            for item in issue_list[:3]:  # Show first 3\n",
        "                if issue_type == \"no_hebrew_content\":\n",
        "                    print(f\"  • {item['identifier']}\")\n",
        "                    print(f\"    Sample: '{item['sample_content']}'\")\n",
        "                elif issue_type == \"all_empty_cells\":\n",
        "                    print(f\"  • {item['identifier']}: {item['total_cells']} total cells, all empty\")\n",
        "                elif issue_type == \"no_table_structure\":\n",
        "                    print(f\"  • {item['identifier']}: {item['rows']}x{item['cols']}, density: {item['content_density']}\")\n",
        "                elif issue_type == \"suspicious_content\":\n",
        "                    print(f\"  • {item['identifier']}: {', '.join(item['patterns'])}\")\n",
        "\n",
        "            if len(issue_list) > 3:\n",
        "                print(f\"  ... and {len(issue_list) - 3} more\")\n",
        "\n",
        "    # Calculate pattern validity score\n",
        "    pattern_score = (valid_files / processed) * 100 if processed > 0 else 0\n",
        "\n",
        "    print(f\"\\n📈 PATTERN VALIDITY SCORE: {pattern_score:.1f}% ({valid_files}/{processed})\")\n",
        "\n",
        "    # Save detailed report\n",
        "    if not os.path.exists(tables_dir + \"/validations/\"):\n",
        "      os.mkdir(tables_dir + \"/validations/\")\n",
        "    report_path = os.path.join(tables_dir, \"validations\", \"validation_content_patterns.json\")\n",
        "    report = {\n",
        "        \"summary\": {\n",
        "            \"files_processed\": processed,\n",
        "            \"valid_patterns\": valid_files,\n",
        "            \"total_issues\": total_issues,\n",
        "            \"pattern_score\": pattern_score,\n",
        "            \"hebrew_stats\": hebrew_stats\n",
        "        },\n",
        "        \"issues\": issues\n",
        "    }\n",
        "\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\n💾 Detailed report saved to: {report_path}\")\n",
        "\n",
        "    return pattern_score > 85  # Consider success if >85% files have valid patterns\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run pattern validation\n",
        "    is_valid = validate_content_patterns()\n",
        "\n",
        "    if is_valid:\n",
        "        print(\"\\n✅ Content pattern validation PASSED\")\n",
        "    else:\n",
        "        print(\"\\n❌ Content pattern validation FAILED - review pattern issues\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2q3oBDOKwCz",
        "outputId": "83a3bbc8-f6ba-49b6-8b71-d6b86db069d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔤 Validating content patterns for 6732 CSV files...\n",
            "\n",
            "============================================================\n",
            "🔤 CONTENT PATTERN VALIDATION REPORT\n",
            "============================================================\n",
            "📊 Files processed: 6732\n",
            "✅ Valid patterns: 6732\n",
            "❌ Pattern issues: 1906\n",
            "🔤 Files with Hebrew: 6732 (100.0%)\n",
            "🔤 Total Hebrew characters: 8,807,636\n",
            "\n",
            "🚨 SUSPICIOUS CONTENT (1906):\n",
            "  • 2_06_2001: repetitive_content\n",
            "  • 2_03_2001: repetitive_content\n",
            "  • 14_03_2001: repetitive_content\n",
            "  ... and 1903 more\n",
            "\n",
            "📈 PATTERN VALIDITY SCORE: 100.0% (6732/6732)\n",
            "\n",
            "💾 Detailed report saved to: /content/tables/validations/validation_content_patterns.json\n",
            "\n",
            "✅ Content pattern validation PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from docx import Document\n",
        "from pathlib import Path\n",
        "\n",
        "def map_table_positions(base_dir=\"/content/reports\", tables_dir=\"/content/tables\"):\n",
        "    \"\"\"\n",
        "    Re-scan original documents to map positions of all \"לוח\" tables\n",
        "    and compare against extracted tables to detect gaps.\n",
        "    \"\"\"\n",
        "    summary_path = os.path.join(tables_dir, \"tables_summary.json\")\n",
        "\n",
        "    if not os.path.exists(summary_path):\n",
        "        print(f\"❌ ERROR: tables_summary.json not found at {summary_path}\")\n",
        "        return False\n",
        "\n",
        "    # Load extraction summary\n",
        "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        extracted_summary = json.load(f)\n",
        "\n",
        "    print(f\"📍 Mapping table positions and comparing against {len(extracted_summary)} extracted tables...\")\n",
        "\n",
        "    position_map = {}\n",
        "    comparison_results = {\n",
        "        \"documents_processed\": 0,\n",
        "        \"documents_with_gaps\": [],\n",
        "        \"position_mismatches\": [],\n",
        "        \"extraction_summary\": {}\n",
        "    }\n",
        "\n",
        "    # Process each year and document\n",
        "    for year in range(2001, 2025):\n",
        "        year_path = os.path.join(base_dir, str(year))\n",
        "        if not os.path.isdir(year_path):\n",
        "            continue\n",
        "\n",
        "        for fname in os.listdir(year_path):\n",
        "            if not fname.endswith(\".docx\"):\n",
        "                continue\n",
        "\n",
        "            chapter = fname.split(\"_\")[0]\n",
        "            fpath = os.path.join(year_path, fname)\n",
        "            doc_key = f\"{chapter}_{year}\"\n",
        "\n",
        "            try:\n",
        "                doc = Document(fpath)\n",
        "                comparison_results[\"documents_processed\"] += 1\n",
        "\n",
        "                # Find all \"לוח\" tables with their positions\n",
        "                found_tables = []\n",
        "\n",
        "                for table_idx, table in enumerate(doc.tables):\n",
        "                    # Check if this table contains \"לוח\"\n",
        "                    has_luach = False\n",
        "                    luach_text = \"\"\n",
        "\n",
        "                    if len(table.rows) > 0:\n",
        "                        for cell in table.rows[0].cells:\n",
        "                            if \"לוח\" in cell.text:\n",
        "                                has_luach = True\n",
        "                                luach_text = cell.text.strip()\n",
        "                                break\n",
        "\n",
        "                    if has_luach:\n",
        "                        found_tables.append({\n",
        "                            \"document_table_index\": table_idx,\n",
        "                            \"serial_in_doc\": len(found_tables) + 1,\n",
        "                            \"luach_text\": luach_text,\n",
        "                            \"table_dimensions\": f\"{len(table.rows)}x{len(table.columns) if table.rows else 0}\"\n",
        "                        })\n",
        "\n",
        "                # Store position mapping\n",
        "                position_map[doc_key] = {\n",
        "                    \"document_path\": fpath,\n",
        "                    \"found_tables\": found_tables,\n",
        "                    \"total_luach_tables\": len(found_tables)\n",
        "                }\n",
        "\n",
        "                # Compare against extracted tables\n",
        "                extracted_for_doc = {}\n",
        "                for identifier, table_name in extracted_summary.items():\n",
        "                    parts = identifier.split(\"_\")\n",
        "                    if len(parts) == 3:\n",
        "                        serial, id_chapter, id_year = parts\n",
        "                        if f\"{id_chapter}_{id_year}\" == doc_key:\n",
        "                            extracted_for_doc[int(serial)] = {\n",
        "                                \"identifier\": identifier,\n",
        "                                \"table_name\": table_name\n",
        "                            }\n",
        "\n",
        "                # Analyze gaps and mismatches\n",
        "                found_count = len(found_tables)\n",
        "                extracted_count = len(extracted_for_doc)\n",
        "                extracted_serials = sorted(extracted_for_doc.keys())\n",
        "\n",
        "                doc_analysis = {\n",
        "                    \"found_in_document\": found_count,\n",
        "                    \"extracted_count\": extracted_count,\n",
        "                    \"extracted_serials\": extracted_serials,\n",
        "                    \"expected_serials\": list(range(1, found_count + 1)) if found_count > 0 else []\n",
        "                }\n",
        "\n",
        "                # Check for missing serials (gaps)\n",
        "                if found_count > 0:\n",
        "                    expected_serials = set(range(1, found_count + 1))\n",
        "                    actual_serials = set(extracted_serials)\n",
        "                    missing_serials = expected_serials - actual_serials\n",
        "                    extra_serials = actual_serials - expected_serials\n",
        "\n",
        "                    if missing_serials or extra_serials or found_count != extracted_count:\n",
        "                        comparison_results[\"documents_with_gaps\"].append({\n",
        "                            \"document\": doc_key,\n",
        "                            \"path\": fpath,\n",
        "                            \"found_tables\": found_count,\n",
        "                            \"extracted_tables\": extracted_count,\n",
        "                            \"missing_serials\": sorted(list(missing_serials)),\n",
        "                            \"extra_serials\": sorted(list(extra_serials)),\n",
        "                            \"gap_analysis\": {\n",
        "                                \"missing_count\": len(missing_serials),\n",
        "                                \"extra_count\": len(extra_serials),\n",
        "                                \"extraction_rate\": f\"{extracted_count}/{found_count}\" if found_count > 0 else \"0/0\"\n",
        "                            }\n",
        "                        })\n",
        "\n",
        "                comparison_results[\"extraction_summary\"][doc_key] = doc_analysis\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  Error processing {fpath}: {e}\")\n",
        "                comparison_results[\"position_mismatches\"].append({\n",
        "                    \"document\": doc_key,\n",
        "                    \"path\": fpath,\n",
        "                    \"error\": str(e)\n",
        "                })\n",
        "\n",
        "    # Generate report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"📍 TABLE POSITION MAPPING REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    total_docs = comparison_results[\"documents_processed\"]\n",
        "    docs_with_gaps = len(comparison_results[\"documents_with_gaps\"])\n",
        "    docs_perfect = total_docs - docs_with_gaps - len(comparison_results[\"position_mismatches\"])\n",
        "\n",
        "    print(f\"📄 Documents processed: {total_docs}\")\n",
        "    print(f\"✅ Perfect extractions: {docs_perfect}\")\n",
        "    print(f\"⚠️  Documents with gaps: {docs_with_gaps}\")\n",
        "    print(f\"❌ Processing errors: {len(comparison_results['position_mismatches'])}\")\n",
        "\n",
        "    if comparison_results[\"documents_with_gaps\"]:\n",
        "        print(f\"\\n🔍 EXTRACTION GAPS DETECTED ({docs_with_gaps} documents):\")\n",
        "        for gap_info in comparison_results[\"documents_with_gaps\"][:5]:  # Show first 5\n",
        "            gap = gap_info[\"gap_analysis\"]\n",
        "            print(f\"  • {gap_info['document']}: {gap['extraction_rate']}\")\n",
        "            if gap_info[\"missing_serials\"]:\n",
        "                print(f\"    Missing serials: {gap_info['missing_serials']}\")\n",
        "            if gap_info[\"extra_serials\"]:\n",
        "                print(f\"    Extra serials: {gap_info['extra_serials']}\")\n",
        "\n",
        "        if docs_with_gaps > 5:\n",
        "            print(f\"  ... and {docs_with_gaps - 5} more documents with gaps\")\n",
        "\n",
        "    if comparison_results[\"position_mismatches\"]:\n",
        "        print(f\"\\n❌ PROCESSING ERRORS ({len(comparison_results['position_mismatches'])}):\")\n",
        "        for error in comparison_results[\"position_mismatches\"][:3]:\n",
        "            print(f\"  • {error['document']}: {error['error']}\")\n",
        "\n",
        "    # Calculate extraction completeness\n",
        "    total_found = sum(doc_info.get(\"total_luach_tables\", 0) for doc_info in position_map.values())\n",
        "    total_extracted = len(extracted_summary)\n",
        "    completeness_rate = (total_extracted / total_found) * 100 if total_found > 0 else 0\n",
        "\n",
        "    print(f\"\\n📊 OVERALL EXTRACTION COMPLETENESS:\")\n",
        "    print(f\"   Tables found in documents: {total_found}\")\n",
        "    print(f\"   Tables extracted: {total_extracted}\")\n",
        "    print(f\"   Completeness rate: {completeness_rate:.1f}%\")\n",
        "\n",
        "    # Save detailed mapping\n",
        "    mapping_report = {\n",
        "        \"summary\": {\n",
        "            \"documents_processed\": total_docs,\n",
        "            \"perfect_extractions\": docs_perfect,\n",
        "            \"documents_with_gaps\": docs_with_gaps,\n",
        "            \"total_tables_found\": total_found,\n",
        "            \"total_tables_extracted\": total_extracted,\n",
        "            \"completeness_rate\": completeness_rate\n",
        "        },\n",
        "        \"position_mapping\": position_map,\n",
        "        \"gap_analysis\": comparison_results\n",
        "    }\n",
        "\n",
        "    if not os.path.exists(tables_dir + \"/validations/\"):\n",
        "      os.mkdir(tables_dir + \"/validations/\")\n",
        "    report_path = os.path.join(tables_dir, \"validations\", \"validation_position_mapping.json\")\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(mapping_report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\n💾 Detailed mapping saved to: {report_path}\")\n",
        "\n",
        "    return completeness_rate > 95  # Consider success if >95% completeness\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run position mapping\n",
        "    is_complete = map_table_positions()\n",
        "\n",
        "    if is_complete:\n",
        "        print(\"\\n✅ Table position mapping validation PASSED\")\n",
        "    else:\n",
        "        print(\"\\n❌ Table position mapping validation FAILED - review extraction gaps\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf6_r4UgUORK",
        "outputId": "9e5dfdd4-e70d-4c28-b90b-53b0b56c56d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📍 Mapping table positions and comparing against 6732 extracted tables...\n",
            "\n",
            "============================================================\n",
            "📍 TABLE POSITION MAPPING REPORT\n",
            "============================================================\n",
            "📄 Documents processed: 346\n",
            "✅ Perfect extractions: 328\n",
            "⚠️  Documents with gaps: 18\n",
            "❌ Processing errors: 0\n",
            "\n",
            "🔍 EXTRACTION GAPS DETECTED (18 documents):\n",
            "  • 07_2001: 32/33\n",
            "    Missing serials: [33]\n",
            "  • 07_2002: 27/28\n",
            "    Missing serials: [28]\n",
            "  • 07_2003: 25/26\n",
            "    Missing serials: [26]\n",
            "  • 14_2007: 37/38\n",
            "    Missing serials: [38]\n",
            "  • 14_2010: 39/40\n",
            "    Missing serials: [40]\n",
            "  ... and 13 more documents with gaps\n",
            "\n",
            "📊 OVERALL EXTRACTION COMPLETENESS:\n",
            "   Tables found in documents: 6750\n",
            "   Tables extracted: 6732\n",
            "   Completeness rate: 99.7%\n",
            "\n",
            "💾 Detailed mapping saved to: /content/tables/validations/validation_position_mapping.json\n",
            "\n",
            "✅ Table position mapping validation PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "def validate_file_completeness(tables_dir=\"/content/tables\"):\n",
        "    \"\"\"\n",
        "    Validate that all CSV files referenced in tables_summary.json exist\n",
        "    and follow the expected naming convention.\n",
        "    \"\"\"\n",
        "    summary_path = os.path.join(tables_dir, \"tables_summary.json\")\n",
        "\n",
        "    # Check if summary file exists\n",
        "    if not os.path.exists(summary_path):\n",
        "        print(f\"❌ ERROR: tables_summary.json not found at {summary_path}\")\n",
        "        return False\n",
        "\n",
        "    # Load summary data\n",
        "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        summary = json.load(f)\n",
        "\n",
        "    print(f\"📊 Checking {len(summary)} table entries...\")\n",
        "\n",
        "    missing_files = []\n",
        "    naming_issues = []\n",
        "    found_files = []\n",
        "\n",
        "    for identifier, table_name in summary.items():\n",
        "        # Parse identifier: serial_chapter_year\n",
        "        try:\n",
        "            parts = identifier.split(\"_\")\n",
        "            if len(parts) != 3:\n",
        "                naming_issues.append(f\"Invalid identifier format: {identifier}\")\n",
        "                continue\n",
        "\n",
        "            serial, chapter, year = parts\n",
        "\n",
        "            # Construct expected file path\n",
        "            expected_path = os.path.join(tables_dir, year, chapter, f\"{identifier}.csv\")\n",
        "\n",
        "            if os.path.exists(expected_path):\n",
        "                found_files.append(expected_path)\n",
        "            else:\n",
        "                missing_files.append({\n",
        "                    \"identifier\": identifier,\n",
        "                    \"expected_path\": expected_path,\n",
        "                    \"table_name\": table_name\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            naming_issues.append(f\"Error parsing identifier {identifier}: {e}\")\n",
        "\n",
        "    # Generate report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"📋 FILE COMPLETENESS VALIDATION REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"✅ Files found: {len(found_files)}\")\n",
        "    print(f\"❌ Files missing: {len(missing_files)}\")\n",
        "    print(f\"⚠️  Naming issues: {len(naming_issues)}\")\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"\\n🔍 MISSING FILES ({len(missing_files)}):\")\n",
        "        for item in missing_files[:10]:  # Show first 10\n",
        "            print(f\"  • {item['identifier']} → {item['expected_path']}\")\n",
        "            print(f\"    Table: {item['table_name'][:50]}...\")\n",
        "        if len(missing_files) > 10:\n",
        "            print(f\"  ... and {len(missing_files) - 10} more\")\n",
        "\n",
        "    if naming_issues:\n",
        "        print(f\"\\n⚠️  NAMING ISSUES ({len(naming_issues)}):\")\n",
        "        for issue in naming_issues[:5]:  # Show first 5\n",
        "            print(f\"  • {issue}\")\n",
        "        if len(naming_issues) > 5:\n",
        "            print(f\"  ... and {len(naming_issues) - 5} more\")\n",
        "\n",
        "    # Calculate success rate\n",
        "    total_expected = len(summary)\n",
        "    success_rate = (len(found_files) / total_expected) * 100 if total_expected > 0 else 0\n",
        "\n",
        "    print(f\"\\n📈 SUCCESS RATE: {success_rate:.1f}% ({len(found_files)}/{total_expected})\")\n",
        "\n",
        "    # Save detailed report\n",
        "    if not os.path.exists(tables_dir + \"/validations/\"):\n",
        "      os.mkdir(tables_dir + \"/validations/\")\n",
        "    report_path = os.path.join(tables_dir, \"validations\", \"validation_file_completeness.json\")\n",
        "    report = {\n",
        "        \"summary\": {\n",
        "            \"total_expected\": total_expected,\n",
        "            \"files_found\": len(found_files),\n",
        "            \"files_missing\": len(missing_files),\n",
        "            \"naming_issues\": len(naming_issues),\n",
        "            \"success_rate\": success_rate\n",
        "        },\n",
        "        \"missing_files\": missing_files,\n",
        "        \"naming_issues\": naming_issues,\n",
        "        \"found_files\": found_files\n",
        "    }\n",
        "\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\n💾 Detailed report saved to: {report_path}\")\n",
        "\n",
        "    return success_rate > 95  # Consider success if >95% files found\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run validation\n",
        "    is_valid = validate_file_completeness()\n",
        "\n",
        "    if is_valid:\n",
        "        print(\"\\n✅ File completeness validation PASSED\")\n",
        "    else:\n",
        "        print(\"\\n❌ File completeness validation FAILED - review missing files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcDj_NrZOtaP",
        "outputId": "336476d7-abe8-44c4-9360-a69ecd082071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Checking 6732 table entries...\n",
            "\n",
            "============================================================\n",
            "📋 FILE COMPLETENESS VALIDATION REPORT\n",
            "============================================================\n",
            "✅ Files found: 6732\n",
            "❌ Files missing: 0\n",
            "⚠️  Naming issues: 0\n",
            "\n",
            "📈 SUCCESS RATE: 100.0% (6732/6732)\n",
            "\n",
            "💾 Detailed report saved to: /content/tables/validations/validation_file_completeness.json\n",
            "\n",
            "✅ File completeness validation PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "def validate_serial_sequences(tables_dir=\"/content/tables\"):\n",
        "    \"\"\"\n",
        "    Validate that serial numbers within each document are consecutive\n",
        "    (1, 2, 3... with no gaps) by analyzing the tables_summary.json.\n",
        "    \"\"\"\n",
        "    summary_path = os.path.join(tables_dir, \"tables_summary.json\")\n",
        "\n",
        "    if not os.path.exists(summary_path):\n",
        "        print(f\"❌ ERROR: tables_summary.json not found at {summary_path}\")\n",
        "        return False\n",
        "\n",
        "    # Load extraction summary\n",
        "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        summary = json.load(f)\n",
        "\n",
        "    print(f\"🔢 Validating serial number sequences for {len(summary)} extracted tables...\")\n",
        "\n",
        "    # Group by document (chapter_year)\n",
        "    documents = defaultdict(list)\n",
        "\n",
        "    for identifier, table_name in summary.items():\n",
        "        try:\n",
        "            parts = identifier.split(\"_\")\n",
        "            if len(parts) != 3:\n",
        "                print(f\"⚠️  Invalid identifier format: {identifier}\")\n",
        "                continue\n",
        "\n",
        "            serial, chapter, year = parts\n",
        "            doc_key = f\"{chapter}_{year}\"\n",
        "\n",
        "            # Validate serial is numeric\n",
        "            try:\n",
        "                serial_num = int(serial)\n",
        "            except ValueError:\n",
        "                print(f\"⚠️  Non-numeric serial in {identifier}: {serial}\")\n",
        "                continue\n",
        "\n",
        "            documents[doc_key].append({\n",
        "                \"serial\": serial_num,\n",
        "                \"identifier\": identifier,\n",
        "                \"table_name\": table_name\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Error parsing identifier {identifier}: {e}\")\n",
        "\n",
        "    # Analyze sequences for each document\n",
        "    sequence_issues = {\n",
        "        \"missing_serials\": [],\n",
        "        \"duplicate_serials\": [],\n",
        "        \"non_consecutive\": [],\n",
        "        \"starting_issues\": []\n",
        "    }\n",
        "\n",
        "    valid_documents = 0\n",
        "    total_documents = len(documents)\n",
        "\n",
        "    for doc_key, tables in documents.items():\n",
        "        # Sort by serial number\n",
        "        tables.sort(key=lambda x: x[\"serial\"])\n",
        "        serials = [t[\"serial\"] for t in tables]\n",
        "\n",
        "        doc_issues = []\n",
        "\n",
        "        # Check 1: Should start with 1\n",
        "        if serials and serials[0] != 1:\n",
        "            sequence_issues[\"starting_issues\"].append({\n",
        "                \"document\": doc_key,\n",
        "                \"first_serial\": serials[0],\n",
        "                \"expected\": 1,\n",
        "                \"serials\": serials\n",
        "            })\n",
        "            doc_issues.append(\"wrong_start\")\n",
        "\n",
        "        # Check 2: Look for duplicates\n",
        "        seen_serials = set()\n",
        "        duplicates = []\n",
        "        for table in tables:\n",
        "            serial = table[\"serial\"]\n",
        "            if serial in seen_serials:\n",
        "                duplicates.append(serial)\n",
        "            seen_serials.add(serial)\n",
        "\n",
        "        if duplicates:\n",
        "            sequence_issues[\"duplicate_serials\"].append({\n",
        "                \"document\": doc_key,\n",
        "                \"duplicates\": list(set(duplicates)),\n",
        "                \"serials\": serials,\n",
        "                \"affected_tables\": [t[\"identifier\"] for t in tables if t[\"serial\"] in duplicates]\n",
        "            })\n",
        "            doc_issues.append(\"duplicates\")\n",
        "\n",
        "        # Check 3: Look for missing serials (gaps in sequence)\n",
        "        if serials:\n",
        "            expected_max = max(serials)\n",
        "            expected_sequence = set(range(1, expected_max + 1))\n",
        "            actual_sequence = set(serials)\n",
        "            missing = expected_sequence - actual_sequence\n",
        "\n",
        "            if missing:\n",
        "                sequence_issues[\"missing_serials\"].append({\n",
        "                    \"document\": doc_key,\n",
        "                    \"missing_serials\": sorted(list(missing)),\n",
        "                    \"present_serials\": serials,\n",
        "                    \"expected_range\": f\"1-{expected_max}\",\n",
        "                    \"gap_count\": len(missing)\n",
        "                })\n",
        "                doc_issues.append(\"missing\")\n",
        "\n",
        "        # Check 4: Non-consecutive (should be 1,2,3,4... not 1,3,5,7...)\n",
        "        if serials and not duplicates:  # Only if no duplicates\n",
        "            for i in range(len(serials) - 1):\n",
        "                if serials[i+1] != serials[i] + 1:\n",
        "                    sequence_issues[\"non_consecutive\"].append({\n",
        "                        \"document\": doc_key,\n",
        "                        \"serials\": serials,\n",
        "                        \"break_points\": [(serials[i], serials[i+1]) for i in range(len(serials)-1)\n",
        "                                       if serials[i+1] != serials[i] + 1]\n",
        "                    })\n",
        "                    doc_issues.append(\"non_consecutive\")\n",
        "                    break\n",
        "\n",
        "        # Count valid documents (no issues)\n",
        "        if not doc_issues:\n",
        "            valid_documents += 1\n",
        "\n",
        "    # Generate report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🔢 SERIAL NUMBER SEQUENCE VALIDATION REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    total_issues = sum(len(issue_list) for issue_list in sequence_issues.values())\n",
        "\n",
        "    print(f\"📄 Documents analyzed: {total_documents}\")\n",
        "    print(f\"✅ Perfect sequences: {valid_documents}\")\n",
        "    print(f\"⚠️  Documents with issues: {total_documents - valid_documents}\")\n",
        "    print(f\"🚨 Total sequence issues: {total_issues}\")\n",
        "\n",
        "    for issue_type, issue_list in sequence_issues.items():\n",
        "        if issue_list:\n",
        "            print(f\"\\n🔍 {issue_type.replace('_', ' ').upper()} ({len(issue_list)} documents):\")\n",
        "\n",
        "            for item in issue_list[:3]:  # Show first 3\n",
        "                if issue_type == \"missing_serials\":\n",
        "                    print(f\"  • {item['document']}: Missing {item['missing_serials']} from range {item['expected_range']}\")\n",
        "                    print(f\"    Present: {item['present_serials']}\")\n",
        "                elif issue_type == \"duplicate_serials\":\n",
        "                    print(f\"  • {item['document']}: Duplicate serials {item['duplicates']}\")\n",
        "                    print(f\"    Affected: {', '.join(item['affected_tables'][:3])}\")\n",
        "                elif issue_type == \"starting_issues\":\n",
        "                    print(f\"  • {item['document']}: Starts with {item['first_serial']} instead of 1\")\n",
        "                    print(f\"    Sequence: {item['serials']}\")\n",
        "                elif issue_type == \"non_consecutive\":\n",
        "                    print(f\"  • {item['document']}: Non-consecutive sequence\")\n",
        "                    print(f\"    Breaks: {item['break_points']}\")\n",
        "\n",
        "            if len(issue_list) > 3:\n",
        "                print(f\"  ... and {len(issue_list) - 3} more\")\n",
        "\n",
        "    # Calculate sequence validity\n",
        "    sequence_score = (valid_documents / total_documents) * 100 if total_documents > 0 else 0\n",
        "\n",
        "    print(f\"\\n📈 SEQUENCE VALIDITY SCORE: {sequence_score:.1f}% ({valid_documents}/{total_documents})\")\n",
        "\n",
        "    # Generate document-level summary\n",
        "    doc_summary = {}\n",
        "    for doc_key, tables in documents.items():\n",
        "        serials = sorted([t[\"serial\"] for t in tables])\n",
        "        doc_summary[doc_key] = {\n",
        "            \"table_count\": len(tables),\n",
        "            \"serial_range\": f\"{min(serials)}-{max(serials)}\" if serials else \"empty\",\n",
        "            \"is_consecutive\": len(serials) > 0 and serials == list(range(serials[0], serials[-1] + 1)),\n",
        "            \"starts_with_one\": len(serials) > 0 and serials[0] == 1,\n",
        "            \"has_duplicates\": len(serials) != len(set(serials))\n",
        "        }\n",
        "\n",
        "    # Save detailed report\n",
        "    if not os.path.exists(tables_dir + \"/validations/\"):\n",
        "      os.mkdir(tables_dir + \"/validations/\")\n",
        "    report_path = os.path.join(tables_dir, \"validations\", \"validation_serial_sequences.json\")\n",
        "    report = {\n",
        "        \"summary\": {\n",
        "            \"documents_analyzed\": total_documents,\n",
        "            \"perfect_sequences\": valid_documents,\n",
        "            \"documents_with_issues\": total_documents - valid_documents,\n",
        "            \"total_issues\": total_issues,\n",
        "            \"sequence_score\": sequence_score\n",
        "        },\n",
        "        \"issues\": sequence_issues,\n",
        "        \"document_summary\": doc_summary\n",
        "    }\n",
        "\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\n💾 Detailed report saved to: {report_path}\")\n",
        "\n",
        "    return sequence_score > 90  # Consider success if >90% documents have perfect sequences\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run sequence validation\n",
        "    is_valid = validate_serial_sequences()\n",
        "\n",
        "    if is_valid:\n",
        "        print(\"\\n✅ Serial number sequence validation PASSED\")\n",
        "    else:\n",
        "        print(\"\\n❌ Serial number sequence validation FAILED - review sequence issues\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpHNwMzaO7Iw",
        "outputId": "634e7581-b7a8-4068-8307-c576b21ec110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔢 Validating serial number sequences for 6732 extracted tables...\n",
            "\n",
            "============================================================\n",
            "🔢 SERIAL NUMBER SEQUENCE VALIDATION REPORT\n",
            "============================================================\n",
            "📄 Documents analyzed: 316\n",
            "✅ Perfect sequences: 316\n",
            "⚠️  Documents with issues: 0\n",
            "🚨 Total sequence issues: 0\n",
            "\n",
            "📈 SEQUENCE VALIDITY SCORE: 100.0% (316/316)\n",
            "\n",
            "💾 Detailed report saved to: /content/tables/validations/validation_serial_sequences.json\n",
            "\n",
            "✅ Serial number sequence validation PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import statistics\n",
        "from collections import defaultdict\n",
        "\n",
        "def analyze_cross_chapter_patterns(tables_dir=\"/content/tables\"):\n",
        "    \"\"\"\n",
        "    Track historical patterns for each chapter name and flag extreme deviations\n",
        "    (>50% change from historical range) to identify potential extraction issues.\n",
        "    \"\"\"\n",
        "    summary_path = os.path.join(tables_dir, \"tables_summary.json\")\n",
        "\n",
        "    if not os.path.exists(summary_path):\n",
        "        print(f\"❌ ERROR: tables_summary.json not found at {summary_path}\")\n",
        "        return False\n",
        "\n",
        "    # Load extraction summary\n",
        "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        summary = json.load(f)\n",
        "\n",
        "    print(f\"📊 Analyzing cross-chapter patterns for {len(summary)} extracted tables...\")\n",
        "\n",
        "    # Group tables by chapter and year\n",
        "    chapter_data = defaultdict(lambda: defaultdict(int))  # chapter -> year -> count\n",
        "\n",
        "    for identifier, table_name in summary.items():\n",
        "        try:\n",
        "            parts = identifier.split(\"_\")\n",
        "            if len(parts) != 3:\n",
        "                continue\n",
        "\n",
        "            serial, chapter, year = parts\n",
        "            chapter_data[chapter][int(year)] += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Error parsing identifier {identifier}: {e}\")\n",
        "\n",
        "    # Analyze patterns for each chapter\n",
        "    analysis_results = {\n",
        "        \"chapter_patterns\": {},\n",
        "        \"flagged_chapters\": [],\n",
        "        \"year_comparisons\": [],\n",
        "        \"statistical_summary\": {}\n",
        "    }\n",
        "\n",
        "    flagged_count = 0\n",
        "    total_chapters = len(chapter_data)\n",
        "\n",
        "    for chapter, year_counts in chapter_data.items():\n",
        "        years = sorted(year_counts.keys())\n",
        "        counts = [year_counts[year] for year in years]\n",
        "\n",
        "        if len(counts) < 2:  # Need at least 2 years for comparison\n",
        "            continue\n",
        "\n",
        "        # Calculate statistical measures\n",
        "        mean_count = statistics.mean(counts)\n",
        "        median_count = statistics.median(counts)\n",
        "\n",
        "        if len(counts) >= 3:\n",
        "            std_dev = statistics.stdev(counts)\n",
        "        else:\n",
        "            std_dev = 0\n",
        "\n",
        "        min_count = min(counts)\n",
        "        max_count = max(counts)\n",
        "\n",
        "        # Calculate historical range (mean ± 1 std dev, but at least min-max range)\n",
        "        if std_dev > 0:\n",
        "            lower_bound = max(min_count, mean_count - std_dev)\n",
        "            upper_bound = min(max_count, mean_count + std_dev)\n",
        "        else:\n",
        "            lower_bound = min_count\n",
        "            upper_bound = max_count\n",
        "\n",
        "        # Define extreme deviation threshold (50% outside historical range)\n",
        "        deviation_threshold = 0.5\n",
        "        extreme_lower = lower_bound * (1 - deviation_threshold)\n",
        "        extreme_upper = upper_bound * (1 + deviation_threshold)\n",
        "\n",
        "        # Find outliers (years with extreme deviations)\n",
        "        outliers = []\n",
        "        for year in years:\n",
        "            count = year_counts[year]\n",
        "            if count < extreme_lower or count > extreme_upper:\n",
        "                deviation_pct = abs(count - mean_count) / mean_count * 100 if mean_count > 0 else 0\n",
        "                outliers.append({\n",
        "                    \"year\": year,\n",
        "                    \"count\": count,\n",
        "                    \"expected_range\": f\"{lower_bound:.1f}-{upper_bound:.1f}\",\n",
        "                    \"deviation_percent\": deviation_pct,\n",
        "                    \"type\": \"low\" if count < extreme_lower else \"high\"\n",
        "                })\n",
        "\n",
        "        # Store chapter pattern analysis\n",
        "        chapter_pattern = {\n",
        "            \"years_analyzed\": years,\n",
        "            \"table_counts\": counts,\n",
        "            \"statistics\": {\n",
        "                \"mean\": round(mean_count, 1),\n",
        "                \"median\": median_count,\n",
        "                \"std_dev\": round(std_dev, 1),\n",
        "                \"min\": min_count,\n",
        "                \"max\": max_count,\n",
        "                \"range\": f\"{min_count}-{max_count}\"\n",
        "            },\n",
        "            \"historical_bounds\": {\n",
        "                \"lower\": round(lower_bound, 1),\n",
        "                \"upper\": round(upper_bound, 1)\n",
        "            },\n",
        "            \"extreme_thresholds\": {\n",
        "                \"lower\": round(extreme_lower, 1),\n",
        "                \"upper\": round(extreme_upper, 1)\n",
        "            },\n",
        "            \"outliers\": outliers,\n",
        "            \"stability_score\": round((1 - (std_dev / mean_count)) * 100, 1) if mean_count > 0 else 0\n",
        "        }\n",
        "\n",
        "        analysis_results[\"chapter_patterns\"][chapter] = chapter_pattern\n",
        "\n",
        "        # Flag chapters with outliers\n",
        "        if outliers:\n",
        "            flagged_count += 1\n",
        "            analysis_results[\"flagged_chapters\"].append({\n",
        "                \"chapter\": chapter,\n",
        "                \"outlier_count\": len(outliers),\n",
        "                \"outliers\": outliers,\n",
        "                \"severity\": \"high\" if any(o[\"deviation_percent\"] > 75 for o in outliers) else \"medium\"\n",
        "            })\n",
        "\n",
        "    # Generate year-over-year comparison\n",
        "    all_years = sorted(set(year for chapter_years in chapter_data.values() for year in chapter_years.keys()))\n",
        "\n",
        "    for i in range(len(all_years) - 1):\n",
        "        year1, year2 = all_years[i], all_years[i + 1]\n",
        "\n",
        "        year_comparison = {\n",
        "            \"years\": f\"{year1}-{year2}\",\n",
        "            \"chapter_changes\": []\n",
        "        }\n",
        "\n",
        "        for chapter in chapter_data:\n",
        "            count1 = chapter_data[chapter].get(year1, 0)\n",
        "            count2 = chapter_data[chapter].get(year2, 0)\n",
        "\n",
        "            if count1 > 0 and count2 > 0:  # Both years have data\n",
        "                change_pct = ((count2 - count1) / count1) * 100\n",
        "                if abs(change_pct) > 50:  # Significant change\n",
        "                    year_comparison[\"chapter_changes\"].append({\n",
        "                        \"chapter\": chapter,\n",
        "                        \"from_count\": count1,\n",
        "                        \"to_count\": count2,\n",
        "                        \"change_percent\": round(change_pct, 1)\n",
        "                    })\n",
        "\n",
        "        if year_comparison[\"chapter_changes\"]:\n",
        "            analysis_results[\"year_comparisons\"].append(year_comparison)\n",
        "\n",
        "    # Overall statistical summary\n",
        "    all_counts = []\n",
        "    for chapter_pattern in analysis_results[\"chapter_patterns\"].values():\n",
        "        all_counts.extend(chapter_pattern[\"table_counts\"])\n",
        "\n",
        "    if all_counts:\n",
        "        analysis_results[\"statistical_summary\"] = {\n",
        "            \"total_data_points\": len(all_counts),\n",
        "            \"overall_mean\": round(statistics.mean(all_counts), 1),\n",
        "            \"overall_median\": statistics.median(all_counts),\n",
        "            \"overall_std\": round(statistics.stdev(all_counts), 1) if len(all_counts) > 1 else 0,\n",
        "            \"chapters_analyzed\": total_chapters,\n",
        "            \"chapters_flagged\": flagged_count,\n",
        "            \"stability_rate\": round((1 - flagged_count / total_chapters) * 100, 1) if total_chapters > 0 else 0\n",
        "        }\n",
        "\n",
        "    # Generate report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"📊 CROSS-CHAPTER PATTERN ANALYSIS REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    summary_stats = analysis_results[\"statistical_summary\"]\n",
        "    print(f\"📈 Chapters analyzed: {summary_stats['chapters_analyzed']}\")\n",
        "    print(f\"⚠️  Chapters flagged: {summary_stats['chapters_flagged']}\")\n",
        "    print(f\"📊 Stability rate: {summary_stats['stability_rate']}%\")\n",
        "    print(f\"📋 Overall mean tables/chapter: {summary_stats['overall_mean']}\")\n",
        "    print(f\"📋 Overall std deviation: {summary_stats['overall_std']}\")\n",
        "\n",
        "    if analysis_results[\"flagged_chapters\"]:\n",
        "        print(f\"\\n🚨 FLAGGED CHAPTERS FOR MANUAL VERIFICATION ({len(analysis_results['flagged_chapters'])}):\")\n",
        "\n",
        "        # Sort by severity and deviation\n",
        "        flagged_sorted = sorted(analysis_results[\"flagged_chapters\"],\n",
        "                              key=lambda x: (x[\"severity\"] == \"high\", x[\"outlier_count\"]),\n",
        "                              reverse=True)\n",
        "\n",
        "        for flag_info in flagged_sorted[:5]:  # Show top 5\n",
        "            print(f\"  • {flag_info['chapter']} ({flag_info['severity']} severity)\")\n",
        "            for outlier in flag_info[\"outliers\"][:2]:  # Show first 2 outliers\n",
        "                print(f\"    - {outlier['year']}: {outlier['count']} tables \" +\n",
        "                      f\"({outlier['deviation_percent']:.1f}% deviation, expected {outlier['expected_range']})\")\n",
        "\n",
        "        if len(analysis_results[\"flagged_chapters\"]) > 5:\n",
        "            print(f\"  ... and {len(analysis_results['flagged_chapters']) - 5} more\")\n",
        "\n",
        "    if analysis_results[\"year_comparisons\"]:\n",
        "        print(f\"\\n📅 SIGNIFICANT YEAR-OVER-YEAR CHANGES ({len(analysis_results['year_comparisons'])} periods):\")\n",
        "        for comparison in analysis_results[\"year_comparisons\"][:3]:  # Show first 3\n",
        "            print(f\"  • {comparison['years']}: {len(comparison['chapter_changes'])} chapters with >50% change\")\n",
        "\n",
        "    # Save detailed report\n",
        "    report_path = os.path.join(tables_dir, \"validation_cross_chapter_analysis.json\")\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analysis_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\n💾 Detailed analysis saved to: {report_path}\")\n",
        "\n",
        "    # Generate manual verification priority list\n",
        "    verification_list = []\n",
        "    for flag_info in flagged_sorted:\n",
        "        for outlier in flag_info[\"outliers\"]:\n",
        "            verification_list.append({\n",
        "                \"priority\": \"high\" if flag_info[\"severity\"] == \"high\" else \"medium\",\n",
        "                \"chapter\": flag_info[\"chapter\"],\n",
        "                \"year\": outlier[\"year\"],\n",
        "                \"extracted_count\": outlier[\"count\"],\n",
        "                \"expected_range\": outlier[\"expected_range\"],\n",
        "                \"deviation\": f\"{outlier['deviation_percent']:.1f}%\",\n",
        "                \"verification_action\": \"Check TOC manually\"\n",
        "            })\n",
        "\n",
        "    # Save verification priority list\n",
        "    if not os.path.exists(tables_dir + \"/validations/\"):\n",
        "      os.mkdir(tables_dir + \"/validations/\")\n",
        "    verification_path = os.path.join(tables_dir, \"validations\", \"manual_verification_priority.json\")\n",
        "    with open(verification_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(verification_list, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"📋 Manual verification list saved to: {verification_path}\")\n",
        "    print(f\"   → {len(verification_list)} chapter-year combinations need TOC verification\")\n",
        "\n",
        "    return summary_stats[\"stability_rate\"] > 75  # Consider success if >75% chapters are stable\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run cross-chapter analysis\n",
        "    is_stable = analyze_cross_chapter_patterns()\n",
        "\n",
        "    if is_stable:\n",
        "        print(\"\\n✅ Cross-chapter pattern analysis PASSED\")\n",
        "    else:\n",
        "        print(\"\\n❌ Cross-chapter pattern analysis FAILED - many chapters flagged for review\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vs78mXDtPGeR",
        "outputId": "ae0983e2-7ab2-4407-f390-9532d8a6a030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Analyzing cross-chapter patterns for 6732 extracted tables...\n",
            "\n",
            "============================================================\n",
            "📊 CROSS-CHAPTER PATTERN ANALYSIS REPORT\n",
            "============================================================\n",
            "📈 Chapters analyzed: 15\n",
            "⚠️  Chapters flagged: 13\n",
            "📊 Stability rate: 13.3%\n",
            "📋 Overall mean tables/chapter: 21.3\n",
            "📋 Overall std deviation: 14.1\n",
            "\n",
            "🚨 FLAGGED CHAPTERS FOR MANUAL VERIFICATION (13):\n",
            "  • 01 (high severity)\n",
            "    - 2012: 31 tables (137.6% deviation, expected 5.9-20.2)\n",
            "    - 2017: 2 tables (84.7% deviation, expected 5.9-20.2)\n",
            "  • 12 (high severity)\n",
            "    - 2017: 1 tables (97.0% deviation, expected 16.3-50.1)\n",
            "    - 2018: 1 tables (97.0% deviation, expected 16.3-50.1)\n",
            "  • 14 (high severity)\n",
            "    - 2017: 1 tables (96.8% deviation, expected 15.0-48.0)\n",
            "    - 2018: 2 tables (93.7% deviation, expected 15.0-48.0)\n",
            "  • 03 (high severity)\n",
            "    - 2017: 1 tables (94.2% deviation, expected 10.2-24.4)\n",
            "    - 2018: 1 tables (94.2% deviation, expected 10.2-24.4)\n",
            "  • 13 (high severity)\n",
            "    - 2017: 1 tables (95.9% deviation, expected 13.1-35.8)\n",
            "    - 2018: 1 tables (95.9% deviation, expected 13.1-35.8)\n",
            "  ... and 8 more\n",
            "\n",
            "📅 SIGNIFICANT YEAR-OVER-YEAR CHANGES (13 periods):\n",
            "  • 2001-2002: 2 chapters with >50% change\n",
            "  • 2003-2004: 1 chapters with >50% change\n",
            "  • 2004-2005: 1 chapters with >50% change\n",
            "\n",
            "💾 Detailed analysis saved to: /content/tables/validation_cross_chapter_analysis.json\n",
            "📋 Manual verification list saved to: /content/tables/validations/manual_verification_priority.json\n",
            "   → 30 chapter-year combinations need TOC verification\n",
            "\n",
            "❌ Cross-chapter pattern analysis FAILED - many chapters flagged for review\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import statistics\n",
        "from collections import defaultdict\n",
        "\n",
        "def analyze_table_size_distribution(tables_dir=\"/content/tables\"):\n",
        "    \"\"\"\n",
        "    Analyze table size distributions to detect extraction errors through\n",
        "    unusually small (1-2 rows) or extremely large tables.\n",
        "    \"\"\"\n",
        "    summary_path = os.path.join(tables_dir, \"tables_summary.json\")\n",
        "\n",
        "    if not os.path.exists(summary_path):\n",
        "        print(f\"❌ ERROR: tables_summary.json not found at {summary_path}\")\n",
        "        return False\n",
        "\n",
        "    # Load extraction summary\n",
        "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        summary = json.load(f)\n",
        "\n",
        "    print(f\"📏 Analyzing table size distributions for {len(summary)} tables...\")\n",
        "\n",
        "    table_sizes = []\n",
        "    size_issues = {\n",
        "        \"too_small\": [],\n",
        "        \"extremely_large\": [],\n",
        "        \"unreadable\": [],\n",
        "        \"suspicious_dimensions\": []\n",
        "    }\n",
        "\n",
        "    processed = 0\n",
        "\n",
        "    for identifier, table_name in summary.items():\n",
        "        try:\n",
        "            # Parse identifier and construct path\n",
        "            parts = identifier.split(\"_\")\n",
        "            if len(parts) != 3:\n",
        "                continue\n",
        "\n",
        "            serial, chapter, year = parts\n",
        "            csv_path = os.path.join(tables_dir, year, chapter, f\"{identifier}.csv\")\n",
        "\n",
        "            if not os.path.exists(csv_path):\n",
        "                continue  # Already handled by other validators\n",
        "\n",
        "            processed += 1\n",
        "\n",
        "            try:\n",
        "                # Read CSV to get dimensions\n",
        "                df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
        "                rows, cols = df.shape\n",
        "\n",
        "                # Calculate file size for additional context\n",
        "                file_size_bytes = os.path.getsize(csv_path)\n",
        "                file_size_kb = file_size_bytes / 1024\n",
        "\n",
        "                table_info = {\n",
        "                    \"identifier\": identifier,\n",
        "                    \"path\": csv_path,\n",
        "                    \"rows\": rows,\n",
        "                    \"cols\": cols,\n",
        "                    \"file_size_kb\": round(file_size_kb, 2),\n",
        "                    \"table_name\": table_name,\n",
        "                    \"chapter\": chapter,\n",
        "                    \"year\": int(year)\n",
        "                }\n",
        "\n",
        "                table_sizes.append(table_info)\n",
        "\n",
        "                # Flag 1: Too small (suspicious)\n",
        "                if rows <= 2:  # Header + 1 row or less\n",
        "                    size_issues[\"too_small\"].append({\n",
        "                        **table_info,\n",
        "                        \"issue\": f\"Only {rows} rows (likely header only or truncated)\"\n",
        "                    })\n",
        "\n",
        "                # Flag 2: Extremely large (possible extraction error)\n",
        "                if rows > 500 or cols > 30:\n",
        "                    size_issues[\"extremely_large\"].append({\n",
        "                        **table_info,\n",
        "                        \"issue\": f\"Unusually large: {rows}x{cols} (possible merged tables or extraction error)\"\n",
        "                    })\n",
        "\n",
        "                # Flag 3: Suspicious dimensions\n",
        "                suspicious_reasons = []\n",
        "\n",
        "                # Single column tables (unusual for Hebrew statistical tables)\n",
        "                if cols == 1 and rows > 2:\n",
        "                    suspicious_reasons.append(\"single_column\")\n",
        "\n",
        "                # Very wide tables (might be formatting issues)\n",
        "                if cols > 20:\n",
        "                    suspicious_reasons.append(\"too_wide\")\n",
        "\n",
        "                # Extremely rectangular (might indicate extraction problems)\n",
        "                if rows > 0 and cols > 0:\n",
        "                    aspect_ratio = max(rows, cols) / min(rows, cols)\n",
        "                    if aspect_ratio > 20:  # One dimension is 20x larger than other\n",
        "                        suspicious_reasons.append(\"extreme_aspect_ratio\")\n",
        "\n",
        "                # Large file size but small table (might indicate extraction errors)\n",
        "                if file_size_kb > 100 and rows * cols < 50:\n",
        "                    suspicious_reasons.append(\"large_file_small_table\")\n",
        "\n",
        "                if suspicious_reasons:\n",
        "                    size_issues[\"suspicious_dimensions\"].append({\n",
        "                        **table_info,\n",
        "                        \"suspicious_patterns\": suspicious_reasons,\n",
        "                        \"aspect_ratio\": round(max(rows, cols) / min(rows, cols), 1) if rows > 0 and cols > 0 else 0\n",
        "                    })\n",
        "\n",
        "            except Exception as e:\n",
        "                size_issues[\"unreadable\"].append({\n",
        "                    \"identifier\": identifier,\n",
        "                    \"path\": csv_path,\n",
        "                    \"error\": str(e),\n",
        "                    \"table_name\": table_name\n",
        "                })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Error processing {identifier}: {e}\")\n",
        "\n",
        "    # Statistical analysis of table sizes\n",
        "    if table_sizes:\n",
        "        rows_data = [t[\"rows\"] for t in table_sizes]\n",
        "        cols_data = [t[\"cols\"] for t in table_sizes]\n",
        "        size_data = [t[\"file_size_kb\"] for t in table_sizes]\n",
        "\n",
        "        # Calculate statistics\n",
        "        stats = {\n",
        "            \"rows\": {\n",
        "                \"mean\": round(statistics.mean(rows_data), 1),\n",
        "                \"median\": statistics.median(rows_data),\n",
        "                \"std\": round(statistics.stdev(rows_data), 1) if len(rows_data) > 1 else 0,\n",
        "                \"min\": min(rows_data),\n",
        "                \"max\": max(rows_data),\n",
        "                \"quartiles\": [\n",
        "                    round(statistics.quantiles(rows_data, n=4)[0], 1) if len(rows_data) >= 4 else min(rows_data),\n",
        "                    round(statistics.quantiles(rows_data, n=4)[2], 1) if len(rows_data) >= 4 else max(rows_data)\n",
        "                ]\n",
        "            },\n",
        "            \"cols\": {\n",
        "                \"mean\": round(statistics.mean(cols_data), 1),\n",
        "                \"median\": statistics.median(cols_data),\n",
        "                \"std\": round(statistics.stdev(cols_data), 1) if len(cols_data) > 1 else 0,\n",
        "                \"min\": min(cols_data),\n",
        "                \"max\": max(cols_data),\n",
        "                \"quartiles\": [\n",
        "                    round(statistics.quantiles(cols_data, n=4)[0], 1) if len(cols_data) >= 4 else min(cols_data),\n",
        "                    round(statistics.quantiles(cols_data, n=4)[2], 1) if len(cols_data) >= 4 else max(cols_data)\n",
        "                ]\n",
        "            },\n",
        "            \"file_size_kb\": {\n",
        "                \"mean\": round(statistics.mean(size_data), 1),\n",
        "                \"median\": round(statistics.median(size_data), 1),\n",
        "                \"std\": round(statistics.stdev(size_data), 1) if len(size_data) > 1 else 0,\n",
        "                \"min\": round(min(size_data), 2),\n",
        "                \"max\": round(max(size_data), 2)\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Identify statistical outliers using IQR method\n",
        "        def find_outliers(data, multiplier=1.5):\n",
        "            if len(data) < 4:\n",
        "                return []\n",
        "            q1, q3 = statistics.quantiles(data, n=4)[0], statistics.quantiles(data, n=4)[2]\n",
        "            iqr = q3 - q1\n",
        "            lower_bound = q1 - multiplier * iqr\n",
        "            upper_bound = q3 + multiplier * iqr\n",
        "            return [x for x in data if x < lower_bound or x > upper_bound]\n",
        "\n",
        "        row_outliers = find_outliers(rows_data)\n",
        "        col_outliers = find_outliers(cols_data)\n",
        "\n",
        "        # Group tables by year for trend analysis\n",
        "        yearly_stats = defaultdict(lambda: {\"count\": 0, \"avg_rows\": 0, \"avg_cols\": 0})\n",
        "        for table in table_sizes:\n",
        "            year = table[\"year\"]\n",
        "            yearly_stats[year][\"count\"] += 1\n",
        "            yearly_stats[year][\"avg_rows\"] += table[\"rows\"]\n",
        "            yearly_stats[year][\"avg_cols\"] += table[\"cols\"]\n",
        "\n",
        "        for year in yearly_stats:\n",
        "            count = yearly_stats[year][\"count\"]\n",
        "            yearly_stats[year][\"avg_rows\"] = round(yearly_stats[year][\"avg_rows\"] / count, 1)\n",
        "            yearly_stats[year][\"avg_cols\"] = round(yearly_stats[year][\"avg_cols\"] / count, 1)\n",
        "\n",
        "    # Generate report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"📏 TABLE SIZE DISTRIBUTION ANALYSIS REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    total_issues = sum(len(issue_list) for issue_list in size_issues.values())\n",
        "\n",
        "    print(f\"📊 Tables analyzed: {processed}\")\n",
        "    print(f\"⚠️  Size anomalies detected: {total_issues}\")\n",
        "\n",
        "    if table_sizes:\n",
        "        print(f\"\\n📈 SIZE STATISTICS:\")\n",
        "        print(f\"   Rows: μ={stats['rows']['mean']}, σ={stats['rows']['std']}, range={stats['rows']['min']}-{stats['rows']['max']}\")\n",
        "        print(f\"   Cols: μ={stats['cols']['mean']}, σ={stats['cols']['std']}, range={stats['cols']['min']}-{stats['cols']['max']}\")\n",
        "        print(f\"   File Size: μ={stats['file_size_kb']['mean']}KB, range={stats['file_size_kb']['min']}-{stats['file_size_kb']['max']}KB\")\n",
        "\n",
        "        if row_outliers or col_outliers:\n",
        "            print(f\"   Statistical outliers: {len(set(row_outliers + col_outliers))} tables\")\n",
        "\n",
        "    for issue_type, issue_list in size_issues.items():\n",
        "        if issue_list:\n",
        "            print(f\"\\n🚨 {issue_type.replace('_', ' ').upper()} ({len(issue_list)}):\")\n",
        "\n",
        "            for item in issue_list[:3]:  # Show first 3\n",
        "                if issue_type == \"too_small\":\n",
        "                    print(f\"  • {item['identifier']}: {item['rows']}x{item['cols']} - {item['issue']}\")\n",
        "                elif issue_type == \"extremely_large\":\n",
        "                    print(f\"  • {item['identifier']}: {item['rows']}x{item['cols']} ({item['file_size_kb']}KB)\")\n",
        "                elif issue_type == \"suspicious_dimensions\":\n",
        "                    patterns = \", \".join(item['suspicious_patterns'])\n",
        "                    print(f\"  • {item['identifier']}: {item['rows']}x{item['cols']} - {patterns}\")\n",
        "                elif issue_type == \"unreadable\":\n",
        "                    print(f\"  • {item['identifier']}: {item['error']}\")\n",
        "\n",
        "            if len(issue_list) > 3:\n",
        "                print(f\"  ... and {len(issue_list) - 3} more\")\n",
        "\n",
        "    # Calculate size validity score\n",
        "    size_score = ((processed - total_issues) / processed) * 100 if processed > 0 else 0\n",
        "\n",
        "    print(f\"\\n📈 SIZE VALIDITY SCORE: {size_score:.1f}% ({processed - total_issues}/{processed})\")\n",
        "\n",
        "    # Save detailed report\n",
        "    analysis_results = {\n",
        "        \"summary\": {\n",
        "            \"tables_analyzed\": processed,\n",
        "            \"total_issues\": total_issues,\n",
        "            \"size_validity_score\": size_score,\n",
        "            \"statistics\": stats if table_sizes else {}\n",
        "        },\n",
        "        \"size_issues\": size_issues,\n",
        "        \"yearly_trends\": dict(yearly_stats) if table_sizes else {},\n",
        "        \"table_details\": table_sizes\n",
        "    }\n",
        "\n",
        "    if not os.path.exists(tables_dir + \"/validations/\"):\n",
        "      os.mkdir(tables_dir + \"/validations/\")\n",
        "    report_path = os.path.join(tables_dir, \"validations\", \"validation_size_distribution.json\")\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(analysis_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\n💾 Detailed analysis saved to: {report_path}\")\n",
        "\n",
        "    return size_score > 85  # Consider success if >85% tables have normal sizes\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run size distribution analysis\n",
        "    is_valid = analyze_table_size_distribution()\n",
        "\n",
        "    if is_valid:\n",
        "        print(\"\\n✅ Table size distribution analysis PASSED\")\n",
        "    else:\n",
        "        print(\"\\n❌ Table size distribution analysis FAILED - many size anomalies detected\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxfkyeAGP000",
        "outputId": "d578cf51-4bfe-420e-f2da-a8aa74132285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📏 Analyzing table size distributions for 6732 tables...\n",
            "\n",
            "============================================================\n",
            "📏 TABLE SIZE DISTRIBUTION ANALYSIS REPORT\n",
            "============================================================\n",
            "📊 Tables analyzed: 6732\n",
            "⚠️  Size anomalies detected: 469\n",
            "\n",
            "📈 SIZE STATISTICS:\n",
            "   Rows: μ=23.6, σ=26.2, range=1-209\n",
            "   Cols: μ=8.0, σ=4.2, range=1-57\n",
            "   File Size: μ=3.8KB, range=0.11-148.41KB\n",
            "   Statistical outliers: 140 tables\n",
            "\n",
            "🚨 TOO SMALL (260):\n",
            "  • 1_07_2001: 1x1 - Only 1 rows (likely header only or truncated)\n",
            "  • 8_14_2001: 1x1 - Only 1 rows (likely header only or truncated)\n",
            "  • 1_07_2002: 1x1 - Only 1 rows (likely header only or truncated)\n",
            "  ... and 257 more\n",
            "\n",
            "🚨 EXTREMELY LARGE (24):\n",
            "  • 12_10_2004: 193x35 (97.48KB)\n",
            "  • 11_13_2004: 113x56 (82.53KB)\n",
            "  • 9_10_2005: 194x35 (98.36KB)\n",
            "  ... and 21 more\n",
            "\n",
            "🚨 SUSPICIOUS DIMENSIONS (185):\n",
            "  • 6_10_2001: 70x23 - too_wide\n",
            "  • 4_03_2002: 178x23 - too_wide\n",
            "  • 23_07_2002: 29x21 - too_wide\n",
            "  ... and 182 more\n",
            "\n",
            "📈 SIZE VALIDITY SCORE: 93.0% (6263/6732)\n",
            "\n",
            "💾 Detailed analysis saved to: /content/tables/validations/validation_size_distribution.json\n",
            "\n",
            "✅ Table size distribution analysis PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "def generate_completeness_overview(tables_dir=\"/content/tables\"):\n",
        "    \"\"\"\n",
        "    Generate summary matrix showing table counts per chapter-year\n",
        "    for visual pattern recognition and completeness assessment.\n",
        "    \"\"\"\n",
        "    summary_path = os.path.join(tables_dir, \"tables_summary.json\")\n",
        "\n",
        "    if not os.path.exists(summary_path):\n",
        "        print(f\"❌ ERROR: tables_summary.json not found at {summary_path}\")\n",
        "        return False\n",
        "\n",
        "    # Load extraction summary\n",
        "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        summary = json.load(f)\n",
        "\n",
        "    print(f\"📋 Generating completeness overview for {len(summary)} extracted tables...\")\n",
        "\n",
        "    # Group data by chapter and year\n",
        "    data_matrix = defaultdict(lambda: defaultdict(int))  # chapter -> year -> count\n",
        "    all_chapters = set()\n",
        "    all_years = set()\n",
        "\n",
        "    for identifier, table_name in summary.items():\n",
        "        try:\n",
        "            parts = identifier.split(\"_\")\n",
        "            if len(parts) != 3:\n",
        "                continue\n",
        "\n",
        "            serial, chapter, year = parts\n",
        "            year_int = int(year)\n",
        "\n",
        "            data_matrix[chapter][year_int] += 1\n",
        "            all_chapters.add(chapter)\n",
        "            all_years.add(year_int)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Error parsing identifier {identifier}: {e}\")\n",
        "\n",
        "    # Sort for consistent output\n",
        "    all_chapters = sorted(all_chapters)\n",
        "    all_years = sorted(all_years)\n",
        "\n",
        "    # Create DataFrame for matrix representation\n",
        "    matrix_data = []\n",
        "    for chapter in all_chapters:\n",
        "        row = {\"Chapter\": chapter}\n",
        "        for year in all_years:\n",
        "            row[str(year)] = data_matrix[chapter][year]\n",
        "        matrix_data.append(row)\n",
        "\n",
        "    df_matrix = pd.DataFrame(matrix_data)\n",
        "\n",
        "    # Calculate summary statistics\n",
        "    stats = {\n",
        "        \"total_chapters\": len(all_chapters),\n",
        "        \"total_years\": len(all_years),\n",
        "        \"year_range\": f\"{min(all_years)}-{max(all_years)}\",\n",
        "        \"total_extractions\": sum(sum(year_data.values()) for year_data in data_matrix.values())\n",
        "    }\n",
        "\n",
        "    # Analyze completeness patterns\n",
        "    completeness_analysis = {\n",
        "        \"missing_data\": [],  # Chapter-year combinations with 0 tables\n",
        "        \"low_extraction\": [],  # Suspiciously low counts\n",
        "        \"high_extraction\": [],  # Suspiciously high counts\n",
        "        \"chapter_statistics\": {},\n",
        "        \"year_statistics\": {}\n",
        "    }\n",
        "\n",
        "    # Analyze each chapter across years\n",
        "    for chapter in all_chapters:\n",
        "        chapter_counts = [data_matrix[chapter][year] for year in all_years]\n",
        "        non_zero_counts = [c for c in chapter_counts if c > 0]\n",
        "\n",
        "        chapter_stats = {\n",
        "            \"total_tables\": sum(chapter_counts),\n",
        "            \"years_with_data\": len(non_zero_counts),\n",
        "            \"years_missing\": len(all_years) - len(non_zero_counts),\n",
        "            \"avg_tables_per_year\": round(sum(non_zero_counts) / len(non_zero_counts), 1) if non_zero_counts else 0,\n",
        "            \"min_tables\": min(non_zero_counts) if non_zero_counts else 0,\n",
        "            \"max_tables\": max(non_zero_counts) if non_zero_counts else 0,\n",
        "            \"consistency_score\": 0\n",
        "        }\n",
        "\n",
        "        # Calculate consistency (lower std dev = more consistent)\n",
        "        if len(non_zero_counts) > 1:\n",
        "            import statistics\n",
        "            mean_count = statistics.mean(non_zero_counts)\n",
        "            std_dev = statistics.stdev(non_zero_counts)\n",
        "            chapter_stats[\"consistency_score\"] = round(max(0, 100 - (std_dev / mean_count * 100)), 1) if mean_count > 0 else 0\n",
        "        elif len(non_zero_counts) == 1:\n",
        "            chapter_stats[\"consistency_score\"] = 100  # Perfect consistency with 1 data point\n",
        "\n",
        "        completeness_analysis[\"chapter_statistics\"][chapter] = chapter_stats\n",
        "\n",
        "        # Flag missing data\n",
        "        for year in all_years:\n",
        "            if data_matrix[chapter][year] == 0:\n",
        "                completeness_analysis[\"missing_data\"].append({\n",
        "                    \"chapter\": chapter,\n",
        "                    \"year\": year,\n",
        "                    \"expected_range\": f\"{chapter_stats['min_tables']}-{chapter_stats['max_tables']}\" if chapter_stats['max_tables'] > 0 else \"unknown\"\n",
        "                })\n",
        "\n",
        "        # Flag suspiciously low/high extractions\n",
        "        if non_zero_counts:\n",
        "            avg = chapter_stats[\"avg_tables_per_year\"]\n",
        "            for year in all_years:\n",
        "                count = data_matrix[chapter][year]\n",
        "                if count > 0:\n",
        "                    if count < avg * 0.5:  # Less than 50% of average\n",
        "                        completeness_analysis[\"low_extraction\"].append({\n",
        "                            \"chapter\": chapter,\n",
        "                            \"year\": year,\n",
        "                            \"count\": count,\n",
        "                            \"expected_avg\": avg,\n",
        "                            \"deviation_pct\": round((avg - count) / avg * 100, 1)\n",
        "                        })\n",
        "                    elif count > avg * 2:  # More than 200% of average\n",
        "                        completeness_analysis[\"high_extraction\"].append({\n",
        "                            \"chapter\": chapter,\n",
        "                            \"year\": year,\n",
        "                            \"count\": count,\n",
        "                            \"expected_avg\": avg,\n",
        "                            \"deviation_pct\": round((count - avg) / avg * 100, 1)\n",
        "                        })\n",
        "\n",
        "    # Analyze each year across chapters\n",
        "    for year in all_years:\n",
        "        year_counts = [data_matrix[chapter][year] for chapter in all_chapters]\n",
        "        non_zero_counts = [c for c in year_counts if c > 0]\n",
        "\n",
        "        year_stats = {\n",
        "            \"total_tables\": sum(year_counts),\n",
        "            \"chapters_with_data\": len(non_zero_counts),\n",
        "            \"chapters_missing\": len(all_chapters) - len(non_zero_counts),\n",
        "            \"avg_tables_per_chapter\": round(sum(non_zero_counts) / len(non_zero_counts), 1) if non_zero_counts else 0,\n",
        "            \"min_tables\": min(non_zero_counts) if non_zero_counts else 0,\n",
        "            \"max_tables\": max(non_zero_counts) if non_zero_counts else 0\n",
        "        }\n",
        "\n",
        "        completeness_analysis[\"year_statistics\"][year] = year_stats\n",
        "\n",
        "    # Generate visual matrix representation (text-based)\n",
        "    def create_text_matrix():\n",
        "        # Create a compact visual representation\n",
        "        visual_lines = []\n",
        "        visual_lines.append(\"📊 TABLE COUNT MATRIX (Chapter × Year)\")\n",
        "        visual_lines.append(\"=\" * 60)\n",
        "\n",
        "        # Header with years\n",
        "        header = \"Chapter\".ljust(12)\n",
        "        for year in all_years:\n",
        "            header += f\"{str(year)[-2:].rjust(3)}\"  # Last 2 digits of year\n",
        "        visual_lines.append(header)\n",
        "        visual_lines.append(\"-\" * len(header))\n",
        "\n",
        "        # Data rows\n",
        "        for chapter in all_chapters:\n",
        "            row = chapter[:10].ljust(12)  # Truncate long chapter names\n",
        "            for year in all_years:\n",
        "                count = data_matrix[chapter][year]\n",
        "                if count == 0:\n",
        "                    row += \"  .\"\n",
        "                else:\n",
        "                    row += f\"{count:3d}\"\n",
        "            visual_lines.append(row)\n",
        "\n",
        "        return \"\\n\".join(visual_lines)\n",
        "\n",
        "    text_matrix = create_text_matrix()\n",
        "\n",
        "    # Generate report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"📋 CHAPTER COMPLETENESS OVERVIEW REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"📊 Data Coverage:\")\n",
        "    print(f\"   Chapters: {stats['total_chapters']}\")\n",
        "    print(f\"   Years: {stats['total_years']} ({stats['year_range']})\")\n",
        "    print(f\"   Total extractions: {stats['total_extractions']}\")\n",
        "\n",
        "    missing_count = len(completeness_analysis[\"missing_data\"])\n",
        "    low_count = len(completeness_analysis[\"low_extraction\"])\n",
        "    high_count = len(completeness_analysis[\"high_extraction\"])\n",
        "\n",
        "    print(f\"\\n⚠️  Potential Issues:\")\n",
        "    print(f\"   Missing data points: {missing_count}\")\n",
        "    print(f\"   Low extraction flags: {low_count}\")\n",
        "    print(f\"   High extraction flags: {high_count}\")\n",
        "\n",
        "    # Show most consistent chapters\n",
        "    consistent_chapters = sorted(\n",
        "        completeness_analysis[\"chapter_statistics\"].items(),\n",
        "        key=lambda x: x[1][\"consistency_score\"],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\n✅ Most Consistent Chapters (top 5):\")\n",
        "    for chapter, stats_data in consistent_chapters[:5]:\n",
        "        print(f\"   • {chapter}: {stats_data['consistency_score']}% consistency, \" +\n",
        "              f\"avg {stats_data['avg_tables_per_year']} tables/year\")\n",
        "\n",
        "    # Show chapters needing attention\n",
        "    problem_chapters = sorted(\n",
        "        completeness_analysis[\"chapter_statistics\"].items(),\n",
        "        key=lambda x: (x[1][\"years_missing\"], -x[1][\"consistency_score\"])\n",
        "    )\n",
        "\n",
        "    print(f\"\\n🚨 Chapters Needing Attention (top 5):\")\n",
        "    for chapter, stats_data in problem_chapters[-5:]:\n",
        "        issues = []\n",
        "        if stats_data[\"years_missing\"] > 0:\n",
        "            issues.append(f\"{stats_data['years_missing']} missing years\")\n",
        "        if stats_data[\"consistency_score\"] < 50:\n",
        "            issues.append(f\"low consistency ({stats_data['consistency_score']}%)\")\n",
        "\n",
        "        if issues:\n",
        "            print(f\"   • {chapter}: {', '.join(issues)}\")\n",
        "\n",
        "    # Display text matrix\n",
        "    print(f\"\\n{text_matrix}\")\n",
        "\n",
        "    # Save detailed data\n",
        "    overview_data = {\n",
        "        \"summary\": stats,\n",
        "        \"completeness_analysis\": completeness_analysis,\n",
        "        \"data_matrix\": {chapter: dict(year_data) for chapter, year_data in data_matrix.items()},\n",
        "        \"visual_matrix\": text_matrix\n",
        "    }\n",
        "\n",
        "    if not os.path.exists(tables_dir + \"/validations/\"):\n",
        "      os.mkdir(tables_dir + \"/validations/\")\n",
        "    report_path = os.path.join(tables_dir, \"validations\", \"validation_completeness_overview.json\")\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(overview_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Save CSV matrix for external analysis\n",
        "    csv_path = os.path.join(tables_dir, \"validations\", \"completeness_matrix.csv\")\n",
        "    df_matrix.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "    print(f\"\\n💾 Detailed overview saved to: {report_path}\")\n",
        "    print(f\"📊 CSV matrix saved to: {csv_path}\")\n",
        "\n",
        "    # Calculate overall completeness score\n",
        "    total_possible = len(all_chapters) * len(all_years)\n",
        "    actual_data_points = total_possible - missing_count\n",
        "    completeness_score = (actual_data_points / total_possible) * 100 if total_possible > 0 else 0\n",
        "\n",
        "    print(f\"\\n📈 OVERALL COMPLETENESS SCORE: {completeness_score:.1f}% ({actual_data_points}/{total_possible})\")\n",
        "\n",
        "    return completeness_score > 80  # Consider success if >80% completeness\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run completeness overview\n",
        "    is_complete = generate_completeness_overview()\n",
        "\n",
        "    if is_complete:\n",
        "        print(\"\\n✅ Chapter completeness overview PASSED\")\n",
        "    else:\n",
        "        print(\"\\n❌ Chapter completeness overview FAILED - significant data gaps detected\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS8moG43QUlv",
        "outputId": "c9dfa3ca-56a9-4907-e707-9fa884bdab94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📋 Generating completeness overview for 6732 extracted tables...\n",
            "\n",
            "============================================================\n",
            "📋 CHAPTER COMPLETENESS OVERVIEW REPORT\n",
            "============================================================\n",
            "📊 Data Coverage:\n",
            "   Chapters: 15\n",
            "   Years: 24 (2001-2024)\n",
            "   Total extractions: 6732\n",
            "\n",
            "⚠️  Potential Issues:\n",
            "   Missing data points: 44\n",
            "   Low extraction flags: 39\n",
            "   High extraction flags: 3\n",
            "\n",
            "✅ Most Consistent Chapters (top 5):\n",
            "   • 08: 75.1% consistency, avg 35.7 tables/year\n",
            "   • 09: 70.8% consistency, avg 19.8 tables/year\n",
            "   • 10: 69.9% consistency, avg 20.8 tables/year\n",
            "   • 11: 69.5% consistency, avg 6.3 tables/year\n",
            "   • 07: 63.7% consistency, avg 26.8 tables/year\n",
            "\n",
            "🚨 Chapters Needing Attention (top 5):\n",
            "   • 09: 4 missing years\n",
            "   • 10: 4 missing years\n",
            "   • 11: 4 missing years\n",
            "   • 04: 4 missing years\n",
            "   • 15: 10 missing years\n",
            "\n",
            "📊 TABLE COUNT MATRIX (Chapter × Year)\n",
            "============================================================\n",
            "Chapter      01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
            "------------------------------------------------------------------------------------\n",
            "01           23 19 14 16 12 14 15 20 15 14 14 31  . 14 13  .  2  2 11  1 10 13 13  1\n",
            "02           11 15 15 13 13 12 14 15 15 13 16 18 18 21 19 17  1  1 16  . 10 11 10  .\n",
            "03           22 20 20 21 15 20 24 25 20 23 21 21 18 21 22 22  1  1 15  . 15 15 15  1\n",
            "04            4  5  7 11 11 14 14 12 17 14 13 13 11 10 12 10  .  .  6  .  5  5  5  .\n",
            "05           23 38 49 43  . 30 33 34 40 39 46 57 49 59 60 67  1  . 46  1 39 35 35  .\n",
            "06            6  8  7  7 13 15 14  7  . 16 20 19 25 25 19 18  1  1  7  .  4  4  4  .\n",
            "07           32 27 25 24 24 24 30 34 29 33 35 41 29 35 33 33  .  . 36  . 11 11 11  5\n",
            "08           29 29 31 40 36 36 33 41 39 43 36 38 40 42 38 44  .  . 43  . 40 35 35  2\n",
            "09           17 17 17  . 19 21 22 22 21 24 21 31 18 25 26 22  .  . 20  . 17 17 17  1\n",
            "10           10 21 24 21 18 23 20 18 21 25 28 25 24 26 29  .  1  . 22  . 21 19 19  .\n",
            "11            9  9 11  .  5  6  7  8  7  7  5  6  6  6  6  6  .  .  5  .  5  5  5  2\n",
            "12           25 26 27 31 23 33 35 42 41 43 50 59 43 45 52 52  1  1 44  1 39 40 40  4\n",
            "13           20 21 19 20 20 22 24 23 23 34 28 36 33 36 39 40  1  1 38  . 29 27 27  2\n",
            "14           34 35 36 38 28 38 37 43 45 39 44 55 60 10 42 45  1  2 36  2 26 26 28  6\n",
            "15            8  8  5  5  5  4  8 10 11  9 12 15 14  . 12  .  .  .  .  .  .  .  .  .\n",
            "\n",
            "💾 Detailed overview saved to: /content/tables/validations/validation_completeness_overview.json\n",
            "📊 CSV matrix saved to: /content/tables/validations/completeness_matrix.csv\n",
            "\n",
            "📈 OVERALL COMPLETENESS SCORE: 87.8% (316/360)\n",
            "\n",
            "✅ Chapter completeness overview PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def generate_high_risk_samples(tables_dir=\"/content/tables\"):\n",
        "    \"\"\"\n",
        "    Consolidate all validation findings into a prioritized manual verification queue.\n",
        "    Combines results from all previous validation phases to identify highest-risk cases.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load all validation reports\n",
        "    reports = {}\n",
        "    report_files = {\n",
        "        \"completeness\": \"validation_file_completeness.json\",\n",
        "        \"integrity\": \"validation_data_integrity.json\",\n",
        "        \"patterns\": \"validation_content_patterns.json\",\n",
        "        \"positions\": \"validation_position_mapping.json\",\n",
        "        \"sequences\": \"validation_serial_sequences.json\",\n",
        "        \"cross_chapter\": \"validation_cross_chapter_analysis.json\",\n",
        "        \"size_distribution\": \"validation_size_distribution.json\",\n",
        "        \"overview\": \"validation_completeness_overview.json\"\n",
        "    }\n",
        "\n",
        "    print(\"🔍 Loading validation reports...\")\n",
        "\n",
        "    for report_name, filename in report_files.items():\n",
        "        filepath = os.path.join(tables_dir, \"validations\", filename)\n",
        "        if os.path.exists(filepath):\n",
        "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "                reports[report_name] = json.load(f)\n",
        "            print(f\"   ✅ Loaded {report_name}\")\n",
        "        else:\n",
        "            print(f\"   ⚠️  Missing {report_name} report: {filename}\")\n",
        "            reports[report_name] = {}\n",
        "\n",
        "    # Initialize risk assessment structure\n",
        "    risk_assessment = {\n",
        "        \"document_risks\": defaultdict(lambda: {\n",
        "            \"risk_score\": 0,\n",
        "            \"risk_factors\": [],\n",
        "            \"issues\": [],\n",
        "            \"priority\": \"low\"\n",
        "        }),\n",
        "        \"table_risks\": defaultdict(lambda: {\n",
        "            \"risk_score\": 0,\n",
        "            \"risk_factors\": [],\n",
        "            \"issues\": [],\n",
        "            \"priority\": \"low\"\n",
        "        })\n",
        "    }\n",
        "\n",
        "    print(f\"\\n📊 Analyzing risk factors from {len(reports)} validation reports...\")\n",
        "\n",
        "    # Process completeness issues (missing files)\n",
        "    if \"completeness\" in reports and \"missing_files\" in reports[\"completeness\"]:\n",
        "        for missing in reports[\"completeness\"][\"missing_files\"]:\n",
        "            identifier = missing[\"identifier\"]\n",
        "            parts = identifier.split(\"_\")\n",
        "            if len(parts) == 3:\n",
        "                doc_key = f\"{parts[1]}_{parts[2]}\"  # chapter_year\n",
        "\n",
        "                risk_assessment[\"document_risks\"][doc_key][\"risk_score\"] += 10\n",
        "                risk_assessment[\"document_risks\"][doc_key][\"risk_factors\"].append(\"missing_files\")\n",
        "                risk_assessment[\"document_risks\"][doc_key][\"issues\"].append({\n",
        "                    \"type\": \"missing_file\",\n",
        "                    \"identifier\": identifier,\n",
        "                    \"details\": missing[\"table_name\"][:50]\n",
        "                })\n",
        "\n",
        "    # Process integrity issues\n",
        "    if \"integrity\" in reports and \"issues\" in reports[\"integrity\"]:\n",
        "        for issue_type, issue_list in reports[\"integrity\"][\"issues\"].items():\n",
        "            for issue in issue_list:\n",
        "                identifier = issue[\"identifier\"]\n",
        "                parts = identifier.split(\"_\")\n",
        "                if len(parts) == 3:\n",
        "                    doc_key = f\"{parts[1]}_{parts[2]}\"\n",
        "\n",
        "                    # Weight different integrity issues\n",
        "                    weights = {\n",
        "                        \"empty_files\": 8,\n",
        "                        \"truncated_tables\": 6,\n",
        "                        \"encoding_errors\": 5,\n",
        "                        \"missing_luach\": 7,\n",
        "                        \"malformed_csv\": 5,\n",
        "                        \"suspicious_size\": 3\n",
        "                    }\n",
        "\n",
        "                    weight = weights.get(issue_type, 3)\n",
        "                    risk_assessment[\"table_risks\"][identifier][\"risk_score\"] += weight\n",
        "                    risk_assessment[\"table_risks\"][identifier][\"risk_factors\"].append(issue_type)\n",
        "                    risk_assessment[\"table_risks\"][identifier][\"issues\"].append({\n",
        "                        \"type\": issue_type,\n",
        "                        \"details\": issue\n",
        "                    })\n",
        "\n",
        "                    # Also flag document\n",
        "                    risk_assessment[\"document_risks\"][doc_key][\"risk_score\"] += weight // 2\n",
        "                    if issue_type not in risk_assessment[\"document_risks\"][doc_key][\"risk_factors\"]:\n",
        "                        risk_assessment[\"document_risks\"][doc_key][\"risk_factors\"].append(f\"table_{issue_type}\")\n",
        "\n",
        "    # Process pattern issues\n",
        "    if \"patterns\" in reports and \"issues\" in reports[\"patterns\"]:\n",
        "        for issue_type, issue_list in reports[\"patterns\"][\"issues\"].items():\n",
        "            for issue in issue_list:\n",
        "                identifier = issue[\"identifier\"]\n",
        "                parts = identifier.split(\"_\")\n",
        "                if len(parts) == 3:\n",
        "                    doc_key = f\"{parts[1]}_{parts[2]}\"\n",
        "\n",
        "                    # Weight pattern issues\n",
        "                    weights = {\n",
        "                        \"no_hebrew_content\": 9,\n",
        "                        \"all_empty_cells\": 8,\n",
        "                        \"no_table_structure\": 6,\n",
        "                        \"suspicious_content\": 4\n",
        "                    }\n",
        "\n",
        "                    weight = weights.get(issue_type, 3)\n",
        "                    risk_assessment[\"table_risks\"][identifier][\"risk_score\"] += weight\n",
        "                    risk_assessment[\"table_risks\"][identifier][\"risk_factors\"].append(issue_type)\n",
        "                    risk_assessment[\"table_risks\"][identifier][\"issues\"].append({\n",
        "                        \"type\": issue_type,\n",
        "                        \"details\": issue\n",
        "                    })\n",
        "\n",
        "    # Process position mapping gaps\n",
        "    if \"positions\" in reports and \"gap_analysis\" in reports[\"positions\"]:\n",
        "        for gap_doc in reports[\"positions\"][\"gap_analysis\"].get(\"documents_with_gaps\", []):\n",
        "            doc_key = gap_doc[\"document\"]\n",
        "            gap_analysis = gap_doc[\"gap_analysis\"]\n",
        "\n",
        "            # Weight based on severity of gaps\n",
        "            missing_count = gap_analysis[\"missing_count\"]\n",
        "            extra_count = gap_analysis[\"extra_count\"]\n",
        "\n",
        "            risk_score = missing_count * 5 + extra_count * 3\n",
        "            risk_assessment[\"document_risks\"][doc_key][\"risk_score\"] += risk_score\n",
        "            risk_assessment[\"document_risks\"][doc_key][\"risk_factors\"].append(\"extraction_gaps\")\n",
        "            risk_assessment[\"document_risks\"][doc_key][\"issues\"].append({\n",
        "                \"type\": \"extraction_gaps\",\n",
        "                \"missing_serials\": gap_doc.get(\"missing_serials\", []),\n",
        "                \"extra_serials\": gap_doc.get(\"extra_serials\", []),\n",
        "                \"details\": gap_analysis\n",
        "            })\n",
        "\n",
        "    # Process sequence issues\n",
        "    if \"sequences\" in reports and \"issues\" in reports[\"sequences\"]:\n",
        "        for issue_type, issue_list in reports[\"sequences\"][\"issues\"].items():\n",
        "            for issue in issue_list:\n",
        "                doc_key = issue[\"document\"]\n",
        "\n",
        "                # Weight sequence issues\n",
        "                weights = {\n",
        "                    \"missing_serials\": 7,\n",
        "                    \"duplicate_serials\": 8,\n",
        "                    \"non_consecutive\": 5,\n",
        "                    \"starting_issues\": 4\n",
        "                }\n",
        "\n",
        "                weight = weights.get(issue_type, 3)\n",
        "                risk_assessment[\"document_risks\"][doc_key][\"risk_score\"] += weight\n",
        "                risk_assessment[\"document_risks\"][doc_key][\"risk_factors\"].append(issue_type)\n",
        "                risk_assessment[\"document_risks\"][doc_key][\"issues\"].append({\n",
        "                    \"type\": issue_type,\n",
        "                    \"details\": issue\n",
        "                })\n",
        "\n",
        "    # Process cross-chapter anomalies\n",
        "    if \"cross_chapter\" in reports and \"flagged_chapters\" in reports[\"cross_chapter\"]:\n",
        "        for flagged in reports[\"cross_chapter\"][\"flagged_chapters\"]:\n",
        "            chapter = flagged[\"chapter\"]\n",
        "            severity = flagged[\"severity\"]\n",
        "\n",
        "            for outlier in flagged[\"outliers\"]:\n",
        "                year = outlier[\"year\"]\n",
        "                doc_key = f\"{chapter}_{year}\"\n",
        "\n",
        "                # Weight by severity and deviation\n",
        "                weight = 8 if severity == \"high\" else 5\n",
        "                deviation = outlier.get(\"deviation_percent\", 0)\n",
        "                if deviation > 75:\n",
        "                    weight += 3\n",
        "\n",
        "                risk_assessment[\"document_risks\"][doc_key][\"risk_score\"] += weight\n",
        "                risk_assessment[\"document_risks\"][doc_key][\"risk_factors\"].append(\"cross_chapter_anomaly\")\n",
        "                risk_assessment[\"document_risks\"][doc_key][\"issues\"].append({\n",
        "                    \"type\": \"cross_chapter_anomaly\",\n",
        "                    \"severity\": severity,\n",
        "                    \"details\": outlier\n",
        "                })\n",
        "\n",
        "    # Process size distribution anomalies\n",
        "    if \"size_distribution\" in reports and \"size_issues\" in reports[\"size_distribution\"]:\n",
        "        for issue_type, issue_list in reports[\"size_distribution\"][\"size_issues\"].items():\n",
        "            for issue in issue_list:\n",
        "                identifier = issue[\"identifier\"]\n",
        "                parts = identifier.split(\"_\")\n",
        "                if len(parts) == 3:\n",
        "                    doc_key = f\"{parts[1]}_{parts[2]}\"\n",
        "\n",
        "                    # Weight size issues\n",
        "                    weights = {\n",
        "                        \"too_small\": 6,\n",
        "                        \"extremely_large\": 5,\n",
        "                        \"unreadable\": 8,\n",
        "                        \"suspicious_dimensions\": 3\n",
        "                    }\n",
        "\n",
        "                    weight = weights.get(issue_type, 3)\n",
        "                    risk_assessment[\"table_risks\"][identifier][\"risk_score\"] += weight\n",
        "                    risk_assessment[\"table_risks\"][identifier][\"risk_factors\"].append(issue_type)\n",
        "                    risk_assessment[\"table_risks\"][identifier][\"issues\"].append({\n",
        "                        \"type\": issue_type,\n",
        "                        \"details\": issue\n",
        "                    })\n",
        "\n",
        "    # Assign priority levels based on risk scores\n",
        "    def assign_priority(risk_score):\n",
        "        if risk_score >= 20:\n",
        "            return \"critical\"\n",
        "        elif risk_score >= 15:\n",
        "            return \"high\"\n",
        "        elif risk_score >= 8:\n",
        "            return \"medium\"\n",
        "        else:\n",
        "            return \"low\"\n",
        "\n",
        "    # Finalize risk assessment and prioritization\n",
        "    for doc_key, risk_data in risk_assessment[\"document_risks\"].items():\n",
        "        risk_data[\"priority\"] = assign_priority(risk_data[\"risk_score\"])\n",
        "\n",
        "    for table_id, risk_data in risk_assessment[\"table_risks\"].items():\n",
        "        risk_data[\"priority\"] = assign_priority(risk_data[\"risk_score\"])\n",
        "\n",
        "    # Generate prioritized verification queues\n",
        "    doc_queue = sorted(\n",
        "        [(doc_key, data) for doc_key, data in risk_assessment[\"document_risks\"].items()],\n",
        "        key=lambda x: (-x[1][\"risk_score\"], x[0])\n",
        "    )\n",
        "\n",
        "    table_queue = sorted(\n",
        "        [(table_id, data) for table_id, data in risk_assessment[\"table_risks\"].items()],\n",
        "        key=lambda x: (-x[1][\"risk_score\"], x[0])\n",
        "    )\n",
        "\n",
        "    # Generate summary statistics\n",
        "    priority_counts = {\n",
        "        \"documents\": Counter(data[\"priority\"] for _, data in doc_queue),\n",
        "        \"tables\": Counter(data[\"priority\"] for _, data in table_queue)\n",
        "    }\n",
        "\n",
        "    # Generate report\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🎯 HIGH-RISK SAMPLE GENERATOR REPORT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(f\"📊 Risk Assessment Summary:\")\n",
        "    print(f\"   Documents flagged: {len(doc_queue)}\")\n",
        "    print(f\"   Tables flagged: {len(table_queue)}\")\n",
        "\n",
        "    print(f\"\\n🚨 Priority Distribution:\")\n",
        "    print(f\"   Documents - Critical: {priority_counts['documents']['critical']}, \" +\n",
        "          f\"High: {priority_counts['documents']['high']}, \" +\n",
        "          f\"Medium: {priority_counts['documents']['medium']}, \" +\n",
        "          f\"Low: {priority_counts['documents']['low']}\")\n",
        "    print(f\"   Tables - Critical: {priority_counts['tables']['critical']}, \" +\n",
        "          f\"High: {priority_counts['tables']['high']}, \" +\n",
        "          f\"Medium: {priority_counts['tables']['medium']}, \" +\n",
        "          f\"Low: {priority_counts['tables']['low']}\")\n",
        "\n",
        "    print(f\"\\n🎯 TOP PRIORITY DOCUMENTS FOR MANUAL VERIFICATION:\")\n",
        "    for i, (doc_key, risk_data) in enumerate(doc_queue[:10]):\n",
        "        factors = \", \".join(risk_data[\"risk_factors\"][:3])\n",
        "        if len(risk_data[\"risk_factors\"]) > 3:\n",
        "            factors += f\" (+{len(risk_data['risk_factors'])-3} more)\"\n",
        "        print(f\"   {i+1}. {doc_key} (Score: {risk_data['risk_score']}, Priority: {risk_data['priority']})\")\n",
        "        print(f\"      Issues: {factors}\")\n",
        "\n",
        "    print(f\"\\n🔍 TOP PRIORITY TABLES FOR DETAILED REVIEW:\")\n",
        "    for i, (table_id, risk_data) in enumerate(table_queue[:5]):\n",
        "        factors = \", \".join(risk_data[\"risk_factors\"])\n",
        "        print(f\"   {i+1}. {table_id} (Score: {risk_data['risk_score']}, Priority: {risk_data['priority']})\")\n",
        "        print(f\"      Issues: {factors}\")\n",
        "\n",
        "    # Generate actionable verification lists\n",
        "    verification_actions = {\n",
        "        \"immediate_action\": [],  # Critical priority items\n",
        "        \"high_priority\": [],     # High priority items\n",
        "        \"routine_check\": []      # Medium priority items\n",
        "    }\n",
        "\n",
        "    for doc_key, risk_data in doc_queue:\n",
        "        action_item = {\n",
        "            \"document\": doc_key,\n",
        "            \"risk_score\": risk_data[\"risk_score\"],\n",
        "            \"issues\": risk_data[\"issues\"],\n",
        "            \"recommended_action\": \"Compare extraction count against TOC manually\",\n",
        "            \"risk_factors\": risk_data[\"risk_factors\"]\n",
        "        }\n",
        "\n",
        "        if risk_data[\"priority\"] == \"critical\":\n",
        "            verification_actions[\"immediate_action\"].append(action_item)\n",
        "        elif risk_data[\"priority\"] == \"high\":\n",
        "            verification_actions[\"high_priority\"].append(action_item)\n",
        "        elif risk_data[\"priority\"] == \"medium\":\n",
        "            verification_actions[\"routine_check\"].append(action_item)\n",
        "\n",
        "    # Save comprehensive risk assessment\n",
        "    risk_report = {\n",
        "        \"summary\": {\n",
        "            \"documents_flagged\": len(doc_queue),\n",
        "            \"tables_flagged\": len(table_queue),\n",
        "            \"priority_distribution\": priority_counts\n",
        "        },\n",
        "        \"verification_actions\": verification_actions,\n",
        "        \"document_risks\": dict(risk_assessment[\"document_risks\"]),\n",
        "        \"table_risks\": dict(risk_assessment[\"table_risks\"]),\n",
        "        \"prioritized_queues\": {\n",
        "            \"documents\": [(doc, data) for doc, data in doc_queue],\n",
        "            \"tables\": [(table, data) for table, data in table_queue]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if not os.path.exists(tables_dir + \"/validations/\"):\n",
        "      os.mkdir(tables_dir + \"/validations/\")\n",
        "    report_path = os.path.join(tables_dir, \"validations\", \"validation_high_risk_samples.json\")\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(risk_report, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # Generate simplified action list for manual verification\n",
        "    action_list = []\n",
        "    for action in verification_actions[\"immediate_action\"] + verification_actions[\"high_priority\"]:\n",
        "        action_list.append({\n",
        "            \"document\": action[\"document\"],\n",
        "            \"priority\": \"IMMEDIATE\" if action in verification_actions[\"immediate_action\"] else \"HIGH\",\n",
        "            \"risk_score\": action[\"risk_score\"],\n",
        "            \"key_issues\": action[\"risk_factors\"][:3],\n",
        "            \"action\": \"Manual TOC verification required\"\n",
        "        })\n",
        "\n",
        "    action_path = os.path.join(tables_dir, \"validations\", \"manual_verification_actions.json\")\n",
        "    with open(action_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(action_list, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"\\n💾 Risk assessment saved to: {report_path}\")\n",
        "    print(f\"📋 Action list saved to: {action_path}\")\n",
        "    print(f\"\\n📈 RECOMMENDED MANUAL VERIFICATION:\")\n",
        "    print(f\"   Immediate action needed: {len(verification_actions['immediate_action'])} documents\")\n",
        "    print(f\"   High priority review: {len(verification_actions['high_priority'])} documents\")\n",
        "    print(f\"   Routine verification: {len(verification_actions['routine_check'])} documents\")\n",
        "\n",
        "    # Success criteria: reasonable number of high-risk items\n",
        "    total_high_risk = len(verification_actions['immediate_action']) + len(verification_actions['high_priority'])\n",
        "    total_docs = len(risk_assessment[\"document_risks\"])\n",
        "\n",
        "    return total_high_risk < (total_docs * 0.2)  # Success if <20% are high-risk\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run high-risk sample generation\n",
        "    is_manageable = generate_high_risk_samples()\n",
        "\n",
        "    if is_manageable:\n",
        "        print(\"\\n✅ High-risk sample generation PASSED - manageable number of issues\")\n",
        "    else:\n",
        "        print(\"\\n❌ High-risk sample generation FAILED - many high-priority issues detected\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP0yufBMRC2M",
        "outputId": "115f2e9a-ad84-445f-d6ad-7acf380d5132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Loading validation reports...\n",
            "   ✅ Loaded completeness\n",
            "   ✅ Loaded integrity\n",
            "   ✅ Loaded patterns\n",
            "   ⚠️  Missing positions report: validation_position_mapping.json\n",
            "   ✅ Loaded sequences\n",
            "   ✅ Loaded cross_chapter\n",
            "   ✅ Loaded size_distribution\n",
            "   ✅ Loaded overview\n",
            "\n",
            "📊 Analyzing risk factors from 8 validation reports...\n",
            "\n",
            "============================================================\n",
            "🎯 HIGH-RISK SAMPLE GENERATOR REPORT\n",
            "============================================================\n",
            "📊 Risk Assessment Summary:\n",
            "   Documents flagged: 168\n",
            "   Tables flagged: 2168\n",
            "\n",
            "🚨 Priority Distribution:\n",
            "   Documents - Critical: 3, High: 4, Medium: 80, Low: 81\n",
            "   Tables - Critical: 0, High: 196, Medium: 83, Low: 1889\n",
            "\n",
            "🎯 TOP PRIORITY DOCUMENTS FOR MANUAL VERIFICATION:\n",
            "   1. 01_2012 (Score: 29, Priority: critical)\n",
            "      Issues: table_truncated_tables, table_truncated_tables, table_truncated_tables (+6 more)\n",
            "   2. 07_2011 (Score: 25, Priority: critical)\n",
            "      Issues: table_truncated_tables, table_truncated_tables, table_truncated_tables (+6 more)\n",
            "   3. 07_2019 (Score: 24, Priority: critical)\n",
            "      Issues: table_truncated_tables, table_truncated_tables, table_truncated_tables (+9 more)\n",
            "   4. 07_2012 (Score: 19, Priority: high)\n",
            "      Issues: table_truncated_tables, table_truncated_tables, table_truncated_tables (+4 more)\n",
            "   5. 07_2013 (Score: 19, Priority: high)\n",
            "      Issues: table_truncated_tables, table_truncated_tables, table_truncated_tables (+4 more)\n",
            "   6. 14_2015 (Score: 19, Priority: high)\n",
            "      Issues: table_truncated_tables, table_truncated_tables, table_truncated_tables (+6 more)\n",
            "   7. 07_2014 (Score: 16, Priority: high)\n",
            "      Issues: table_truncated_tables, table_truncated_tables, table_truncated_tables (+3 more)\n",
            "   8. 02_2014 (Score: 14, Priority: medium)\n",
            "      Issues: table_truncated_tables, table_truncated_tables, table_truncated_tables (+3 more)\n",
            "   9. 10_2017 (Score: 14, Priority: medium)\n",
            "      Issues: table_truncated_tables, cross_chapter_anomaly\n",
            "   10. 13_2024 (Score: 14, Priority: medium)\n",
            "      Issues: table_truncated_tables, cross_chapter_anomaly\n",
            "\n",
            "🔍 TOP PRIORITY TABLES FOR DETAILED REVIEW:\n",
            "   1. 10_02_2022 (Score: 19, Priority: high)\n",
            "      Issues: truncated_tables, suspicious_size, suspicious_content, too_small\n",
            "   2. 12_02_2016 (Score: 19, Priority: high)\n",
            "      Issues: truncated_tables, suspicious_size, suspicious_content, too_small\n",
            "   3. 13_02_2012 (Score: 19, Priority: high)\n",
            "      Issues: truncated_tables, suspicious_size, suspicious_content, too_small\n",
            "   4. 13_02_2013 (Score: 19, Priority: high)\n",
            "      Issues: truncated_tables, suspicious_size, suspicious_content, too_small\n",
            "   5. 13_02_2019 (Score: 19, Priority: high)\n",
            "      Issues: truncated_tables, suspicious_size, suspicious_content, too_small\n",
            "\n",
            "💾 Risk assessment saved to: /content/tables/validations/validation_high_risk_samples.json\n",
            "📋 Action list saved to: /content/tables/validations/manual_verification_actions.json\n",
            "\n",
            "📈 RECOMMENDED MANUAL VERIFICATION:\n",
            "   Immediate action needed: 3 documents\n",
            "   High priority review: 4 documents\n",
            "   Routine verification: 80 documents\n",
            "\n",
            "✅ High-risk sample generation PASSED - manageable number of issues\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploads content/tables/ folder to drive\n",
        "import os\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "target_folder_id = '1EhFlIijSkO9HzTH6ymdFyE5_wGouS63z'\n",
        "local_file_path = \"/content/tables\"\n",
        "\n",
        "def upload_files_to_drive(local_path, target_folder_id):\n",
        "    \"\"\"\n",
        "    Upload a single file or entire folder structure to a specified Google Drive folder.\n",
        "\n",
        "    Args:\n",
        "        local_path (str): Path to the local file or folder to upload\n",
        "        target_folder_id (str): Drive folder ID where the content should be uploaded\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all uploads successful, False otherwise\n",
        "    \"\"\"\n",
        "    # Handle single file\n",
        "    if os.path.isfile(local_path):\n",
        "        try:\n",
        "            # Extract filename from local path\n",
        "            filename = os.path.basename(local_path)\n",
        "\n",
        "            # Fix duplicate chapter numbers (e.g., \"09_09.docx\" -> \"09.docx\")\n",
        "            if \"_\" in filename and filename.endswith(\".docx\"):\n",
        "                parts = filename.replace(\".docx\", \"\").split(\"_\")\n",
        "                if len(parts) == 2 and parts[0] == parts[1]:\n",
        "                    filename = f\"{parts[0]}.docx\"\n",
        "\n",
        "            # Set up file metadata\n",
        "            file_metadata = {\n",
        "                'name': filename,\n",
        "                'parents': [target_folder_id]\n",
        "            }\n",
        "\n",
        "            # Create media upload object\n",
        "            media = MediaFileUpload(\n",
        "                local_path,\n",
        "                mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document'\n",
        "            )\n",
        "\n",
        "            # Upload the file\n",
        "            uploaded_file = drive_service.files().create(\n",
        "                body=file_metadata,\n",
        "                media_body=media,\n",
        "                fields='id'\n",
        "            ).execute()\n",
        "\n",
        "            file_id = uploaded_file.get('id')\n",
        "            print(f\"✅ Successfully uploaded {filename} (ID: {file_id})\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to upload {local_path}: {e}\")\n",
        "            return False\n",
        "\n",
        "    # Handle folder using os.walk\n",
        "    elif os.path.isdir(local_path):\n",
        "        try:\n",
        "            folder_map = {}  # Maps local folder paths to Drive folder IDs\n",
        "            folder_map[local_path] = target_folder_id  # Root folder maps to target\n",
        "            success_count = 0\n",
        "            total_count = 0\n",
        "\n",
        "            for root, dirs, files in os.walk(local_path):\n",
        "                # Create folders first\n",
        "                for dir_name in dirs:\n",
        "                    total_count += 1\n",
        "                    try:\n",
        "                        current_dir_path = os.path.join(root, dir_name)\n",
        "                        parent_folder_id = folder_map[root]\n",
        "\n",
        "                        # Create folder in Drive\n",
        "                        folder_metadata = {\n",
        "                            'name': dir_name,\n",
        "                            'mimeType': 'application/vnd.google-apps.folder',\n",
        "                            'parents': [parent_folder_id]\n",
        "                        }\n",
        "\n",
        "                        created_folder = drive_service.files().create(\n",
        "                            body=folder_metadata,\n",
        "                            fields='id'\n",
        "                        ).execute()\n",
        "\n",
        "                        folder_id = created_folder.get('id')\n",
        "                        folder_map[current_dir_path] = folder_id\n",
        "                        print(f\"📁 Created folder: {dir_name} (ID: {folder_id})\")\n",
        "                        success_count += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"❌ Failed to create folder {dir_name}: {e}\")\n",
        "\n",
        "                # Upload files in current directory\n",
        "                for file_name in files:\n",
        "                    total_count += 1\n",
        "                    try:\n",
        "                        file_path = os.path.join(root, file_name)\n",
        "                        parent_folder_id = folder_map[root]\n",
        "\n",
        "                        # Fix duplicate chapter numbers (e.g., \"09_09.docx\" -> \"09.docx\")\n",
        "                        display_name = file_name\n",
        "                        if \"_\" in file_name and file_name.endswith(\".docx\"):\n",
        "                            parts = file_name.replace(\".docx\", \"\").split(\"_\")\n",
        "                            if len(parts) == 2 and parts[0] == parts[1]:\n",
        "                                display_name = f\"{parts[0]}.docx\"\n",
        "\n",
        "                        # Set up file metadata\n",
        "                        file_metadata = {\n",
        "                            'name': display_name,\n",
        "                            'parents': [parent_folder_id]\n",
        "                        }\n",
        "\n",
        "                        # Create media upload object\n",
        "                        media = MediaFileUpload(\n",
        "                            file_path,\n",
        "                            mimetype='application/vnd.openxmlformats-officedocument.wordprocessingml.document'\n",
        "                        )\n",
        "\n",
        "                        # Upload the file\n",
        "                        uploaded_file = drive_service.files().create(\n",
        "                            body=file_metadata,\n",
        "                            media_body=media,\n",
        "                            fields='id'\n",
        "                        ).execute()\n",
        "\n",
        "                        file_id = uploaded_file.get('id')\n",
        "                        print(f\"✅ Successfully uploaded {display_name} (ID: {file_id})\")\n",
        "                        success_count += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"❌ Failed to upload {file_name}: {e}\")\n",
        "\n",
        "            print(f\"📊 Upload summary: {success_count}/{total_count} items uploaded successfully\")\n",
        "            return success_count == total_count\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to process folder {local_path}: {e}\")\n",
        "            return False\n",
        "\n",
        "    else:\n",
        "        print(f\"❌ Path does not exist: {local_path}\")\n",
        "        return False\n",
        "\n",
        "upload_files_to_drive(local_file_path, target_folder_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q4ChmqrwZvPA",
        "outputId": "2d0e4238-6b9d-4d4d-ed4f-9154d9a6f09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📁 Created folder: 2004 (ID: 11UCCKzQ8axHtXYemrvuO-4zWXG4hcQqd)\n",
            "📁 Created folder: 2008 (ID: 1SJ38aDr28ZdPtbvfbAOeXY2Uo7vBT5Qs)\n",
            "📁 Created folder: 2016 (ID: 1RcXAQRP_n8gSiwgbyUQS1QRHFK-cxk33)\n",
            "📁 Created folder: .ipynb_checkpoints (ID: 1Gb0eN7flJ0VA26PlZ7P_PcDh-c67kUy1)\n",
            "📁 Created folder: 2011 (ID: 1zfAF8xYPMU8MkS0WojpNkBDqe8pka-Qs)\n",
            "📁 Created folder: 2013 (ID: 1Vag8zU5tWCRQiWp_cInqZRsC8zbW0iti)\n",
            "📁 Created folder: 2007 (ID: 1i_RIo_fAWLc5lRcpBzwinuRGPFMWPMUU)\n",
            "📁 Created folder: 2001 (ID: 1MVV2piMBIN0PegnctM2B0sraEpGKb46E)\n",
            "📁 Created folder: 2023 (ID: 15CNxKks2X1W0N7aMQfOz44BUg4yeGArX)\n",
            "📁 Created folder: 2015 (ID: 1UoZC4PBxSynO-BZtQ9rDvwasmjv16q3h)\n",
            "📁 Created folder: 2018 (ID: 17rdS6eeNyZ0xrwkLAp5MmrtRTpA8ZewG)\n",
            "📁 Created folder: 2021 (ID: 1yeqJGsbgLUHkdZAth_xrKAW0Mo-fy9_l)\n",
            "📁 Created folder: 2006 (ID: 1yp1gOrQ98kAMbb1vsurtE9iWxIPqpI9g)\n",
            "📁 Created folder: 2020 (ID: 19rpGF_Czn5r2WzfYEpYPiLCjgB-wEGmv)\n",
            "📁 Created folder: 2022 (ID: 14qrKXJMNN6B1TN6gEAgcvuz4LotXUL-c)\n",
            "📁 Created folder: 2019 (ID: 1S9FCrv16xJPK_2zKG6E7pBH5Irzt4VSi)\n",
            "📁 Created folder: 2012 (ID: 1BPXSWKfw_Iqk3ZiIK1tZ5dERbbt2rXnQ)\n",
            "📁 Created folder: 2003 (ID: 1FFsJKH8hV_SEnm1VtwWTZhcaUHk_h1io)\n",
            "📁 Created folder: 2010 (ID: 1MWGM1pk_h64Nz3xqkCyua3C2k7xhqNit)\n",
            "📁 Created folder: validations (ID: 1Tm719JX1doEmrwG9vnzjhAtIRPySQBRz)\n",
            "📁 Created folder: 2017 (ID: 1UYgzXfszf4iGEl6GpMoDl3LZSs6aQqK8)\n",
            "📁 Created folder: 2005 (ID: 1Zk1EGjpU-7LjbYDgvrCMC7MrA_iPntxj)\n",
            "📁 Created folder: 2009 (ID: 1lDBNV-ucZ4tvANK19bwB5p-nAA-uW8T1)\n",
            "📁 Created folder: 2002 (ID: 1SjSwJYT1B5mRXmBhfsY3id3KZnkVm5X3)\n",
            "📁 Created folder: 2014 (ID: 1U-teVTgIPTTpeEjKQjDZ_RVh-pY1KgE8)\n",
            "📁 Created folder: 2024 (ID: 1EQvpFlYO9PaxtVYQWVWZrQksPgJIxCxN)\n",
            "✅ Successfully uploaded tables_columns.json (ID: 1cIBbr-_s_rc3Cr5MSd0EHK8N_MillyP-)\n",
            "✅ Successfully uploaded tables_summary.json (ID: 16cz2Rbn8pUXwL3M7cTf3uo0C8QHs9h8j)\n",
            "📁 Created folder: 04 (ID: 1DzV45AEihy93Yhe69hMTevKzwUAD53bm)\n",
            "📁 Created folder: 05 (ID: 1IOxTn3Xaoj8_s_14zf8E-EwWR6KoTAxL)\n",
            "📁 Created folder: 12 (ID: 1tQlG7Nm8L-4YDRMuP_qeaRHz_SrQKHHh)\n",
            "📁 Created folder: 01 (ID: 1BbRltbcm2ADds3GiG0Ag2pMDrMyuzufE)\n",
            "📁 Created folder: 10 (ID: 15Xi-jSnst4Uxhh-YrBLTtarENpR1cQnu)\n",
            "📁 Created folder: 08 (ID: 1ds6dD38AWIxLZaEvXR9kIKegobENRoY7)\n",
            "📁 Created folder: 15 (ID: 1vg7959QQkf-640s2mQPZzy_yiVxtXdz7)\n",
            "📁 Created folder: 13 (ID: 1ritm3jVfECDOiNf4p8krU6WaIEXLDWFa)\n",
            "📁 Created folder: 07 (ID: 1Mx6T1ypYYaMPDFAJRy4UgSMVZvWGHjip)\n",
            "📁 Created folder: 03 (ID: 1t3KlA8FNyULiqTwiJmEZQuoD8MoDGXJU)\n",
            "📁 Created folder: 14 (ID: 1NphuynSNi3V-Zkcy7VqpT0Uwx7s8xS2s)\n",
            "📁 Created folder: 06 (ID: 1jGlauOQfTPbl5Qn3z5uujlIPhF1kKcH4)\n",
            "📁 Created folder: 02 (ID: 16Ju1jFb75-m-D29KEqO-Kdhw0pPBUGvv)\n",
            "✅ Successfully uploaded 5_04_2004.csv (ID: 1kP7Fco-UiUykxvMFrTbYSqCRhRQlaGZF)\n",
            "✅ Successfully uploaded 10_04_2004.csv (ID: 1S-2hVd-U2ZGYzyMqEctPf-XjGF1xGiPJ)\n",
            "✅ Successfully uploaded 7_04_2004.csv (ID: 1DR1ds-wdrOfZe5FtlbANnOvoRcT_S8YU)\n",
            "✅ Successfully uploaded 6_04_2004.csv (ID: 1n4rxcuukdhPFJt9XE6iMRpYJJmZm5QY7)\n",
            "✅ Successfully uploaded 3_04_2004.csv (ID: 1lECAjhZ-7HT0fVAG2MHlAPxVMGZIg7fr)\n",
            "✅ Successfully uploaded 9_04_2004.csv (ID: 1WyAkK2x1RUnvXrKw96vrjh4msJuNkDaG)\n",
            "✅ Successfully uploaded 11_04_2004.csv (ID: 12wbu_ffs34p0PBLd9O2WlwClkWF4zvZM)\n",
            "✅ Successfully uploaded 2_04_2004.csv (ID: 1Vp3q4ezpm1c0pWs8LQZIgXgH4R7VSvnk)\n",
            "✅ Successfully uploaded 1_04_2004.csv (ID: 1wLN0r2kD-7Mfo2C2mwHKtmp62F0JvHv0)\n",
            "✅ Successfully uploaded 4_04_2004.csv (ID: 1FBqKpDVLABEUietWpzuRzEed3eEm2ZqK)\n",
            "✅ Successfully uploaded 8_04_2004.csv (ID: 1ml9WAHrk5fjKP0fZkizdi8qd27KCXoUC)\n",
            "✅ Successfully uploaded 34_05_2004.csv (ID: 1EWF28LiM3ngirVfMIU9omF47K9AZcK2S)\n",
            "✅ Successfully uploaded 43_05_2004.csv (ID: 1y6OEWurVxXo6mWbM6p776OBzWaqwvuEc)\n",
            "✅ Successfully uploaded 7_05_2004.csv (ID: 1hs3HA_GZC_e3xMipzmDw5zTX0hvFmFeO)\n",
            "✅ Successfully uploaded 21_05_2004.csv (ID: 1_p5pWSBC2IAfOymn0gdLdkQtmZRvf-Sz)\n",
            "✅ Successfully uploaded 24_05_2004.csv (ID: 1nvIs2qqeZxKGF_WKcKNBB25hTMh52JNH)\n",
            "✅ Successfully uploaded 19_05_2004.csv (ID: 1_UTb5R3ee3vVHmooWqzofBJVO1s78YKu)\n",
            "✅ Successfully uploaded 38_05_2004.csv (ID: 1_z2O_tsxUiC69i13FyG0x7e-MP_eStdm)\n",
            "✅ Successfully uploaded 9_05_2004.csv (ID: 1_uakeBj95nZ7scapXsUpZTWXZwVonMUX)\n",
            "✅ Successfully uploaded 11_05_2004.csv (ID: 1mUAsx7twdtAO46Qb0j4RTyCIfeC9dx7U)\n",
            "✅ Successfully uploaded 14_05_2004.csv (ID: 1vcK1qZJ44m2HrnIob6JsYjCAf-cNym-K)\n",
            "✅ Successfully uploaded 25_05_2004.csv (ID: 1llrjkdISEfKkXjjLee8vB6tOQA-n22BP)\n",
            "✅ Successfully uploaded 42_05_2004.csv (ID: 1SBskNNS9GydqSXnZJNlSTcAtjG9AYeOD)\n",
            "✅ Successfully uploaded 39_05_2004.csv (ID: 1UZUeDOA8WGrLAoI_fcTh_PLX7bh_YCw4)\n",
            "✅ Successfully uploaded 22_05_2004.csv (ID: 1vdNO1cd07l_rsqf10PNN4loQ1yJjjYzk)\n",
            "✅ Successfully uploaded 16_05_2004.csv (ID: 1s5ZpHG1hsvzQ0HY3CIzTeyWxaVFENayc)\n",
            "✅ Successfully uploaded 18_05_2004.csv (ID: 1GqONJ4kY2UIT3YhrKGZCI3hIyblTe3Li)\n",
            "✅ Successfully uploaded 13_05_2004.csv (ID: 11Irydylqx0oFXT8gyJ-bp0E38y5Ivkq1)\n",
            "✅ Successfully uploaded 26_05_2004.csv (ID: 12Khj2LZsYxeYC0THsiBsv66iSSQu4X8V)\n",
            "✅ Successfully uploaded 29_05_2004.csv (ID: 1hWF1GOyB4jiHR6miKTLJD-Wb4rmzLTS3)\n",
            "✅ Successfully uploaded 4_05_2004.csv (ID: 11VQtbskw4PKwCGjkgpCrOycoTrmAV2Mf)\n",
            "✅ Successfully uploaded 17_05_2004.csv (ID: 18ROqIahzDzHwGWNr4c3cOwGgu6zR_IHu)\n",
            "✅ Successfully uploaded 20_05_2004.csv (ID: 1spTN4kgEgkzL3BRYanWHutKjfIUSxidD)\n",
            "✅ Successfully uploaded 31_05_2004.csv (ID: 1VfhheNPDOXHAruEn24sgoMPyHixusfyc)\n",
            "✅ Successfully uploaded 28_05_2004.csv (ID: 1pufV1zy1QvsNGmxyWeekzIqmJEE_QI0g)\n",
            "✅ Successfully uploaded 15_05_2004.csv (ID: 10IDQ1485g0TdDfDyEs95hpvpXdbgYV3P)\n",
            "✅ Successfully uploaded 33_05_2004.csv (ID: 1C82eVrUm6ScPjSJrBtWXDgct6dAfG3hL)\n",
            "✅ Successfully uploaded 32_05_2004.csv (ID: 1t8TiFIOXGv9WyrrTqdnYz3AaixAjQJ9Z)\n",
            "✅ Successfully uploaded 2_05_2004.csv (ID: 1e2qLExyzu_9nDXl3Zao0J-gVAuAaAlDn)\n",
            "✅ Successfully uploaded 12_05_2004.csv (ID: 1N99BMCpN-0lLlfXcELrPyaME870njDOr)\n",
            "✅ Successfully uploaded 23_05_2004.csv (ID: 1GlTqFJRCwVqAabzAZ8BkVSz1wyQGox_Q)\n",
            "✅ Successfully uploaded 3_05_2004.csv (ID: 1H16ny7lSjVxhtHGr9FjmHiPvw-TJX7K8)\n",
            "✅ Successfully uploaded 30_05_2004.csv (ID: 1zww9tOB3rRq0NBloU7HQkwhpwJkaIDeB)\n",
            "✅ Successfully uploaded 10_05_2004.csv (ID: 1gxlZI5s1xULgGOKx_aU6feHeM0wz7asA)\n",
            "✅ Successfully uploaded 40_05_2004.csv (ID: 1NDwYFo1X1s-qTJk4afcoPDc3JCVouwXI)\n",
            "✅ Successfully uploaded 37_05_2004.csv (ID: 1teJwmTRaozWTl1i-ZMJmygxPFNN2q9e2)\n",
            "✅ Successfully uploaded 27_05_2004.csv (ID: 154EjUXOejatr5ZCx04hVWzpgV3gMDt9B)\n",
            "✅ Successfully uploaded 1_05_2004.csv (ID: 1XLZSSFE4jhSVVX8KrKNRuXYrDAUYvuEU)\n",
            "✅ Successfully uploaded 8_05_2004.csv (ID: 1kqLHQD9K7jfMr1TX46e_44kjdshLOl4d)\n",
            "✅ Successfully uploaded 41_05_2004.csv (ID: 1PdIdTz5yxfUT3T_L8MtBru2pg5YXWfmt)\n",
            "✅ Successfully uploaded 5_05_2004.csv (ID: 16t3h02J0Va2kr0yxXAew-Y9MNkRneZY4)\n",
            "✅ Successfully uploaded 6_05_2004.csv (ID: 1vB0zGEejPTZL07N8dqpOPKZuRW4_s84K)\n",
            "✅ Successfully uploaded 35_05_2004.csv (ID: 1sAR7OYPEp8qi8ja0pkEOKxfvbLL8L1OC)\n",
            "✅ Successfully uploaded 36_05_2004.csv (ID: 1iAohx-E83gR5XpmHYJcbsPIG2mrG5IPJ)\n",
            "✅ Successfully uploaded 30_12_2004.csv (ID: 1rNbDXcNyBhmpMR7YLdSARjeENiAl_aw2)\n",
            "✅ Successfully uploaded 1_12_2004.csv (ID: 1BSM3hXcrI2_MJyRGgJYMFMHshMyzp8YF)\n",
            "✅ Successfully uploaded 17_12_2004.csv (ID: 1-zUHrkSUpIra9lC3ij6V9UqTtR_yy1iM)\n",
            "✅ Successfully uploaded 25_12_2004.csv (ID: 1q47bhtG3pUoY4z9fRcR6jCbafhiMxqL1)\n",
            "✅ Successfully uploaded 11_12_2004.csv (ID: 1hyo_HqZafyFoDZgbUkYBESHAE4_VABqM)\n",
            "✅ Successfully uploaded 12_12_2004.csv (ID: 1KqpoqoLxtDA4c08_2rRPJgygEaf7_0t5)\n",
            "✅ Successfully uploaded 9_12_2004.csv (ID: 1ctCl0Qj0EslaQM_CJQqCDwdJOMxmZIwn)\n",
            "✅ Successfully uploaded 5_12_2004.csv (ID: 12RokyKi2Aidvc6pnmJLSjX19v2_1eCEC)\n",
            "✅ Successfully uploaded 21_12_2004.csv (ID: 1ND7tcWL_StgDxPGBqDlW3u1VlQXzVeD2)\n",
            "✅ Successfully uploaded 18_12_2004.csv (ID: 13_QYXiNxu2WgSsxznOCEK76lKtOOygRD)\n",
            "✅ Successfully uploaded 2_12_2004.csv (ID: 122MO-S1tZTtzX0bIlL1HnfqpFIgQtMvZ)\n",
            "✅ Successfully uploaded 6_12_2004.csv (ID: 19cL4Y7YLUaVn3Xz6x5Y2Ltxa7qIUS8EE)\n",
            "✅ Successfully uploaded 31_12_2004.csv (ID: 1NYHexQXxzNRuQzfuLA5dK9pl-6VOu5pD)\n",
            "✅ Successfully uploaded 3_12_2004.csv (ID: 1dJBdMq04Qpbp-IsoKLN7yqQy34iVtfQ3)\n",
            "✅ Successfully uploaded 4_12_2004.csv (ID: 1J80WVc6sNzi7kJFSZQpBwCbHy9kPGDVq)\n",
            "✅ Successfully uploaded 29_12_2004.csv (ID: 1BkcUt94MpfbCnAQs-M6eMbDfn74Ph_-W)\n",
            "✅ Successfully uploaded 16_12_2004.csv (ID: 16BHnTYsrC9FC3SfFh1BX9j9pAA3jGdhe)\n",
            "✅ Successfully uploaded 14_12_2004.csv (ID: 1DoD2krGlnAwt9n8gD-jDskg2dvPOH59K)\n",
            "✅ Successfully uploaded 7_12_2004.csv (ID: 17WgkLXejRHSMA5Ns8Ur4ZsNrXgZqom_K)\n",
            "✅ Successfully uploaded 19_12_2004.csv (ID: 18xDSob-8h4MRWM_PoJMxrLQUW_DfqNPC)\n",
            "✅ Successfully uploaded 27_12_2004.csv (ID: 1prEP89hKBmpUiLqqLE6CHtDmYmmTsd2A)\n",
            "✅ Successfully uploaded 10_12_2004.csv (ID: 14UsxbsyQPvZ9i-XAUk_d8f_HR47aDe2e)\n",
            "✅ Successfully uploaded 22_12_2004.csv (ID: 1TtHX-mPJohCn9_MWXEtztdWFBfNF2Ir9)\n",
            "✅ Successfully uploaded 8_12_2004.csv (ID: 1kVXjw2u9tWRTU0MQ3vXZJ28FtlSjaV2r)\n",
            "✅ Successfully uploaded 28_12_2004.csv (ID: 1aygYl8-BWdhbDA2H46TOxN61GNx9ActS)\n",
            "✅ Successfully uploaded 24_12_2004.csv (ID: 1mYJuLrA1ivb_qUPp7wBIpc-S6WqUccbQ)\n",
            "✅ Successfully uploaded 15_12_2004.csv (ID: 11Lonf3lF6OQiSApFCxV_aaXvVDJJS5yD)\n",
            "✅ Successfully uploaded 20_12_2004.csv (ID: 1oLx3QWKBIz8vy19v0K0zqbWPBlfMVCCi)\n",
            "✅ Successfully uploaded 13_12_2004.csv (ID: 1S4M1ITPnBoG_kdP5XlZDsNTfanJw1nOO)\n",
            "✅ Successfully uploaded 26_12_2004.csv (ID: 15OBWveSD3kUChwFcQP5IFquDWFoc67u8)\n",
            "✅ Successfully uploaded 23_12_2004.csv (ID: 1lYIP8rJp7c2x1dA0rbsFbLc47es5hQyo)\n",
            "✅ Successfully uploaded 14_01_2004.csv (ID: 1OnuoYebK0NUbhg2URkSVlSF0RREWlpul)\n",
            "✅ Successfully uploaded 10_01_2004.csv (ID: 1bOPi7KiE5IPKPsYfi1-UhU8aQ6hpYmtz)\n",
            "✅ Successfully uploaded 13_01_2004.csv (ID: 1QmHr4ijyHwFG_vxC5T66yt6ENXkaQd9J)\n",
            "✅ Successfully uploaded 12_01_2004.csv (ID: 1CmJ1i41YhuGxMi-1eEjoqs3oQ06AMACw)\n",
            "✅ Successfully uploaded 7_01_2004.csv (ID: 1JIZ2y-Y2i-UjdMAOxEJpiJCDE486NdUg)\n",
            "✅ Successfully uploaded 16_01_2004.csv (ID: 13PPU6g5KdGGIqjbL9c8KSn4bzEKiJLK6)\n",
            "✅ Successfully uploaded 2_01_2004.csv (ID: 155v_k5BGlxBgLP9rkfmXUmoy8kJ1vstA)\n",
            "✅ Successfully uploaded 15_01_2004.csv (ID: 1eWUkTULpoYXPpj-XZ_M4er05lUy4NEW3)\n",
            "✅ Successfully uploaded 11_01_2004.csv (ID: 1dWV7mFVLk17q4EJKCgtkxkG-X8H4maYk)\n",
            "✅ Successfully uploaded 8_01_2004.csv (ID: 1r99tqV7w3hTYZ8CWO9_zAZTfGXrHyqRu)\n",
            "✅ Successfully uploaded 5_01_2004.csv (ID: 1an47NGl_T02rrN6oaTd-X9Kxdk4E8Pms)\n",
            "✅ Successfully uploaded 4_01_2004.csv (ID: 1vMDVzHUMvoVQ_ie-e3BouWkNuUIdsTKo)\n",
            "✅ Successfully uploaded 3_01_2004.csv (ID: 1NTrVvMOHmi5g0aT4lWHrU7C1OtpRkmRo)\n",
            "✅ Successfully uploaded 6_01_2004.csv (ID: 1RYR5UGeo4rdMPShCM3GJ-9AjP8ubchP1)\n",
            "✅ Successfully uploaded 9_01_2004.csv (ID: 1fAsAzeZmJEyJrHJgafJBmQJsNa-eni-V)\n",
            "✅ Successfully uploaded 1_01_2004.csv (ID: 17VpgxmzCTg1--MWx_abJ6KLg5eoTSLaL)\n",
            "✅ Successfully uploaded 17_10_2004.csv (ID: 1jr1oCVRBjsIN7J4EoB0xOThgq4ytJnfV)\n",
            "✅ Successfully uploaded 1_10_2004.csv (ID: 1w_urj5FmYc4Lv9osv-uq6wjoG33Rbtwr)\n",
            "✅ Successfully uploaded 20_10_2004.csv (ID: 1EFkSE2hHkMY6JJrxKahxZqkL_msqrf8W)\n",
            "✅ Successfully uploaded 7_10_2004.csv (ID: 1PDKZAD4v1yAFGFEbMHWJcvPSaaWasDa-)\n",
            "✅ Successfully uploaded 12_10_2004.csv (ID: 1pG1ozuYTmrK6j4NVlhbSW9LyCLT9YiuM)\n",
            "✅ Successfully uploaded 3_10_2004.csv (ID: 1UCNcB0UYZseDS4Ar3Q959BV6wNkEB74e)\n",
            "✅ Successfully uploaded 10_10_2004.csv (ID: 1TqiLPzEcCZ02X6ZJa25Bzw3N745JXnc2)\n",
            "✅ Successfully uploaded 18_10_2004.csv (ID: 1du--bznirzZrqLSkm0Fjree57lPy5jAN)\n",
            "✅ Successfully uploaded 2_10_2004.csv (ID: 1eEvtNBWFsKtMR1Pg-vFfWMS1J2sZnMf5)\n",
            "✅ Successfully uploaded 16_10_2004.csv (ID: 1j9Otm6StOaQIwo2smuYgYCKhTfjWxQ4G)\n",
            "✅ Successfully uploaded 11_10_2004.csv (ID: 1jacuutF13rQ85Rooi7J6AXOQfHAWqwkH)\n",
            "✅ Successfully uploaded 13_10_2004.csv (ID: 1FTaD047G3by1gYVEJYIlS59SZCsy3zNn)\n",
            "✅ Successfully uploaded 21_10_2004.csv (ID: 1EVU7D5ymms6FMt4Id8tiajgZh6ZIGsqi)\n",
            "✅ Successfully uploaded 8_10_2004.csv (ID: 1eaa8GhtmFNVzFp0gVSVMRh6ewMCfuk7F)\n",
            "✅ Successfully uploaded 4_10_2004.csv (ID: 1UZKcqPGMQ69qLuit3wMIb4LwTJjJ1XSB)\n",
            "✅ Successfully uploaded 6_10_2004.csv (ID: 1vjvUw7zgak0-ed6aVbyr1bB7WkyrraBf)\n",
            "✅ Successfully uploaded 14_10_2004.csv (ID: 1JsoWXUKNU-kcHj_JavDHM2shISfnKwA8)\n",
            "✅ Successfully uploaded 19_10_2004.csv (ID: 1OO0cZZYOSLv0LU7E5qWlaW77utQVLCiA)\n",
            "✅ Successfully uploaded 15_10_2004.csv (ID: 15q_BGDXKRvDF0BZWmmMDre9CGqaa8qw5)\n",
            "✅ Successfully uploaded 5_10_2004.csv (ID: 1YZtqZwQuBmjowPiebkXNEyPQzoDRKJTT)\n",
            "✅ Successfully uploaded 9_10_2004.csv (ID: 1Xw_3GWc9pPKkzKRRhxWBVrMDhgg9-Q7O)\n",
            "✅ Successfully uploaded 10_08_2004.csv (ID: 1DH2_VxzBfgjs24W2ivJ29E710YCtTh_2)\n",
            "✅ Successfully uploaded 29_08_2004.csv (ID: 1XEKgjc3t4SJiWDvwSiJIzldYtwOOvcvK)\n",
            "✅ Successfully uploaded 15_08_2004.csv (ID: 1mF_4mT9-wq-X-3GZbE2xiG3kd43XxtKF)\n",
            "✅ Successfully uploaded 30_08_2004.csv (ID: 1pOwRaNcnlbXVR0o9mvUi5omZpVF0g2_M)\n",
            "✅ Successfully uploaded 20_08_2004.csv (ID: 1vLTu11AVH1IB0o4bUQsIv_ffpW-_nT6H)\n",
            "✅ Successfully uploaded 7_08_2004.csv (ID: 1zy-Omjdwi9AeMitfSdZpecVR0co4oNOa)\n",
            "✅ Successfully uploaded 26_08_2004.csv (ID: 1YA_oz_d-L4coOYzbaTqodZDlSKX-F1Le)\n",
            "✅ Successfully uploaded 13_08_2004.csv (ID: 1yDc2kOAmuItsJdlFTEQZ6cFW2DhvOiGK)\n",
            "✅ Successfully uploaded 38_08_2004.csv (ID: 1rx4lgp0wjdS4HF7ickjcDvTlN6_tT39s)\n",
            "✅ Successfully uploaded 23_08_2004.csv (ID: 1GUnrj7ZMGBsCqi2cqPFmLGL4cThjMeyO)\n",
            "✅ Successfully uploaded 24_08_2004.csv (ID: 1pL16oSroGlcYmUoI7a8Qfo7CswDPPtdU)\n",
            "✅ Successfully uploaded 19_08_2004.csv (ID: 10Rb7_9ud_zYFuUoyy0yrliRUKUYjcL5O)\n",
            "✅ Successfully uploaded 5_08_2004.csv (ID: 1i5fTSGuUWTaGxuO7aqR-rjqk2fHB-gFl)\n",
            "✅ Successfully uploaded 35_08_2004.csv (ID: 12LPfu2kDNr47Mka0-iqT4hZlcQehH4Nx)\n",
            "✅ Successfully uploaded 4_08_2004.csv (ID: 18GfxTUm7cDqQfpKjXNN8-kuITSArunK0)\n",
            "✅ Successfully uploaded 16_08_2004.csv (ID: 106JsQW6DjSo6ItT-FYHA3AI9OrtaUzkl)\n",
            "✅ Successfully uploaded 18_08_2004.csv (ID: 1QSFQHletuW96cvQeTqst6m_p6AlXo1tA)\n",
            "✅ Successfully uploaded 32_08_2004.csv (ID: 1A9r0H4tqMqo6KqSAemfimvJvF_otzzKX)\n",
            "✅ Successfully uploaded 17_08_2004.csv (ID: 1ifqW6VviP67tzZiHif_ZGzlm8-MxBo-e)\n",
            "✅ Successfully uploaded 8_08_2004.csv (ID: 1FHcu8w-P1PsQvKQD9SDVNsfP7A75Bx45)\n",
            "✅ Successfully uploaded 33_08_2004.csv (ID: 1FcFhRdkD44-7R2DYywuV1FL5473fFvQd)\n",
            "✅ Successfully uploaded 25_08_2004.csv (ID: 1Wn6cM8RdOdhqg75UC-OBaGZDH3hjO3Ur)\n",
            "✅ Successfully uploaded 6_08_2004.csv (ID: 16ogQjtAuVtA8gN3ZKwd2Z5lLmCzzA0mU)\n",
            "✅ Successfully uploaded 27_08_2004.csv (ID: 15WP20-lCjp98WLKcSltIcjoDouSIQQvv)\n",
            "✅ Successfully uploaded 14_08_2004.csv (ID: 11LC-WJxibm7h6KBFJWm9BskdEFTR4l59)\n",
            "✅ Successfully uploaded 21_08_2004.csv (ID: 17cnhHxXBayRtgVZWXJLNTIr9_0uCm-Su)\n",
            "✅ Successfully uploaded 22_08_2004.csv (ID: 1p3rPnRyHqRsREGhWO62Os4NBF0wjypQ4)\n",
            "✅ Successfully uploaded 34_08_2004.csv (ID: 13mHIs5RsxqLA1KEoIo8wXVYp2A4BReDT)\n",
            "✅ Successfully uploaded 12_08_2004.csv (ID: 12PM2ncr4SnPs-9bFGRvx2tTtoWntXJbl)\n",
            "✅ Successfully uploaded 37_08_2004.csv (ID: 1WzZFCVY783GXsziGvPcGHxB_ZXEEoTkY)\n",
            "✅ Successfully uploaded 31_08_2004.csv (ID: 1egHrXt1ShJ6LLwJEo4Zt5u5Zq3KfFMaR)\n",
            "✅ Successfully uploaded 28_08_2004.csv (ID: 1rlx9mrJmO2I2qBmkYMdvt6YVPs8_7ODh)\n",
            "✅ Successfully uploaded 40_08_2004.csv (ID: 1L-bof9S4O2PMtV5cz1SOZ8FCVQeEK_xy)\n",
            "✅ Successfully uploaded 9_08_2004.csv (ID: 19sEsAyM4rRSifCqis_fdfzJfnr4lfbUE)\n",
            "✅ Successfully uploaded 11_08_2004.csv (ID: 1X8kwNGuqZwMnCX3ltW1ey_eeDYIpJIem)\n",
            "✅ Successfully uploaded 36_08_2004.csv (ID: 1cJJGfZ8rrl8nx2CfdbXj-9fpxYTeErRM)\n",
            "✅ Successfully uploaded 1_08_2004.csv (ID: 1-xpu01fXZ8MmndMGwmjS5RBkQIC-RxVu)\n",
            "✅ Successfully uploaded 39_08_2004.csv (ID: 15eYibQytkd_gqBfVtSHvbODlEQgWRM34)\n",
            "✅ Successfully uploaded 2_08_2004.csv (ID: 1NbkeeDq-N0upRfr7tm7bmYPP-tIxgJXm)\n",
            "✅ Successfully uploaded 3_08_2004.csv (ID: 1BsF_3SmJ8SzuddrANnohMLymPlN966gy)\n",
            "✅ Successfully uploaded 5_15_2004.csv (ID: 1G-ymVt25riXAm3b_AtavKbCpnzgDUBPj)\n",
            "✅ Successfully uploaded 2_15_2004.csv (ID: 14ZiQs5LUBRVG5XQl3d3RxBXcbgOPjotn)\n",
            "✅ Successfully uploaded 4_15_2004.csv (ID: 15_BnSrVSyeSXnguzrftHPdSoQd5xaJwK)\n",
            "✅ Successfully uploaded 1_15_2004.csv (ID: 1O2Vx-yQ-J8YEn1n7uvhEGX07-BLNYL3b)\n",
            "✅ Successfully uploaded 3_15_2004.csv (ID: 1aktugs9HTDdnYhBXEe0BjbZhqqx9FZYV)\n",
            "✅ Successfully uploaded 9_13_2004.csv (ID: 1u7rctbcdln4igvxhc45ouoz6aRAaaD6N)\n",
            "✅ Successfully uploaded 6_13_2004.csv (ID: 1ZIsqgEAcNAu9VDMA1o59SOlkEOe6Fwvn)\n",
            "✅ Successfully uploaded 19_13_2004.csv (ID: 1_VV_UVxhTT6hxGfwZA9A6gMLK6N4SYrL)\n",
            "✅ Successfully uploaded 18_13_2004.csv (ID: 1kf_ic0TPlTEHWbTAOmuOE3qH5HaSHjYF)\n",
            "✅ Successfully uploaded 13_13_2004.csv (ID: 1biVBokLySv_-I2ilAx56XujztDyMSeyK)\n",
            "✅ Successfully uploaded 10_13_2004.csv (ID: 1zVT5F4b2LDvyB0LQD8VijZnmkiixRqdL)\n",
            "✅ Successfully uploaded 2_13_2004.csv (ID: 1lwLvfrSWyWp-HH-mDWgK234NEMrKcNOl)\n",
            "✅ Successfully uploaded 12_13_2004.csv (ID: 19vtroqGW1Dch54p335FWAhmZbemzCK17)\n",
            "✅ Successfully uploaded 7_13_2004.csv (ID: 1GbGMHCZ79XRN5XGUbfKTD8KTYWycJwfv)\n",
            "✅ Successfully uploaded 15_13_2004.csv (ID: 1hJFNkOn0chP-So_iOyYU5TTuggvXA1C7)\n",
            "✅ Successfully uploaded 16_13_2004.csv (ID: 11590w4hipGT4Xd8Lq2JgCousev2RYNSf)\n",
            "✅ Successfully uploaded 14_13_2004.csv (ID: 167qcqvnQN-o__zDSfsOveV-sTwrp1HdZ)\n",
            "✅ Successfully uploaded 17_13_2004.csv (ID: 1In8ouDz7EFVLNYB2zGgMHqMxX3qNZesW)\n",
            "✅ Successfully uploaded 5_13_2004.csv (ID: 1osoyHogkxsK4YvcIAoHpiqI2rhSw-iLw)\n",
            "✅ Successfully uploaded 1_13_2004.csv (ID: 1bg0zU8TglZm7YuzNFhXzS_f4GqTC27DR)\n",
            "✅ Successfully uploaded 20_13_2004.csv (ID: 1rkkv260KLNX7aG2skQigeSqZiHq3xKMS)\n",
            "✅ Successfully uploaded 3_13_2004.csv (ID: 1GyeRJArhCR08yJzxM8yWfIpPPc23G-3F)\n",
            "✅ Successfully uploaded 4_13_2004.csv (ID: 1AlHIjPiJ06EDnaGUC57AJpv50c-QWRme)\n",
            "✅ Successfully uploaded 11_13_2004.csv (ID: 1AUjmIbPeewCkiDkkSpbPPIAKLXciLL4K)\n",
            "✅ Successfully uploaded 8_13_2004.csv (ID: 1l0K0TgFh9vinr62lZ5xCqNT6jvAnE-Dk)\n",
            "✅ Successfully uploaded 6_07_2004.csv (ID: 1mkFFtItDIUIZ7eB17Ajz-Mxa8YG0tUnX)\n",
            "✅ Successfully uploaded 21_07_2004.csv (ID: 1DxfEq0Qj7QRz-DS9pwu995-B02eKtr8n)\n",
            "✅ Successfully uploaded 18_07_2004.csv (ID: 1QDmFdD5GZNCvBB3gz-poB9cQ4yhqP9tv)\n",
            "✅ Successfully uploaded 17_07_2004.csv (ID: 14JZkNkx7rKrCIPXBGWjclzV9vXI2RYJA)\n",
            "✅ Successfully uploaded 2_07_2004.csv (ID: 1LY073eD3AUez37p-g6hxJ7ddcSGlY2CW)\n",
            "✅ Successfully uploaded 19_07_2004.csv (ID: 1ngTYMZ2LDfHyqGvT9xwUGi6zelB2u-50)\n",
            "✅ Successfully uploaded 22_07_2004.csv (ID: 1kN3iRgC-ApmjEqshbhykMxiIciyqafHx)\n",
            "✅ Successfully uploaded 8_07_2004.csv (ID: 1tsA4xALoKsVNzTnZqy9CJVxrljzK6DuM)\n",
            "✅ Successfully uploaded 24_07_2004.csv (ID: 1jTc0V69BVpId8e4jfBPNVjmrylZfV6gp)\n",
            "✅ Successfully uploaded 20_07_2004.csv (ID: 1MY-VGcnUmeuKeJyPF62eRcVQboANCzwo)\n",
            "✅ Successfully uploaded 5_07_2004.csv (ID: 1bRGf3LpfSMVPRPo87tqLjHA4jdWCLwjw)\n",
            "✅ Successfully uploaded 13_07_2004.csv (ID: 1aDgka_uv5k5yEyJ5d6LN39SLbUJzL9eO)\n",
            "✅ Successfully uploaded 23_07_2004.csv (ID: 11cVyvQdmpAD0WViz7AV0OtXA-G26q0RM)\n",
            "✅ Successfully uploaded 15_07_2004.csv (ID: 1-IG9SZycIXzxP8_tI1RtV1WUfl4QAi9z)\n",
            "✅ Successfully uploaded 11_07_2004.csv (ID: 1CuANPPdbXJQO1Z_jXiC-dTijutyjV1ff)\n",
            "✅ Successfully uploaded 16_07_2004.csv (ID: 1whad-sWQ7tsIZd10Y2DE8N_QIvKhlRCO)\n",
            "✅ Successfully uploaded 10_07_2004.csv (ID: 1kPoUksORccb4MgSqFGpUjYnkt7f0bK-L)\n",
            "✅ Successfully uploaded 4_07_2004.csv (ID: 1KpiTzjCv8WKOi54AxnAIopb9VaNYYNz8)\n",
            "✅ Successfully uploaded 12_07_2004.csv (ID: 1XkLAr2YnWLIqY8YDXoBxMo9FG-doZIpI)\n",
            "✅ Successfully uploaded 14_07_2004.csv (ID: 1quQlQV_6q1wTn5gqIEuBjEp9EQDq4syB)\n",
            "✅ Successfully uploaded 1_07_2004.csv (ID: 1PVatCOZXRCn8GeWAPAgat8ddGEjZmHM7)\n",
            "✅ Successfully uploaded 3_07_2004.csv (ID: 1RFgmfOmy12u0nmLZd8XT95BVkkEjUOCq)\n",
            "✅ Successfully uploaded 7_07_2004.csv (ID: 13N1V3ZAlZ5fA5ZRGigfH0pDz3DDVzS83)\n",
            "✅ Successfully uploaded 9_07_2004.csv (ID: 1PgHx9qX92Ru6hfUOuuCkzy5eD1u6_rDw)\n",
            "✅ Successfully uploaded 3_03_2004.csv (ID: 1toIklvcaVeoltoC0EzHIjJv-GFANAric)\n",
            "✅ Successfully uploaded 6_03_2004.csv (ID: 1X1rDHkMc4jX7j-QtE2ndtmfzq_1z9cCC)\n",
            "✅ Successfully uploaded 2_03_2004.csv (ID: 1MlvZbfwayYJqe8BtsN2ml48423jz8Po-)\n",
            "✅ Successfully uploaded 1_03_2004.csv (ID: 1iLsJPrj_hWslSiiwFGStscGdfkVxf8du)\n",
            "✅ Successfully uploaded 12_03_2004.csv (ID: 1jR3Sm79mD4GmyV-1jSDviejePaSUgWsm)\n",
            "✅ Successfully uploaded 8_03_2004.csv (ID: 1Ushycb-Eopy_jixtXaR9Pxyl22Xf5DRX)\n",
            "✅ Successfully uploaded 13_03_2004.csv (ID: 1uEI_psMJnRjTq763TUNNGwqqLcZCXbkm)\n",
            "✅ Successfully uploaded 4_03_2004.csv (ID: 1RpclO0lG_c9VawHPQAbdGk34782-8vIp)\n",
            "✅ Successfully uploaded 17_03_2004.csv (ID: 16D9jLAcpTh5jLy0RRjrl__bTfxL3WLdw)\n",
            "✅ Successfully uploaded 18_03_2004.csv (ID: 1ZUCHcBU4lGMAav5AWj0UTjuKUVwEaG3X)\n",
            "✅ Successfully uploaded 11_03_2004.csv (ID: 14wCLwjunCH-BR1kfFhA2526bHebUi_gS)\n",
            "✅ Successfully uploaded 16_03_2004.csv (ID: 15gbJ84IiiDaGzOMfiA2OSSGWGljYH1bf)\n",
            "✅ Successfully uploaded 21_03_2004.csv (ID: 1cXNPS9OZjUy7GUIAVmqW0XJfkq5qmeZi)\n",
            "✅ Successfully uploaded 14_03_2004.csv (ID: 15w58UYDqUcTUMhKupeFqm0K3P1svdt-2)\n",
            "✅ Successfully uploaded 9_03_2004.csv (ID: 13PvfONurCnC1kUNibSMMwPPdeW5IOvrZ)\n",
            "✅ Successfully uploaded 15_03_2004.csv (ID: 1Egq2YHJwRddfyrrhZB_0IvheO7wf14wD)\n",
            "✅ Successfully uploaded 5_03_2004.csv (ID: 1T70LhQkANX5_z8XEPzp24faRwkEG_JCT)\n",
            "✅ Successfully uploaded 7_03_2004.csv (ID: 1ygO3f7AqM99Pt_LSqs4Bii0jAkxmFARQ)\n",
            "✅ Successfully uploaded 20_03_2004.csv (ID: 1fnO2PZUM1zj9j0jpjvbxvjekjsshA0KA)\n",
            "✅ Successfully uploaded 19_03_2004.csv (ID: 19rX8NVoFRZ-rCFXM4rNkean4WfCOYZRX)\n",
            "✅ Successfully uploaded 10_03_2004.csv (ID: 1YIv8JzFtFYdcYPGMT4R184bbE_LI8Ig-)\n",
            "✅ Successfully uploaded 5_14_2004.csv (ID: 1a4SAl9QpmtKcfO2_WJFfS1TlVhBCje54)\n",
            "✅ Successfully uploaded 18_14_2004.csv (ID: 1UrnbSxF1MHNwNqQ4G38qH1XfBshWg-NJ)\n",
            "✅ Successfully uploaded 36_14_2004.csv (ID: 193F4pcmR7rMma4MtW0L7NCIDxB3L5yfk)\n",
            "✅ Successfully uploaded 31_14_2004.csv (ID: 1LdFLqm-MGuMTg3jLr3Xl98uwMuxOmA2l)\n",
            "✅ Successfully uploaded 16_14_2004.csv (ID: 1ru4ABRD6YKB1Ir_EgdfAkdvVAfXfxu-A)\n",
            "✅ Successfully uploaded 10_14_2004.csv (ID: 1_nPN7go81P4mkTKDNcPnuSZDjrbFWWzU)\n",
            "✅ Successfully uploaded 13_14_2004.csv (ID: 13EGUya4vvVHU4S2X16oQ8OuIM8jdBKS_)\n",
            "✅ Successfully uploaded 35_14_2004.csv (ID: 1VEGktijxIRh5vQ0yBwssRJT7XVJib4sl)\n",
            "✅ Successfully uploaded 37_14_2004.csv (ID: 1-j71C4O8LeDn1Bg5MlMaYpXuwn7j6jKQ)\n",
            "✅ Successfully uploaded 4_14_2004.csv (ID: 1d6kJBkN0BHeqAn_lu51brTbvRfij8Yux)\n",
            "✅ Successfully uploaded 27_14_2004.csv (ID: 1mighU02HiL_Ol1jRTesSZof8CZWJNZhD)\n",
            "✅ Successfully uploaded 19_14_2004.csv (ID: 1tjNf4r3uFnQfElEjXIwf1JXn18FkARCv)\n",
            "✅ Successfully uploaded 38_14_2004.csv (ID: 1zvTepeLxYUwkTu2mZqtJGwpNvRigyel2)\n",
            "✅ Successfully uploaded 6_14_2004.csv (ID: 1RrsLdLyZImU369txn2q-XFGKt1mGrcpM)\n",
            "✅ Successfully uploaded 22_14_2004.csv (ID: 1f_9ZI2kZ4S2hwmxHLMhU7_vDD1l9RbhC)\n",
            "✅ Successfully uploaded 15_14_2004.csv (ID: 1blvxe3w5qxtQmIopaOVF4HmYRnk00d3p)\n",
            "✅ Successfully uploaded 12_14_2004.csv (ID: 1GWYDryn0bN8v3r66BjNkXBWb04p8baFl)\n",
            "✅ Successfully uploaded 23_14_2004.csv (ID: 1wf0e0fsDBItkWemf5gCDpgJlE47sEjvn)\n",
            "✅ Successfully uploaded 3_14_2004.csv (ID: 10Ca-jYAf7JVGBINucmrlcsb0sE3hYYke)\n",
            "✅ Successfully uploaded 2_14_2004.csv (ID: 1nPYhO0uXPJGa7zkByv6OGFqyyFj_1B7c)\n",
            "✅ Successfully uploaded 21_14_2004.csv (ID: 1IXvTjKwe01wCEoI7L4s-ZaQZWCaxfHSC)\n",
            "✅ Successfully uploaded 20_14_2004.csv (ID: 15hS5HxjoqEVAXeB-Db4ikg7WbFP7HlL2)\n",
            "✅ Successfully uploaded 32_14_2004.csv (ID: 1BHXrEoG6ynng3hD3R8qjJbg4dekLmRVm)\n",
            "✅ Successfully uploaded 24_14_2004.csv (ID: 19CVqqk64hBquEL5314U2kVZyShVe16K0)\n",
            "✅ Successfully uploaded 30_14_2004.csv (ID: 1GIXv_x_500b2PK-CfZIrIIOi4tE6L7_0)\n",
            "✅ Successfully uploaded 14_14_2004.csv (ID: 1eArEzoboC6l25qU7TjoyxljnYt8Lnd2W)\n",
            "✅ Successfully uploaded 11_14_2004.csv (ID: 1P6Ayqeei8Wxx7tffI6sjAyseyjht8cHp)\n",
            "✅ Successfully uploaded 25_14_2004.csv (ID: 11YVTuLs5kO2IxNSqhrEzU4NJKVFjIlMD)\n",
            "✅ Successfully uploaded 29_14_2004.csv (ID: 1tVG37zjH0qy27VIUoWa9hekDXGZ-MpGr)\n",
            "✅ Successfully uploaded 1_14_2004.csv (ID: 1M5uRun5cb8VEWtDMEawU1psV5h8wyH3d)\n",
            "✅ Successfully uploaded 8_14_2004.csv (ID: 1TpPtGPzKBQ5wQAz3L2Hg0hPjj1-XNphb)\n",
            "✅ Successfully uploaded 34_14_2004.csv (ID: 1XGtJNEdNe2dqiPH_dC-MV7p9KD9ydonR)\n",
            "✅ Successfully uploaded 7_14_2004.csv (ID: 1VpL312XzmxItjv3N1BgYAFYoPY4TtFcy)\n",
            "✅ Successfully uploaded 9_14_2004.csv (ID: 1AqSgIEVb7T91o4v1XfgfLFRpvkvzJzLI)\n",
            "✅ Successfully uploaded 17_14_2004.csv (ID: 1ijhf5VmR73M9jLbHsnFxKOB4Sy4lE_d4)\n",
            "✅ Successfully uploaded 26_14_2004.csv (ID: 1ecRiqtVrHAcxcCaPWgEqs8AklvMJO4jq)\n",
            "✅ Successfully uploaded 33_14_2004.csv (ID: 1GRfaSZTpqDe7sgD3WkZGLKUwl7GxaXKu)\n",
            "✅ Successfully uploaded 28_14_2004.csv (ID: 1g1RHv8DJEXpSvqBAeOvyvH_TAk8t9GPO)\n",
            "✅ Successfully uploaded 2_06_2004.csv (ID: 1jKmQN49gYJQmyGkXItD8hVtuZwBVA6O7)\n",
            "✅ Successfully uploaded 6_06_2004.csv (ID: 1E1N1ddzAQNzju8qkUDZCv9jqJ9pqzGAX)\n",
            "✅ Successfully uploaded 7_06_2004.csv (ID: 1zZJxj9QLnmtOO23WnTKzGFajGAJWQKFf)\n",
            "✅ Successfully uploaded 3_06_2004.csv (ID: 1zOA8EK-qLZlfGRd5HUVj0Fqt9pznSDWp)\n",
            "✅ Successfully uploaded 5_06_2004.csv (ID: 1HZDjodVkLG82IHelcewYnb8JsCyvmcbc)\n",
            "✅ Successfully uploaded 1_06_2004.csv (ID: 1HYen43O-XSPWQpeV6h5oBxzj1pV1AfpC)\n",
            "✅ Successfully uploaded 4_06_2004.csv (ID: 1T4j9P_w54Vb03KOHqfwOKstcpLohJIw9)\n",
            "✅ Successfully uploaded 2_02_2004.csv (ID: 17LzM0YwDbySOFQxANkiEvZ6HOjOBI-kh)\n",
            "✅ Successfully uploaded 5_02_2004.csv (ID: 1GgKg3nS92ipO6SW1xHuxqPYGk83rbTHs)\n",
            "✅ Successfully uploaded 6_02_2004.csv (ID: 1YN2uBb4FrexdCpqwhZjSHP0MVOVYzziR)\n",
            "✅ Successfully uploaded 4_02_2004.csv (ID: 1WUeqp68Txoq5_nKX7kqmbRxV4bVnri83)\n",
            "✅ Successfully uploaded 11_02_2004.csv (ID: 1DpNJcDZnc2OpqsSrZMzqo50WbBUDRlEN)\n",
            "✅ Successfully uploaded 1_02_2004.csv (ID: 1Aem-ZHOfAPag8-2ZaSFYb9eaICeopEWX)\n",
            "✅ Successfully uploaded 10_02_2004.csv (ID: 14a-tIXZfAATRmNcnn-ggabYBar0581Lh)\n",
            "✅ Successfully uploaded 13_02_2004.csv (ID: 1gOo6IUR0y_1eJfsPtB1LstgQWO1j_qvD)\n",
            "✅ Successfully uploaded 9_02_2004.csv (ID: 1sceJQphAAj3ctj_xU-VBcZolOjW1lN34)\n",
            "✅ Successfully uploaded 7_02_2004.csv (ID: 1bWgL9Lanj8ZEtAX__HhuUUXS9D6Bwpqv)\n",
            "✅ Successfully uploaded 12_02_2004.csv (ID: 1WEhkf7vvX_W7AFQYzuffIjJ7qWCsjZCE)\n",
            "✅ Successfully uploaded 8_02_2004.csv (ID: 1UaBPvGbX0n47vUJ9ajgoEL6cVGth5clf)\n",
            "✅ Successfully uploaded 3_02_2004.csv (ID: 1Ld3z45BHuwDfvEu5-KCOqjzopsuvkNQs)\n",
            "📁 Created folder: 11 (ID: 1idxqsyaaRo3zEU8cmCHSFs4tBrOjibv1)\n",
            "📁 Created folder: 04 (ID: 1DDn0cnCVF06T0ATlu_zHYnLuq-Mf7_Ul)\n",
            "📁 Created folder: 05 (ID: 1-OWOT-gv93zS_9UxekdIVeHZt8GHWu8e)\n",
            "📁 Created folder: 12 (ID: 1LbqwSMtIlODWZl9wvzqf3w1TfNtmS99t)\n",
            "📁 Created folder: 01 (ID: 1HNiaeXx250jNgyuSFK39ghWPDA4uaDGe)\n",
            "📁 Created folder: 10 (ID: 15XcGMHvzzhfJiSSUIKq0UqQ3-ca5hCUG)\n",
            "📁 Created folder: 08 (ID: 1zTRjhEJ73_RwZkEZ6VoMr2IrZ_mDDuAg)\n",
            "📁 Created folder: 09 (ID: 1IT5UWL9M4xdmbhAmczCr5Y3l_UgO4uN4)\n",
            "📁 Created folder: 15 (ID: 1EiXUlh6muEHY5nl7vzq1Hub4MmGygLbV)\n",
            "📁 Created folder: 13 (ID: 1GAjGKUVvoxaF_68U31VSz5yavWEGF97p)\n",
            "📁 Created folder: 07 (ID: 1V6p26MgwpzXE4JQ4PLCSNHGuaZE-sbfb)\n",
            "📁 Created folder: 03 (ID: 1mgTjDqnZEHkEuIIwNxVDsEj2CA71UGfo)\n",
            "📁 Created folder: 14 (ID: 1phSBFucgRvw07txQOpWBH4c_uMgTrp1K)\n",
            "📁 Created folder: 06 (ID: 1KhfV01MDgCNfqsW4BABFGGqUs4OLLazA)\n",
            "📁 Created folder: 02 (ID: 1XZTNSh5iCMGvyvsabpkmu-Re_2IsVIb_)\n",
            "✅ Successfully uploaded 4_11_2008.csv (ID: 1hf-yzNdjVEI-i1bo9NBZ4c18csFkrb4e)\n",
            "✅ Successfully uploaded 3_11_2008.csv (ID: 1iBWKKTAEHksFlwuFlYNSCcXgrQMHAw-0)\n",
            "✅ Successfully uploaded 1_11_2008.csv (ID: 1jR3PiL-Q_Qs6SUvuHi9eyD8M3S-GiYvP)\n",
            "✅ Successfully uploaded 2_11_2008.csv (ID: 1Khcy2qV-yEf9etKotvzbAZqIcdugtRMf)\n",
            "✅ Successfully uploaded 6_11_2008.csv (ID: 13DxLcMZb8W7NQ4ES7wAOBqcMJENraL4h)\n",
            "✅ Successfully uploaded 5_11_2008.csv (ID: 1IyholJhy5VduAuOZ2MElRRl7mDZrruw8)\n",
            "✅ Successfully uploaded 8_11_2008.csv (ID: 15QOCyHCYKkSE8p8XRhGAB_b2S1hMwPDf)\n",
            "✅ Successfully uploaded 7_11_2008.csv (ID: 12GILhrMw8dA0HL1vaRg840cLVpCv2zcH)\n",
            "✅ Successfully uploaded 10_04_2008.csv (ID: 1UWb07zqq-geesfxix417elqRzwafvjsc)\n",
            "✅ Successfully uploaded 5_04_2008.csv (ID: 1MJl2vNkSoWxQYTqC5GfRnxZ2PRcQo7i4)\n",
            "✅ Successfully uploaded 4_04_2008.csv (ID: 135zRAE_e0rTh3gnOfO9kfiH7Kqp1dZF0)\n",
            "✅ Successfully uploaded 1_04_2008.csv (ID: 19qWcGjJMVgFF8eJBXXUXa_RIOMHC_htb)\n",
            "✅ Successfully uploaded 12_04_2008.csv (ID: 1FxzmzTUlwvU5jHdXyNHinJCDewPMTs80)\n",
            "✅ Successfully uploaded 7_04_2008.csv (ID: 1DNqie5tH9MR2XDFrBkIvDX7D-NyV1QYC)\n",
            "✅ Successfully uploaded 2_04_2008.csv (ID: 1ehe_wSPFFR7zgmHqlwQQ19wSC5sT990r)\n",
            "✅ Successfully uploaded 11_04_2008.csv (ID: 1mmcM69kI_hL7XYP77YAilg6F_Fj1pxlI)\n",
            "✅ Successfully uploaded 9_04_2008.csv (ID: 12WUCs5OHE3e-HiKKqT-l3qCjNR5LyUGP)\n",
            "✅ Successfully uploaded 6_04_2008.csv (ID: 1LOIGP0sHpMe9bPtrr8P1xOXf5QFg2mex)\n",
            "✅ Successfully uploaded 3_04_2008.csv (ID: 1RntAF84pFw2lFXTBe7-U4nJfPNA868sT)\n",
            "✅ Successfully uploaded 8_04_2008.csv (ID: 1pwfCMxWQBP4dgHrU5nWQ7oX-FlDN1UD8)\n",
            "✅ Successfully uploaded 18_05_2008.csv (ID: 1Li3siHBlJFChYnWgyWwSUikKJnm1LK_7)\n",
            "✅ Successfully uploaded 16_05_2008.csv (ID: 1XIGpPDNBLM51kESA1VBJXYtfI9sGnOd7)\n",
            "✅ Successfully uploaded 26_05_2008.csv (ID: 1nMnusXGX7a9_CpyaKEj9sSZpw6-h-Wy_)\n",
            "✅ Successfully uploaded 10_05_2008.csv (ID: 1jt4Vg7x1uB-b8CAncGAXaNBPVquvepoH)\n",
            "✅ Successfully uploaded 5_05_2008.csv (ID: 1gjcyt1M7YCFzoMRzrqkcDp4r3bd0hSvZ)\n",
            "✅ Successfully uploaded 21_05_2008.csv (ID: 1H9wtLB0q7q6C0_4gRGm96HcQF3p4AuEf)\n",
            "✅ Successfully uploaded 12_05_2008.csv (ID: 1ey-WcKn3D3p5QOncM78cKkwUgrvOFNic)\n",
            "✅ Successfully uploaded 19_05_2008.csv (ID: 1DaksloPYwgYVg0IWBY9-fxu3TljpM-09)\n",
            "✅ Successfully uploaded 9_05_2008.csv (ID: 1ipegrblJt6_JjwI0tXshhu-bMQDdShdZ)\n",
            "✅ Successfully uploaded 14_05_2008.csv (ID: 1v9r9K_tG_vyZX-6nWuGfJznZ5WQr4zgy)\n",
            "✅ Successfully uploaded 11_05_2008.csv (ID: 1nwFCc1JBJYgV2iVANcjHX5Tv7pw5b5kv)\n",
            "✅ Successfully uploaded 4_05_2008.csv (ID: 1t9UxjYnW4psTZv1r2ka_lg3URFitkJ8M)\n",
            "✅ Successfully uploaded 25_05_2008.csv (ID: 1LYdt8xtrAwIt-rZb3G0k4CvPgh0SFtnu)\n",
            "✅ Successfully uploaded 3_05_2008.csv (ID: 1eL0mpLINZx8bJqwVtusNhEgCgKDPpzdW)\n",
            "✅ Successfully uploaded 24_05_2008.csv (ID: 12OI_4vgFeCs3EnuDdexe3AlNSPt1z8kg)\n",
            "✅ Successfully uploaded 1_05_2008.csv (ID: 1x4sJEkGLXvD7fCfx0EOHZ7Vyp6StYTEJ)\n",
            "✅ Successfully uploaded 30_05_2008.csv (ID: 1pa7oJgnzvRZ_qTRgeztUEVEkvfVaplPr)\n",
            "✅ Successfully uploaded 15_05_2008.csv (ID: 1o9xBIUxvCNVly73jwPdv9NwvJoqwtBzz)\n",
            "✅ Successfully uploaded 22_05_2008.csv (ID: 1i5GwT6B2OsUW1IEZKX4RnK9rI3trCSaA)\n",
            "✅ Successfully uploaded 2_05_2008.csv (ID: 1EC5wFin64ik6rzk4z6fQJyQKWx0s1Ono)\n",
            "✅ Successfully uploaded 27_05_2008.csv (ID: 1WmRKn43E4x4GrKQof4yddt5wo4a3cZpw)\n",
            "✅ Successfully uploaded 31_05_2008.csv (ID: 1KGe-1J0d1uSJj8PeyBti5Ae046jdyHaQ)\n",
            "✅ Successfully uploaded 6_05_2008.csv (ID: 1GqNb4U5fTWAIYy8n9SQNfLRM6IjCN3AF)\n",
            "✅ Successfully uploaded 13_05_2008.csv (ID: 1yimVP6vpvhyMwmqAAzN1a4Z_Fk444_k6)\n",
            "✅ Successfully uploaded 29_05_2008.csv (ID: 1j_Z2Of6k0yIoSTm0HuyuSQxRBwG1ZR4a)\n",
            "✅ Successfully uploaded 8_05_2008.csv (ID: 1hLpUqtn5d2GeAv1uQEQCHZ_rwQufSH8Y)\n",
            "✅ Successfully uploaded 7_05_2008.csv (ID: 1aOkI8Xseyciu9_iMH2Tl4sm9wiSyiT-2)\n",
            "✅ Successfully uploaded 20_05_2008.csv (ID: 1nLarWb33zGSBs2gn4IalOgaepq2D_mMz)\n",
            "✅ Successfully uploaded 23_05_2008.csv (ID: 1FKWzwYKgAu7FZ-jMKZX2LQ8v7CHE9Ez-)\n",
            "✅ Successfully uploaded 32_05_2008.csv (ID: 1vYg-LrLhMEZSIwGE7QMtkg9XqBQ4m7VI)\n",
            "✅ Successfully uploaded 28_05_2008.csv (ID: 1C7RmcLdmpvILWFw1Y1-mC1OVXvnoVmlG)\n",
            "✅ Successfully uploaded 17_05_2008.csv (ID: 1IcZ2qPq7w5yagxtj45Z6TYYk8dcA0eWg)\n",
            "✅ Successfully uploaded 33_05_2008.csv (ID: 1BkYy-pkH9I3cm4xT0aOZPQaAz9qI09cy)\n",
            "✅ Successfully uploaded 34_05_2008.csv (ID: 1JYBPzRRbzHFLLn8cWrCDwcodbLHxg5mx)\n",
            "✅ Successfully uploaded 18_12_2008.csv (ID: 1f18on5Rbp7cH8SPMw_j5Lo4ArIVro4e2)\n",
            "✅ Successfully uploaded 41_12_2008.csv (ID: 1Fd4zjkmvf_A5avflsJ1LQD3RRVy_TCEO)\n",
            "✅ Successfully uploaded 20_12_2008.csv (ID: 1rOSJkn6g-tTQNKPSlVx7nHWCwj4IrmOH)\n",
            "✅ Successfully uploaded 21_12_2008.csv (ID: 1RMe0Veee5eqOp8fheMDTq6fpcPSjDNc0)\n",
            "✅ Successfully uploaded 7_12_2008.csv (ID: 1MQzdcCSXSHkioO-8m7D4qcxl_Hq0BsC8)\n",
            "✅ Successfully uploaded 11_12_2008.csv (ID: 1NUGZNdnKBEGBjusyJmdcYIQG5yg7Yz9n)\n",
            "✅ Successfully uploaded 1_12_2008.csv (ID: 1OnKuB-a0dy8tNi_JUwu_AVMewzlPYmcH)\n",
            "✅ Successfully uploaded 13_12_2008.csv (ID: 1cBf2ihkIpDEwcuMeKZXs5VT7aM2twgs8)\n",
            "✅ Successfully uploaded 37_12_2008.csv (ID: 1GUOSYewPFerYGNDD21_UDVy0WrdjAf3i)\n",
            "✅ Successfully uploaded 17_12_2008.csv (ID: 1eCgmlQcv5V2LIdcqJgeimW5-qub41QXn)\n",
            "✅ Successfully uploaded 31_12_2008.csv (ID: 1vwKob52KP8sJEvsiS5M6DviHF36lBjSF)\n",
            "✅ Successfully uploaded 28_12_2008.csv (ID: 1gDod1zumYNywYSC-3FHtimEXvzr_Jz_2)\n",
            "✅ Successfully uploaded 26_12_2008.csv (ID: 17aLXjY15hi0oCUIBRbF857AWeBGqyDxK)\n",
            "✅ Successfully uploaded 10_12_2008.csv (ID: 1Qh3jYDqACSVNb6qfJmkPesR8mbS5Qo1I)\n",
            "✅ Successfully uploaded 40_12_2008.csv (ID: 1fHc4LZj_bB9wwul_brFWH8qwREzxwGGA)\n",
            "✅ Successfully uploaded 19_12_2008.csv (ID: 1dzBqmCCEmgnQa22yfiEmpfhZM3YZ1Jh4)\n",
            "✅ Successfully uploaded 15_12_2008.csv (ID: 1WwF_TxVysw_mvOkUwQnH0y5vltb5BcKx)\n",
            "✅ Successfully uploaded 3_12_2008.csv (ID: 1_8kG3csiXJwpYdM4j2VjUIZB8QoJKsoi)\n",
            "✅ Successfully uploaded 25_12_2008.csv (ID: 1Dxtc2wWa9nj1Q18m2XOZaTZWq8yUBB2u)\n",
            "✅ Successfully uploaded 8_12_2008.csv (ID: 1-F0oKejPJdAxxRh4am_Y4m10t28sJlVN)\n",
            "✅ Successfully uploaded 30_12_2008.csv (ID: 1rYEYtOYrlGHJ4iC-kqOjpnCvrIoyZzwV)\n",
            "✅ Successfully uploaded 6_12_2008.csv (ID: 1B944nIA016jSiK9lUxl78pg38EPQdcRq)\n",
            "✅ Successfully uploaded 4_12_2008.csv (ID: 14wfHQ8dGvH8UAl8UNScP4L6H4WYoaxSI)\n",
            "✅ Successfully uploaded 39_12_2008.csv (ID: 1lv5YcYH1xYyGJZOE4DAYcw4uzFi2DxEo)\n",
            "✅ Successfully uploaded 33_12_2008.csv (ID: 1toOwZSBtDPaj94zHsycVA6HG1QF36-1u)\n",
            "✅ Successfully uploaded 34_12_2008.csv (ID: 1kDZZE8xXaVglOXgjr3fyWzvAoK4W1LYg)\n",
            "✅ Successfully uploaded 2_12_2008.csv (ID: 1eYnB4N5triEC1AbJwjbIGHff_YQXBQZH)\n",
            "✅ Successfully uploaded 9_12_2008.csv (ID: 1nILnoxorDCJWItpSBeE6pPpJ3m_9Mibe)\n",
            "✅ Successfully uploaded 29_12_2008.csv (ID: 1wyvODNcBLVRSFF5cdr2aWGJUrG4ePBjp)\n",
            "✅ Successfully uploaded 35_12_2008.csv (ID: 1PKTDeqVXOizR8Jzii7sbMtBOHafUApk0)\n",
            "✅ Successfully uploaded 23_12_2008.csv (ID: 151-d_89tlexaIKjNChKna8dI7hr6uFdb)\n",
            "✅ Successfully uploaded 42_12_2008.csv (ID: 1kQmDdNcwRYgrl9BedJqiOc6GnRGINkdX)\n",
            "✅ Successfully uploaded 32_12_2008.csv (ID: 1QJVzmNxGgSWkMZwd7qYhguK0CerdhSEt)\n",
            "✅ Successfully uploaded 27_12_2008.csv (ID: 1YF8bsO3Ru1JjGXua6JfihoDrO2JPfwQe)\n",
            "✅ Successfully uploaded 24_12_2008.csv (ID: 1K22Q6X96p8SJmcEIY529LtGswqlY_0f3)\n",
            "✅ Successfully uploaded 5_12_2008.csv (ID: 1xABPV7_iR3Vj3tvdayGwHz3m6qf-5NlB)\n",
            "✅ Successfully uploaded 38_12_2008.csv (ID: 1I94YvUZgo6cDhZ4RKwWCupJDJEB08Gcw)\n",
            "✅ Successfully uploaded 22_12_2008.csv (ID: 1qO7rTx7LJIfhcrRJe0CoJlE6xbdKiSYj)\n",
            "✅ Successfully uploaded 16_12_2008.csv (ID: 1gu1Az79f6n1wCy8lC6X1F6u1LkyZyHbr)\n",
            "✅ Successfully uploaded 36_12_2008.csv (ID: 1eI42sFVgQJb1Dx3r9jMoA4VSwXjxJSqE)\n",
            "✅ Successfully uploaded 14_12_2008.csv (ID: 1H7eYnXVW3boMM_OrwV8G6tbkzAE1b6q-)\n",
            "✅ Successfully uploaded 12_12_2008.csv (ID: 16wKaIFvSAjoztRYwPA2MUfCnPU7BDSFM)\n",
            "✅ Successfully uploaded 10_01_2008.csv (ID: 1iTqh2Ipy9sjFR3T7Anl7ACUhNtHXVdMs)\n",
            "✅ Successfully uploaded 15_01_2008.csv (ID: 1ugr446ER90RRRMMh-siznUZIAkaoI8bT)\n",
            "✅ Successfully uploaded 7_01_2008.csv (ID: 1IkEndbbPJw9muygYo3xuellBWowgoQpW)\n",
            "✅ Successfully uploaded 9_01_2008.csv (ID: 176UWf0_uZLyjLVxpRyy-_KwjD-FglHE8)\n",
            "✅ Successfully uploaded 4_01_2008.csv (ID: 1WQ0zdds3_5zkhtORRP1YNGrevPuC8o9c)\n",
            "✅ Successfully uploaded 3_01_2008.csv (ID: 13Xm4yqyFwNCNOar0570cZ6Qd7022Q6xv)\n",
            "✅ Successfully uploaded 18_01_2008.csv (ID: 16q4wXuS1BPqy4IwePdaawU6QBLjR2f4P)\n",
            "✅ Successfully uploaded 11_01_2008.csv (ID: 1dxFd9vf4EKoxeSDegUTs8oWI37RgEHev)\n",
            "✅ Successfully uploaded 13_01_2008.csv (ID: 1jd3JMq4soKH7ZWUr0rgMUugmK9Q6Bg3W)\n",
            "✅ Successfully uploaded 5_01_2008.csv (ID: 17jqUq9le5qiMsFroocTvEPOzUWMvVGDW)\n",
            "✅ Successfully uploaded 19_01_2008.csv (ID: 1E_btsLGp2MmDkcNXFiUK8pGaEE2bC4LU)\n",
            "✅ Successfully uploaded 6_01_2008.csv (ID: 1qKxKkuRLz16kwWSfFbTELo-cT6ygNPHB)\n",
            "✅ Successfully uploaded 17_01_2008.csv (ID: 1NCfxlXpgxc7eTHZMqAye3vBYZDe1wnxB)\n",
            "✅ Successfully uploaded 8_01_2008.csv (ID: 1soqk6WPTQgC4JgTuXq5Dsz6j2vRw4vp4)\n",
            "✅ Successfully uploaded 1_01_2008.csv (ID: 1ZX_J949wcqi_NlycNqt1rMgCUX-Dgw9W)\n",
            "✅ Successfully uploaded 12_01_2008.csv (ID: 1VUMw_j29C3xY_h256wv-TlJKuPU04S1n)\n",
            "✅ Successfully uploaded 20_01_2008.csv (ID: 105ctft9PIzBdrIazJYSD5B_CSiLW4nN9)\n",
            "✅ Successfully uploaded 2_01_2008.csv (ID: 1mKbYS0KbHPrxRb8rmmKE2jN3XLgVPjOt)\n",
            "✅ Successfully uploaded 16_01_2008.csv (ID: 19mcRVd1Nych0HHacKg3Piz89QWnVhATj)\n",
            "✅ Successfully uploaded 14_01_2008.csv (ID: 1NmSsJVfmPuq7qPC8lXq2w0-1trJ1k-tJ)\n",
            "✅ Successfully uploaded 12_10_2008.csv (ID: 1BfUWH4U6Q-RdQF0P0-0yCUapxMjIFTlt)\n",
            "✅ Successfully uploaded 15_10_2008.csv (ID: 1_UPQvBL7uGAem9mOFciKCwjNQ48DtnmD)\n",
            "✅ Successfully uploaded 3_10_2008.csv (ID: 14T1LXRyXKIR6xlK3vXn4NOIUpUAfWIEo)\n",
            "✅ Successfully uploaded 16_10_2008.csv (ID: 1BJLiN8s6Ca8HzF_g4tLSImkS60BCTSae)\n",
            "✅ Successfully uploaded 13_10_2008.csv (ID: 14LSqhi9QPLWnPxbFWFqhdYFW9BryafXs)\n",
            "✅ Successfully uploaded 6_10_2008.csv (ID: 1I6mfJX2PA3csLFRN118E-9O67WbhN3mh)\n",
            "✅ Successfully uploaded 7_10_2008.csv (ID: 1KprkcT-Iwx4IM3Eyl1TRLcJQwY6mz465)\n",
            "✅ Successfully uploaded 14_10_2008.csv (ID: 10sqq79R_OwHmUbt9iABLPlCfoCiSsZec)\n",
            "✅ Successfully uploaded 4_10_2008.csv (ID: 14EPECSX1hiMs6iKthhbDziGDNXNkpceW)\n",
            "✅ Successfully uploaded 17_10_2008.csv (ID: 1EKpxn6qheNMmCUe2n_sJmr7KZaqy54Fx)\n",
            "✅ Successfully uploaded 8_10_2008.csv (ID: 1PdqlDwWHBzJNbVWmaKoMysII2-AoORZ2)\n",
            "✅ Successfully uploaded 10_10_2008.csv (ID: 1XnzJFtZJlwgajOgneMX3xTWdZ7J3ByAr)\n",
            "✅ Successfully uploaded 1_10_2008.csv (ID: 1xvOZDwXswvdE6pdi0U6VXzrwen2ofi8H)\n",
            "✅ Successfully uploaded 11_10_2008.csv (ID: 1jI6Sfn6oEjjeRYNfqIFQv7lqJh4F459f)\n",
            "✅ Successfully uploaded 2_10_2008.csv (ID: 1a_Zdcu9FopjzGceCsUAMXva3Ght8OHCE)\n",
            "✅ Successfully uploaded 9_10_2008.csv (ID: 1BxB-EFejbU0qPImB0_9EQQwlIBtQXBYR)\n",
            "✅ Successfully uploaded 5_10_2008.csv (ID: 171wKEXmpjAR1Sq-RTX29lgQQIQvFxRjd)\n",
            "✅ Successfully uploaded 18_10_2008.csv (ID: 1-TP3Hi0aYou2E8dLl0kyaRWA5M6Bq0vA)\n",
            "✅ Successfully uploaded 15_08_2008.csv (ID: 1RHLEik4Rx_Nh04VZjudpapRyGqWmrcE2)\n",
            "✅ Successfully uploaded 17_08_2008.csv (ID: 14HPWFWODxAPL9O1cPIcLe-2w4FfhsKMQ)\n",
            "✅ Successfully uploaded 20_08_2008.csv (ID: 1nkCIpSOj6Zrv6Oy13NjcDyGCLx_sEKAT)\n",
            "✅ Successfully uploaded 6_08_2008.csv (ID: 1Bm55k9txo62Yhn_O-2hZyyklGw8-bwjA)\n",
            "✅ Successfully uploaded 27_08_2008.csv (ID: 1aAhUZw9dH5Nb-pCeeCBHQRwNzbjnnBa7)\n",
            "✅ Successfully uploaded 35_08_2008.csv (ID: 1A8398ic1NjZcf1pnmYv7vQoT7U4awUKo)\n",
            "✅ Successfully uploaded 10_08_2008.csv (ID: 1c01gWCnM_nRAFOzgK0_GHiQeYXpEWnc4)\n",
            "✅ Successfully uploaded 26_08_2008.csv (ID: 1Vs_HNi7FLmNxi-R-aZr5KUC9vpzJn3yo)\n",
            "✅ Successfully uploaded 12_08_2008.csv (ID: 1kCj0RxADLHk6lIh-BAMyJU0GueE6_Zi-)\n",
            "✅ Successfully uploaded 33_08_2008.csv (ID: 1ajWCiA_NWGwS-c0Vt68TwdkWdZIbHRDQ)\n",
            "✅ Successfully uploaded 18_08_2008.csv (ID: 1Pa2DucLFy39qS7LAOJnvSNnKAEh6xWmh)\n",
            "✅ Successfully uploaded 38_08_2008.csv (ID: 1FTBsIA5sVJOEgln4AKoD_bGvQVW2N_m_)\n",
            "✅ Successfully uploaded 41_08_2008.csv (ID: 101brFb4SINX0XI29AMigQ3an1Ih9FrCm)\n",
            "✅ Successfully uploaded 7_08_2008.csv (ID: 1PtcXaxiSWCiXlsZDrInH5pc8AFXFlrPD)\n",
            "✅ Successfully uploaded 37_08_2008.csv (ID: 1_sSmYx5dOB4jJcodF2QX0odgbGXcmhjp)\n",
            "✅ Successfully uploaded 8_08_2008.csv (ID: 1v3o7a9RLVwwkdUrXXxQHshEtJinec4xL)\n",
            "✅ Successfully uploaded 14_08_2008.csv (ID: 1N6BcLFpAZ0CxMSGduHBnz9k16szpx1QS)\n",
            "✅ Successfully uploaded 5_08_2008.csv (ID: 1i8pgUjOr01jXReWB-b663KIOrdpSsTNR)\n",
            "✅ Successfully uploaded 36_08_2008.csv (ID: 1MwEHGZ2VoPVptApHst0_Dhv9NvdvCdFy)\n",
            "✅ Successfully uploaded 30_08_2008.csv (ID: 1p-HsgFR6CU6NYUw14Fsmg_1eOdmmWz79)\n",
            "✅ Successfully uploaded 23_08_2008.csv (ID: 1WOKpSCK0i9gjpZ7DdrYgSAZfQQ3HQwMB)\n",
            "✅ Successfully uploaded 25_08_2008.csv (ID: 1VTuJJkOil8hesaJ_3N4noxpQ7bPggLeW)\n",
            "✅ Successfully uploaded 16_08_2008.csv (ID: 1r9-Dj5Zpl9J5XpI1fGRRJTZRoxlqCOT-)\n",
            "✅ Successfully uploaded 1_08_2008.csv (ID: 1-3PRhoLRSjy3Q2nAy_ZbZ3xFTBkXj54J)\n",
            "✅ Successfully uploaded 39_08_2008.csv (ID: 1hg1CGMkJjHVgfuRc91px8de8nCpiMs3s)\n",
            "✅ Successfully uploaded 32_08_2008.csv (ID: 1nvRDTZWeb9RFhlb79ZV-f98bpa-4U7k7)\n",
            "✅ Successfully uploaded 19_08_2008.csv (ID: 15QJXHRp9FjUKbTR4oIlsRsPYoBKz_GSi)\n",
            "✅ Successfully uploaded 28_08_2008.csv (ID: 1S7Of9kAFhr0hUPhkIYFtHa_hwH5Lu7Km)\n",
            "✅ Successfully uploaded 40_08_2008.csv (ID: 1THsIz-SkZsLhzXV3HcbfCzcvldO8II6m)\n",
            "✅ Successfully uploaded 13_08_2008.csv (ID: 1_hvzJ6Q7hbIjOzuFQuW7E9pzeYU0XZvh)\n",
            "✅ Successfully uploaded 4_08_2008.csv (ID: 17-0y2ObBLzEFi_og13S-D4dXHA5gDoT0)\n",
            "✅ Successfully uploaded 31_08_2008.csv (ID: 1QK-j4RgzFm0tLp4HCASjGlOIF9raQPC-)\n",
            "✅ Successfully uploaded 2_08_2008.csv (ID: 1SvW_-bRLNPkuWyLH-dISJ4HXWsg5S6qb)\n",
            "✅ Successfully uploaded 3_08_2008.csv (ID: 17v29eyJoycFUGHVFM5yrWvZ1DoxCwfP1)\n",
            "✅ Successfully uploaded 29_08_2008.csv (ID: 1MYApDbUdX4TbdC7kcK1S1YHKnf-1E7rN)\n",
            "✅ Successfully uploaded 22_08_2008.csv (ID: 1Jd0AKlzMCJk6blkFWgqZON37L9RFIhcR)\n",
            "✅ Successfully uploaded 11_08_2008.csv (ID: 1nPp6qA5vmNJC2wu6otinERx3WIuj8brH)\n",
            "✅ Successfully uploaded 9_08_2008.csv (ID: 11vQ8FA3an501PUFbgrFpyhP9Hm_Ad3pL)\n",
            "✅ Successfully uploaded 24_08_2008.csv (ID: 1xglZfNM13mqDYHUbl95QRbNg_q5121Za)\n",
            "✅ Successfully uploaded 21_08_2008.csv (ID: 1-U7CnjNtnM31YJbs7j3ThZT0QsYOTHdn)\n",
            "✅ Successfully uploaded 34_08_2008.csv (ID: 1r4CjImp8NErSSK_LA6I_pFzRYagmrc9H)\n",
            "✅ Successfully uploaded 10_09_2008.csv (ID: 1usWP2RioltC6tGaQ1Q-fHoW6CZ_mme4s)\n",
            "✅ Successfully uploaded 22_09_2008.csv (ID: 1dLv33Dn3VssnpDXnWdXiYT2ZawIQoio7)\n",
            "✅ Successfully uploaded 3_09_2008.csv (ID: 1rqRHpOesNJ58sOoHgu6KK3okIx6A-L58)\n",
            "✅ Successfully uploaded 15_09_2008.csv (ID: 1j7IUqb2hzciDDbv32_GjFw_5CkBGLlYg)\n",
            "✅ Successfully uploaded 2_09_2008.csv (ID: 1UL2G-TGXM-iuAgIWsRIGnIe9uPvOGool)\n",
            "✅ Successfully uploaded 21_09_2008.csv (ID: 1RIfRqpL3Qd2PIWg71HQGeflFsTq1RO1v)\n",
            "✅ Successfully uploaded 1_09_2008.csv (ID: 1ykP6ZVs-o9T3z1USCY2kQUSLRzM0STZg)\n",
            "✅ Successfully uploaded 14_09_2008.csv (ID: 158yUxduqJ_j6vog8rjIHuZ-c2_qLKoeZ)\n",
            "✅ Successfully uploaded 12_09_2008.csv (ID: 1wJqo0J_-2-BSsQ1UNGZg7FtB_wQ5HSoM)\n",
            "✅ Successfully uploaded 8_09_2008.csv (ID: 1440M_jXPyvpNv0_VtxrMBdUACsxS5SHZ)\n",
            "✅ Successfully uploaded 20_09_2008.csv (ID: 1FI_F1PtwgV3KDlW9VRDSJWbVXMqWYTqp)\n",
            "✅ Successfully uploaded 19_09_2008.csv (ID: 1RhCkv9PhEtqHYjKFup56cFb0Q8gnLbtR)\n",
            "✅ Successfully uploaded 6_09_2008.csv (ID: 1EK2pAp5ZZh5TnAdts4RoVm-7vVcPoQ59)\n",
            "✅ Successfully uploaded 16_09_2008.csv (ID: 1RgRwFiv8wNynnt0SdJiD3TKqMae2bo07)\n",
            "✅ Successfully uploaded 11_09_2008.csv (ID: 17xRK-2XR4GP0jjorHL0_QQ_j-pECbnEx)\n",
            "✅ Successfully uploaded 4_09_2008.csv (ID: 1zxHIuErte1Op4VokTPLbHEB4Zd4hmirF)\n",
            "✅ Successfully uploaded 5_09_2008.csv (ID: 1TYAMI-ZbVmYEUHATsQ8Tx7FMCJtkYOMs)\n",
            "✅ Successfully uploaded 18_09_2008.csv (ID: 1MTAEdq5xZOLDMwWNXe88EujUaoBsTnuP)\n",
            "✅ Successfully uploaded 9_09_2008.csv (ID: 1mdsNQCRue42ASELMtsMC2mJtLQhHOEgj)\n",
            "✅ Successfully uploaded 7_09_2008.csv (ID: 11MHAHfq9DPl6DetWsxWwBeE5wMOYfpDb)\n",
            "✅ Successfully uploaded 13_09_2008.csv (ID: 1J-tx2W9--uj6OVsA7wa4-S54zHDmMWUS)\n",
            "✅ Successfully uploaded 17_09_2008.csv (ID: 11JX9BCF-b0IWdRdVdd02DAPrDNXlvzFX)\n",
            "✅ Successfully uploaded 8_15_2008.csv (ID: 1O_j_-EfM6LblqGkUAbaZQHqYpt0_zrAj)\n",
            "✅ Successfully uploaded 3_15_2008.csv (ID: 13BhOIgW5r8UUK8fE4c4NF-RDyTokQmMf)\n",
            "✅ Successfully uploaded 2_15_2008.csv (ID: 1-ulBOpIv0H77aFP0unN0glIyLQaqkx5N)\n",
            "✅ Successfully uploaded 5_15_2008.csv (ID: 1ybN1KDIgTT0CNLQeae0tpO9IQWPsv8dL)\n",
            "✅ Successfully uploaded 9_15_2008.csv (ID: 1b9BJY7sNuQXCeH1cpB3J8Szcmz59oT79)\n",
            "✅ Successfully uploaded 1_15_2008.csv (ID: 1smOgVC0PYtVUmDN3a-AuwhDlw3uiAGIn)\n",
            "✅ Successfully uploaded 7_15_2008.csv (ID: 1dW2hvzVpIqmiHhlCMJ5Ioxhw3iIntSE_)\n",
            "✅ Successfully uploaded 6_15_2008.csv (ID: 1lRPHc63GEcI-hOxPzNreq6ZzvAWBRCMh)\n",
            "✅ Successfully uploaded 10_15_2008.csv (ID: 1x8HVjBVlAQzwN3HVQEpZ3z1aWCmCFIpB)\n",
            "✅ Successfully uploaded 4_15_2008.csv (ID: 1WFVa3brHhZE6b3be-kWdaehj459CkvmP)\n",
            "✅ Successfully uploaded 18_13_2008.csv (ID: 1K2dyqoawAhvpX1e9G2CLDFVXpUi16N4t)\n",
            "✅ Successfully uploaded 22_13_2008.csv (ID: 1prUvwSpNi-o1bAT4FZVHrPvmowekDBu5)\n",
            "✅ Successfully uploaded 8_13_2008.csv (ID: 1lml5pfOIJ4ynmfkAn-GB4HY52w_oBMyx)\n",
            "✅ Successfully uploaded 2_13_2008.csv (ID: 1siR6ysdztzJJDYum-XrdnJ1uLdhnFacQ)\n",
            "✅ Successfully uploaded 4_13_2008.csv (ID: 1IU4zCTSQ1fkwvGWU5_iY4ufGCO3-HmUv)\n",
            "✅ Successfully uploaded 7_13_2008.csv (ID: 1RRmR9vw8h8JjD4DxAo1_fJy_sathOmK-)\n",
            "✅ Successfully uploaded 20_13_2008.csv (ID: 1ATSMm1jQaY3CG9x7wF6BYx_LV-zGVNBv)\n",
            "✅ Successfully uploaded 5_13_2008.csv (ID: 1daTD9wUuF314cPdlXo_NFLArPRY3r_-K)\n",
            "✅ Successfully uploaded 6_13_2008.csv (ID: 1jARC-bMich4dkBEEyBC4nM8P4nzenuI9)\n",
            "✅ Successfully uploaded 23_13_2008.csv (ID: 1J27doJfzr6-mYeACQolIYwh3J2ZjgV6_)\n",
            "✅ Successfully uploaded 10_13_2008.csv (ID: 11uY4s3oGQn4rAJFGELWr0gxA-FgE607c)\n",
            "✅ Successfully uploaded 1_13_2008.csv (ID: 1HU1kh35lses1MomOar0epWyALrNYZYv1)\n",
            "✅ Successfully uploaded 15_13_2008.csv (ID: 1npZ8tgg_zPzWNQLAJG2kgCZtyoRCeWfe)\n",
            "✅ Successfully uploaded 16_13_2008.csv (ID: 1LU5TrV6T6ozdXw8QARv9_L9fqXWaHLmW)\n",
            "✅ Successfully uploaded 11_13_2008.csv (ID: 1B1ikPb1Y0ywFNpXowGaRg0UP3ufeSr8M)\n",
            "✅ Successfully uploaded 3_13_2008.csv (ID: 1tNeLG9BmUoK9lp2ibHNxNLSmYJRWGzm6)\n",
            "✅ Successfully uploaded 14_13_2008.csv (ID: 1N9pAEpVChyJcSM3ZGkVxfgYeufQevPBO)\n",
            "✅ Successfully uploaded 9_13_2008.csv (ID: 1qrc8aXjl11SdYknUvGGSxzKjKFHOjs6W)\n",
            "✅ Successfully uploaded 21_13_2008.csv (ID: 1VCNGHzZU31xQMWjEAmSaR1K0f6IP1pEW)\n",
            "✅ Successfully uploaded 19_13_2008.csv (ID: 1IBAtPF1_ITh11Vgp5oio2B-SOhpU1Zuj)\n",
            "✅ Successfully uploaded 13_13_2008.csv (ID: 1Xh5NoPrnXqZKA4lUcoV5AZ1H5JHOyP8t)\n",
            "✅ Successfully uploaded 17_13_2008.csv (ID: 1fLQ84_sN9HJpXlUK5JleNLM535ZKUb_d)\n",
            "✅ Successfully uploaded 12_13_2008.csv (ID: 1mpQR3of36vYme3HmHLrVUHgQ2CQjNcBc)\n",
            "✅ Successfully uploaded 4_07_2008.csv (ID: 1ahnBTg61NYNNS6F_jUnRCn58Ob4IDsks)\n",
            "✅ Successfully uploaded 11_07_2008.csv (ID: 1zJOqGRFSTFEV6jN7BmYdKO83R_k5LbYo)\n",
            "✅ Successfully uploaded 15_07_2008.csv (ID: 1cDpcpNujrPvOlDQ13N9lLscp6dRKGpAE)\n",
            "✅ Successfully uploaded 12_07_2008.csv (ID: 1QyhO_9uOVuIdHl5Nzm3A1nhzQqBKhEse)\n",
            "✅ Successfully uploaded 26_07_2008.csv (ID: 1IBENoKX1GNVw7aKT8dxRXORrnE4X28QT)\n",
            "✅ Successfully uploaded 31_07_2008.csv (ID: 1OqC2nmb6MMbXiczNODnBQwYObiYsVu09)\n",
            "✅ Successfully uploaded 17_07_2008.csv (ID: 1LW1trV69EA72mgTZsCO2ltU-MMx4lbMD)\n",
            "✅ Successfully uploaded 20_07_2008.csv (ID: 140kvkxYM10aP2DZxRIfLqXEgL0xA1mqy)\n",
            "✅ Successfully uploaded 21_07_2008.csv (ID: 1AZJrn0RrBgcNaZnxx6Aqz-btDhpPqg36)\n",
            "✅ Successfully uploaded 3_07_2008.csv (ID: 18xSC6Sqi-OSqApeeTjnVk5ZjHOVGmBGa)\n",
            "✅ Successfully uploaded 5_07_2008.csv (ID: 1FpWWcNZkmIztGepLaBpdUMRFzM_O8rU2)\n",
            "✅ Successfully uploaded 27_07_2008.csv (ID: 1sCEZ5RwUqM2CKmolRsau_lYwHGJ2SAlQ)\n",
            "✅ Successfully uploaded 23_07_2008.csv (ID: 1jcnSSaATlijwgt3lZgBKRHP3kOC8WEOE)\n",
            "✅ Successfully uploaded 18_07_2008.csv (ID: 1sGqqeuizcEoavwjgnM5xZDz4awzijH9X)\n",
            "✅ Successfully uploaded 13_07_2008.csv (ID: 18QDqwEr67NYda24SaT9vyrZgb8pJTWrk)\n",
            "✅ Successfully uploaded 9_07_2008.csv (ID: 1szbBTaRV3qIj2rX4J6LnM-jc1NzVoe5M)\n",
            "✅ Successfully uploaded 33_07_2008.csv (ID: 1w2tBXKP4NULzmeBGAlrUH14Q21UwvFoO)\n",
            "✅ Successfully uploaded 14_07_2008.csv (ID: 1T5kh0xtXSLMmTs9LlFKgSyOEjiXGFCZW)\n",
            "✅ Successfully uploaded 10_07_2008.csv (ID: 1LQeeubyFnCFmv2l6B3T2Y1Qn2kM1ncyv)\n",
            "✅ Successfully uploaded 2_07_2008.csv (ID: 1cHdvsl-ay6Sq2cUdYSghCaIDgATJW1x9)\n",
            "✅ Successfully uploaded 32_07_2008.csv (ID: 1pRh_5mmbmKStsf5LFuXlHbEBLvyGeMGJ)\n",
            "✅ Successfully uploaded 30_07_2008.csv (ID: 1RFGdJvrbbW9v2axGEVoGnsg9-slWHvjr)\n",
            "✅ Successfully uploaded 19_07_2008.csv (ID: 1y6V4Ty9Tn7PtJ3NrtFB9BGXm2GzDLZP6)\n",
            "✅ Successfully uploaded 34_07_2008.csv (ID: 1i1PlRYTpsG7hiEDOWQFiPDwkIeugEpeu)\n",
            "✅ Successfully uploaded 16_07_2008.csv (ID: 1xvlJgAJFD5U4061Tv_Fe786xFRa9HpIY)\n",
            "✅ Successfully uploaded 24_07_2008.csv (ID: 19k11pk_u6hkJn12JLRsIPkQ_xSkBdKcL)\n",
            "✅ Successfully uploaded 28_07_2008.csv (ID: 1aK8oT3lk4OpLhSGOm1UqVop9bmJ7uce-)\n",
            "✅ Successfully uploaded 8_07_2008.csv (ID: 1evSCLxHaYMJkG_9ViXuq4iwF27x5YD1S)\n",
            "✅ Successfully uploaded 29_07_2008.csv (ID: 14EcYmSNUDyLfv6hP2j8eGekE4y8x-fQ7)\n",
            "✅ Successfully uploaded 1_07_2008.csv (ID: 1Q0n5sIIaQ754eVmwdX6C9Bcc8T3cdsVL)\n",
            "✅ Successfully uploaded 6_07_2008.csv (ID: 1Sm2ngeJvJZ_pskyLmsTi5emMG4j_H0Pi)\n",
            "✅ Successfully uploaded 22_07_2008.csv (ID: 1BWW2OCn9zBd4EOkIEM7Ac4ZVmAbKxMwG)\n",
            "✅ Successfully uploaded 25_07_2008.csv (ID: 1ZN4hKgHH8Gzi6FkJOurshmv0_OdpXsS8)\n",
            "✅ Successfully uploaded 7_07_2008.csv (ID: 1mRGSYdAATP0Cj8oRGDtpmpckQ8pP02lh)\n",
            "✅ Successfully uploaded 22_03_2008.csv (ID: 13t4Z81m-Ps8IZbUaPcHGdK9cGDCA2ngg)\n",
            "✅ Successfully uploaded 17_03_2008.csv (ID: 1FDWUoPWgjVUy-i-esvL-1ZGF2Xzl6wPW)\n",
            "✅ Successfully uploaded 21_03_2008.csv (ID: 1ZSSZMk3wMGrjGxlhWAQBpTtG9t04cg4Y)\n",
            "✅ Successfully uploaded 20_03_2008.csv (ID: 1vVvk5NjhHe37dL_Z-Czpe9SzY71Bm-h5)\n",
            "✅ Successfully uploaded 25_03_2008.csv (ID: 1dJdx-RCDktY9hipU6AbM2WJfTWioYA7K)\n",
            "✅ Successfully uploaded 10_03_2008.csv (ID: 1SrfNjmUYZuGaMNpeUYsFPagkZ3AePxZj)\n",
            "✅ Successfully uploaded 15_03_2008.csv (ID: 18aUk4eYavCJ5jLv-7Aj6pMP6JTUY3Z5P)\n",
            "✅ Successfully uploaded 14_03_2008.csv (ID: 1DvuKLXr8cK20uNSZFxcHG5SiXijY21Ei)\n",
            "✅ Successfully uploaded 7_03_2008.csv (ID: 1rq_ooV3tCmrqHrahMBszjEitxNN0jI74)\n",
            "✅ Successfully uploaded 19_03_2008.csv (ID: 1Pmyq-h7_amzkADDYQUDdn1YW5PZsW65b)\n",
            "✅ Successfully uploaded 9_03_2008.csv (ID: 13OarUM4Ng2bkOCOGMiwGXonzisFhLCt2)\n",
            "✅ Successfully uploaded 18_03_2008.csv (ID: 1zOUFADjEtHDfVopIHPiy81xWrEvoEEnk)\n",
            "✅ Successfully uploaded 12_03_2008.csv (ID: 1LhxjqY8RUP8qBVsIcjdtCPKntJHlPL73)\n",
            "✅ Successfully uploaded 3_03_2008.csv (ID: 1-84573lnAzTm1phbz-GmMnCRr6T9qRkb)\n",
            "✅ Successfully uploaded 8_03_2008.csv (ID: 16Gms8aE76n7XGmKM5lvfOnDZbRid18gb)\n",
            "✅ Successfully uploaded 2_03_2008.csv (ID: 1EL4prO5lgaq7tuYmRv_nTw2qLjMigrQe)\n",
            "✅ Successfully uploaded 4_03_2008.csv (ID: 1lpc4K3kx8Zt-L7quOhV4ZoMe8PJFJ2Vt)\n",
            "✅ Successfully uploaded 16_03_2008.csv (ID: 12Ds0-FuMzuvbVHc3qgn5TSu2gE7Hyjn6)\n",
            "✅ Successfully uploaded 24_03_2008.csv (ID: 1JE72Qm9Fcuk2DmLDPmF2iwRbtBqmunOY)\n",
            "✅ Successfully uploaded 5_03_2008.csv (ID: 1w2j285bxDiJ1b3xLEs-NJJtlhDMvjewN)\n",
            "✅ Successfully uploaded 13_03_2008.csv (ID: 1hPdfeRqAu56KCq2x5lpR9uongzCeOjXz)\n",
            "✅ Successfully uploaded 23_03_2008.csv (ID: 1vod_Fk6a_TKRTgkMV3zAyLeaS26W4d7Z)\n",
            "✅ Successfully uploaded 11_03_2008.csv (ID: 1or_F3UC9aET7H8jI2YUZBHBt-SvtORXk)\n",
            "✅ Successfully uploaded 1_03_2008.csv (ID: 1UkSrvYMvLMi5TEzjZ9CSgCUButHS0xWT)\n",
            "✅ Successfully uploaded 6_03_2008.csv (ID: 1NV8dYQoRgBJKW4P8MAGEhRmH7ScTDlCe)\n",
            "✅ Successfully uploaded 31_14_2008.csv (ID: 1PnHwpbcRnqrZCREG_kHwfC7VVUkL3SNE)\n",
            "✅ Successfully uploaded 22_14_2008.csv (ID: 1DK95HcG0UAS909n4T-1kVMiJk5H8t37e)\n",
            "✅ Successfully uploaded 14_14_2008.csv (ID: 1o5JJofOObmzUJYANHpCq_9keSTQktBPV)\n",
            "✅ Successfully uploaded 23_14_2008.csv (ID: 1lGoPm5AcG1wVwT6VFv2jwMdCS656NquL)\n",
            "✅ Successfully uploaded 1_14_2008.csv (ID: 1FmacCOglbhpYkIb413l_0RNxPtlqfN0x)\n",
            "✅ Successfully uploaded 17_14_2008.csv (ID: 1FoaAKGQBYyVDorBD5Y-wT72krTzobHrM)\n",
            "✅ Successfully uploaded 34_14_2008.csv (ID: 10DTPjCnZzQVsYGktAZ51qhVYUczR-1lF)\n",
            "✅ Successfully uploaded 43_14_2008.csv (ID: 1WQAwO-lSE98ijLmdyhFKY-u2eTzkmsqX)\n",
            "✅ Successfully uploaded 37_14_2008.csv (ID: 1rf7loti3OPd05x7W9WhvR4MVthNAU75b)\n",
            "✅ Successfully uploaded 40_14_2008.csv (ID: 1IVYl55pgJfWvd_9vrrMQskwcMX1f-x2h)\n",
            "✅ Successfully uploaded 5_14_2008.csv (ID: 1QdPIBF76F5VjALu9SAmRjRjKV9bwLsfL)\n",
            "✅ Successfully uploaded 30_14_2008.csv (ID: 1a4NrE3-UHHqKDIRCMlEpy8vZcATmE7ti)\n",
            "✅ Successfully uploaded 3_14_2008.csv (ID: 1TaUUEGoRiOC-HdKvu4asjd_afVtCedhm)\n",
            "✅ Successfully uploaded 42_14_2008.csv (ID: 1JquayorKD5MF_g5b-FHoaTFRtqoJ6dc5)\n",
            "✅ Successfully uploaded 20_14_2008.csv (ID: 16PLBznAxbYQietZUiOVYKb8d3D_i7ndj)\n",
            "✅ Successfully uploaded 38_14_2008.csv (ID: 19uUqs7MiBgZSGti9TcoLSAPWqLLQ7lPz)\n",
            "✅ Successfully uploaded 8_14_2008.csv (ID: 17XpAURNcLtRyF08zX8kermkyX3Y7M_JW)\n",
            "✅ Successfully uploaded 39_14_2008.csv (ID: 1euEha92Ir8MtWEcROpWtgOTTh1poyEWs)\n",
            "✅ Successfully uploaded 16_14_2008.csv (ID: 1YNsmg_1Cvl_Gq6sXaPyWZkY1gB6nQzM6)\n",
            "✅ Successfully uploaded 11_14_2008.csv (ID: 1JPSfxURJAok7xIC86dOIZMVjwgKL2dxQ)\n",
            "✅ Successfully uploaded 15_14_2008.csv (ID: 1LjtPkpPlL9wYiFNVETZSifPuO86BbEEY)\n",
            "✅ Successfully uploaded 25_14_2008.csv (ID: 1xz6xInV46VkLCeMiztwbuLtKxMXTGN5F)\n",
            "✅ Successfully uploaded 19_14_2008.csv (ID: 193F9R6T71uJFM71vD8GrHwnG7ZocOXF8)\n",
            "✅ Successfully uploaded 10_14_2008.csv (ID: 1rTBH8QmT-xVnDEOvpKYnRr1xCeUFtAf7)\n",
            "✅ Successfully uploaded 32_14_2008.csv (ID: 1YrFgo8VWFq5eOMpSn3GUDbf5oDj07Brm)\n",
            "✅ Successfully uploaded 4_14_2008.csv (ID: 1h5Iq4185t1t3i4G9jP8YfkVu77C9qqdm)\n",
            "✅ Successfully uploaded 27_14_2008.csv (ID: 1QABHxSx29QWx7r0irGFOcYkGd63OJQTw)\n",
            "✅ Successfully uploaded 7_14_2008.csv (ID: 1yizsM5Omb0Dfd5BLQWBORQ69_D3espPw)\n",
            "✅ Successfully uploaded 41_14_2008.csv (ID: 1uJVdnCt6duHs7l1ceVvUo1e85IPzyh6i)\n",
            "✅ Successfully uploaded 26_14_2008.csv (ID: 1XUu_HbxoA7FibzaflYm-o-D8dFflXhEh)\n",
            "✅ Successfully uploaded 28_14_2008.csv (ID: 1x_dQe09qVZGfJNbgMyR5OwHSMmGltZ01)\n",
            "✅ Successfully uploaded 2_14_2008.csv (ID: 1F77Wc-MbbA967VII5FkTvr1Ph20fmKTb)\n",
            "✅ Successfully uploaded 24_14_2008.csv (ID: 1D0QXeUpborkZ5hYUq_00_wk9gZF977ws)\n",
            "✅ Successfully uploaded 9_14_2008.csv (ID: 1sJN5T1wAqcge2xvJFKbLpIKVQVHLOXYa)\n",
            "✅ Successfully uploaded 18_14_2008.csv (ID: 1A2egmhs0RrDgUiThGwIx-RerrANnczK8)\n",
            "✅ Successfully uploaded 6_14_2008.csv (ID: 1wbCQ8ug6nxpv4hJormx0KpXUoYprh9ov)\n",
            "✅ Successfully uploaded 13_14_2008.csv (ID: 1dfk3-g26NMs0r_57SS4gU6EfDZA-YdIV)\n",
            "✅ Successfully uploaded 21_14_2008.csv (ID: 1kGKMJrEXZHYObpMfMpvnqyqpbZ6OdF-k)\n",
            "✅ Successfully uploaded 12_14_2008.csv (ID: 1sCY87F17OZe5F_MaESj62d72sExOn5xM)\n",
            "✅ Successfully uploaded 29_14_2008.csv (ID: 1Rr5dmejrqQMo_YTKDgFp6maFW5mXVRqI)\n",
            "✅ Successfully uploaded 33_14_2008.csv (ID: 1v2TB98GBJTucj3c_p5R-V1rPUHQpnQp6)\n",
            "✅ Successfully uploaded 35_14_2008.csv (ID: 1ogQ-IaEsd2Upc4kF9aLZGM0UuLxq4Itc)\n",
            "✅ Successfully uploaded 36_14_2008.csv (ID: 1BkykUY0PVqtmHd3kxS1xJ5FOKWFdiZ0q)\n",
            "✅ Successfully uploaded 6_06_2008.csv (ID: 1xZo7SPbP3qVl8CmCQOp15peZCrhI2RZO)\n",
            "✅ Successfully uploaded 1_06_2008.csv (ID: 18_HJzSyl3stOSsdxt3KPQ_RFc6xSFqmR)\n",
            "✅ Successfully uploaded 5_06_2008.csv (ID: 1xFGjczpr950O2uteysFE2EIdM27qBUya)\n",
            "✅ Successfully uploaded 4_06_2008.csv (ID: 14AXg_PooRga8NU8RLWw4Kq75IzttmSc9)\n",
            "✅ Successfully uploaded 7_06_2008.csv (ID: 1fLUxTPQpS2k4UvRV5GlnkKvJ1I3oLZ2v)\n",
            "✅ Successfully uploaded 2_06_2008.csv (ID: 12mWHrUHSd1VjeOFpPlmQXm7Hfqfd7WTN)\n",
            "✅ Successfully uploaded 3_06_2008.csv (ID: 1Jy-4co9Ozyu_D-AZMt68max2p7rd-z5h)\n",
            "✅ Successfully uploaded 12_02_2008.csv (ID: 19wI8gQYo4E6LencbvEW6kOwMkZ2_R31R)\n",
            "✅ Successfully uploaded 2_02_2008.csv (ID: 1iShFZh8hzt_GCg1LbLa4HugllWEFNNIr)\n",
            "✅ Successfully uploaded 14_02_2008.csv (ID: 12V_WpUDSvMVDtcw9Y6eHmk4kFlR_NuEd)\n",
            "✅ Successfully uploaded 13_02_2008.csv (ID: 1ouMSFXq237s_KH03o4hB21F6vwt6DCqj)\n",
            "✅ Successfully uploaded 10_02_2008.csv (ID: 11VhUzn1YoFvUyJ4JOhH5jNdJYFGTE2Ly)\n",
            "✅ Successfully uploaded 5_02_2008.csv (ID: 1g-zxJsS0fGIOaKcV9IvfdlrRiieVvhx4)\n",
            "✅ Successfully uploaded 4_02_2008.csv (ID: 1Pgw1idyQItYVp_HlGM3k2JXHk63BGc0O)\n",
            "✅ Successfully uploaded 11_02_2008.csv (ID: 1Neng6GaQ1-ymqZut3a62xR_nnLQPwMQ3)\n",
            "✅ Successfully uploaded 1_02_2008.csv (ID: 196Rq32toeIHYxB5URzhk7cL8jzYvK4TY)\n",
            "✅ Successfully uploaded 6_02_2008.csv (ID: 1Itssuca53ldCpVMzxpnNWY05ScH3RdGk)\n",
            "✅ Successfully uploaded 9_02_2008.csv (ID: 1EvQQUAXlJX1OTMbb_CbnkyFK2EVhlGhq)\n",
            "✅ Successfully uploaded 3_02_2008.csv (ID: 1wkeKz2glVRmDhYpTXWsTQz6hqOOknSwr)\n",
            "✅ Successfully uploaded 15_02_2008.csv (ID: 1IMD2Y1C41s20JVkKU0efYXNV-9eqY1lm)\n",
            "✅ Successfully uploaded 8_02_2008.csv (ID: 1dCdF3BhkW5kx6q7k81c9e-d3Ruilnjx9)\n",
            "✅ Successfully uploaded 7_02_2008.csv (ID: 112YjvS4HrLXX9bYjrj_mhXXhUH7C4rIR)\n",
            "📁 Created folder: 11 (ID: 1Qy_nLsCwswO_yjA6d2jMIjokA0uV0CYe)\n",
            "📁 Created folder: 04 (ID: 1UBDfwDfG21r9E4ahRUs-2tFD8eyz-AwO)\n",
            "📁 Created folder: 05 (ID: 1TWxBDtnDvt2_f024L63vAxfJ2U_tOr_D)\n",
            "📁 Created folder: 12 (ID: 1pV4mNdR8923lfu2QBriPt_ipEhEjJECW)\n",
            "📁 Created folder: 08 (ID: 1fG4jdA6EwYRYft4YPnpE2lce34QcsIeG)\n",
            "📁 Created folder: 09 (ID: 1uglq8qoWTIY99HrMtkIpqi4iYT70Clg6)\n",
            "📁 Created folder: 13 (ID: 1whZrAH9qRS_oSzDmU3nBNv0_rfJveSCc)\n",
            "📁 Created folder: 07 (ID: 1VmP4f-cWo5oFgLUUKVf8QTcYN7UOqylY)\n",
            "📁 Created folder: 03 (ID: 1RBUpVG0978Wn3MlBO7zwlqEhG1GRrwOn)\n",
            "📁 Created folder: 14 (ID: 1sF_XiXkdnf6AE65xE9pZzNaBGJxn5lPA)\n",
            "📁 Created folder: 06 (ID: 1utfEhwb0rX0qBXOA0w3t4WkteFilEhLM)\n",
            "📁 Created folder: 02 (ID: 1Ag3fhV1HgBHyQ8yjwt4ToesZ5eYcRuZG)\n",
            "✅ Successfully uploaded 3_11_2016.csv (ID: 1e1BBkUDHLNJOSIdPYhjDz8YfPBrFK7ks)\n",
            "✅ Successfully uploaded 5_11_2016.csv (ID: 1y4l7aWaOrU2YDW6rqurYNfOpUCG-RW3C)\n",
            "✅ Successfully uploaded 4_11_2016.csv (ID: 1ozaCY6JZYuElyeNaOtyBLUlNUD0EHMpj)\n",
            "✅ Successfully uploaded 1_11_2016.csv (ID: 1emo8qOY1Fo32iFTbXdv4vXzShFCZ0gLt)\n",
            "✅ Successfully uploaded 2_11_2016.csv (ID: 1dwgFWpeELN_Jq3rnSI8LyUGAPZ0VBn-o)\n",
            "✅ Successfully uploaded 6_11_2016.csv (ID: 1V1ARd6VCLf4TigS2sCj4eVx2Gs9-zUJx)\n",
            "✅ Successfully uploaded 7_04_2016.csv (ID: 1haTekEZo8eXYncR4CkD9MJ7O4cIeEW4p)\n",
            "✅ Successfully uploaded 10_04_2016.csv (ID: 1eCtCVswESHqJY8bHrkZuvFgWzFjY778s)\n",
            "✅ Successfully uploaded 6_04_2016.csv (ID: 1DaAe-ZAF6T2U0aLjt9fEKEkxAoU9LFn1)\n",
            "✅ Successfully uploaded 1_04_2016.csv (ID: 1rgV-z4SlbfgZl94rtJHPtNZmKy_pWuvz)\n",
            "✅ Successfully uploaded 3_04_2016.csv (ID: 1K7SQR-MBjtItJyq0sNL8rFEFIBUH_8TP)\n",
            "✅ Successfully uploaded 8_04_2016.csv (ID: 1Zrfy3NzKUpgsM7I6n6ayX-oHtIhkSAYF)\n",
            "✅ Successfully uploaded 9_04_2016.csv (ID: 1P5lvuny5LepnqP51RN1NLqmjnNyO_ylg)\n",
            "✅ Successfully uploaded 4_04_2016.csv (ID: 1qirBywx0GKp1uPfk6FiQA26vJ3HwvX3w)\n",
            "✅ Successfully uploaded 5_04_2016.csv (ID: 1hehkrNmgkGNm_T6OQSviabFKSFimJRNB)\n",
            "✅ Successfully uploaded 2_04_2016.csv (ID: 1R2Y45juz_rD1DYjQfxw7DTvWGnavY2NQ)\n",
            "✅ Successfully uploaded 14_05_2016.csv (ID: 1NPnaZdlRh8Pi2s0H6p9o1v5ZCJCgUdhr)\n",
            "✅ Successfully uploaded 17_05_2016.csv (ID: 1W_xOQh_O13QNn-KifbguKqFMi-jsmjND)\n",
            "✅ Successfully uploaded 48_05_2016.csv (ID: 1sToegM8MunVZye-V_nf4EweOeGCdeGm6)\n",
            "✅ Successfully uploaded 33_05_2016.csv (ID: 1VzaEOHuz3mOduaY17fLQUhLG7F3eLpW4)\n",
            "✅ Successfully uploaded 11_05_2016.csv (ID: 1qhRBokxXiQaP0Lq6nR4u-xmq0lPyOl1l)\n",
            "✅ Successfully uploaded 25_05_2016.csv (ID: 1qMW4QFeATEuvIUtMUF-JjjzREwWKgfRd)\n",
            "✅ Successfully uploaded 53_05_2016.csv (ID: 1247e20COcf5Ckb01brueHx59wnHepJ0C)\n",
            "✅ Successfully uploaded 35_05_2016.csv (ID: 1T-qgJXcbvaGDGW2wfNvuPMlhk4S3dRlM)\n",
            "✅ Successfully uploaded 5_05_2016.csv (ID: 1cb5djtxdkHJRznXqR50eBrx3lWADw1JV)\n",
            "✅ Successfully uploaded 60_05_2016.csv (ID: 1E97eHT9qwf9mgr934js-NUCXYBj2vtP4)\n",
            "✅ Successfully uploaded 10_05_2016.csv (ID: 1j1K03nn13IA3R6PAjOSfuHuCCCADCQSa)\n",
            "✅ Successfully uploaded 49_05_2016.csv (ID: 186cIS_IEEuLuGupkUTssi38n9hFmRg-s)\n",
            "✅ Successfully uploaded 63_05_2016.csv (ID: 1PXYc6C74xBxZvg6yKtsvQKNKTfdeGSax)\n",
            "✅ Successfully uploaded 58_05_2016.csv (ID: 1SN1ZRKZIFNn8l_KGMnsunkLk9t6ZAxzF)\n",
            "✅ Successfully uploaded 36_05_2016.csv (ID: 1NvUTm7gc0zJXsgiOT7nk6nMRbZjPdxa1)\n",
            "✅ Successfully uploaded 37_05_2016.csv (ID: 1Ojs3ywk915DSnLHBUaBYo3X5MjnbDJSX)\n",
            "✅ Successfully uploaded 32_05_2016.csv (ID: 1FzzDmCw2h3XygvcEx_O1nf69dp16mN1T)\n",
            "✅ Successfully uploaded 39_05_2016.csv (ID: 17LiosXrQgzSFDA70LW1uZTmxDY4kw19L)\n",
            "✅ Successfully uploaded 43_05_2016.csv (ID: 1VeV-_1Bf7ub7tvPQbLi684pBhkU_SzuR)\n",
            "✅ Successfully uploaded 28_05_2016.csv (ID: 1iBxM4JHt6w0rmVEJGE4Ub7OVkDE1GMgL)\n",
            "✅ Successfully uploaded 8_05_2016.csv (ID: 1BAWvekrNQ_4rlaQyeB1hMYZe2xkqRttV)\n",
            "✅ Successfully uploaded 22_05_2016.csv (ID: 1g4rfV35at_OXIUZCkFqY1XQc8dtZRAL6)\n",
            "✅ Successfully uploaded 26_05_2016.csv (ID: 1MfmInuAn-ou_H4S8M4jSJiSyhrOj_vav)\n",
            "✅ Successfully uploaded 57_05_2016.csv (ID: 1Ylb4dO3uhpS0wok0ejLEefPpTxBy3RIh)\n",
            "✅ Successfully uploaded 59_05_2016.csv (ID: 1AONCgMZCiUOgIz7wEnqYRQmvonQG9El5)\n",
            "✅ Successfully uploaded 29_05_2016.csv (ID: 1xkSIbvRIdHbXC1CMQcxqWybYp7CPfdfy)\n",
            "✅ Successfully uploaded 20_05_2016.csv (ID: 1QF_DTg22kJSNLrm8ngF-RzfPiYhTDVjX)\n",
            "✅ Successfully uploaded 40_05_2016.csv (ID: 1NLM7OInCHPttmlFUr-ZHfDEsa0bRpufM)\n",
            "✅ Successfully uploaded 7_05_2016.csv (ID: 1gYjhjmAfvDTQDZHCpBKuR6G67SN7oOSt)\n",
            "✅ Successfully uploaded 31_05_2016.csv (ID: 1zsfhvKC6n7FDa3NYPA2dNuZu9ZOBB8Hl)\n",
            "✅ Successfully uploaded 1_05_2016.csv (ID: 1nI2vfhik_4XLaOelYdVMXycKViGPqqhB)\n",
            "✅ Successfully uploaded 18_05_2016.csv (ID: 1EnqSxlB3gBcDfirYwlXt2E6JwHP7ZtSj)\n",
            "✅ Successfully uploaded 2_05_2016.csv (ID: 16hxuEdhgfaOm2KzBIsodp0cox_yiVJYp)\n",
            "✅ Successfully uploaded 54_05_2016.csv (ID: 139YVJSaWNOkQ_JBEgvTR9t3qbyLjwt4I)\n",
            "✅ Successfully uploaded 67_05_2016.csv (ID: 1377zDgIFmxxVRA1SCzyxDh5J6jfy-kcL)\n",
            "✅ Successfully uploaded 15_05_2016.csv (ID: 1KBMEGkHCVaOg-xEkuSGFBeiKWprgiRMS)\n",
            "✅ Successfully uploaded 34_05_2016.csv (ID: 1rC1KVUZ3SUWoubyFsPWUN3FpTPrGZPmN)\n",
            "✅ Successfully uploaded 44_05_2016.csv (ID: 1ioN4r6AK5OtxdkndBUno0rPChRkv9v24)\n",
            "✅ Successfully uploaded 38_05_2016.csv (ID: 1kBaGyyygCCZglRszbj3H3zf06_Udidwm)\n",
            "✅ Successfully uploaded 13_05_2016.csv (ID: 1rGn-jUzIX4I6izHyO6KtaTCOxOGrhFZm)\n",
            "✅ Successfully uploaded 24_05_2016.csv (ID: 1eVFVvxeUmPmM4pgcOsSGDfWozejOQB9B)\n",
            "✅ Successfully uploaded 50_05_2016.csv (ID: 1FElltQvgiQkf95wtuuL0RO3bb4UXW_je)\n",
            "✅ Successfully uploaded 64_05_2016.csv (ID: 1u8zbHAjaezPAUfvUlTk_7dW5BJVHh8HX)\n",
            "✅ Successfully uploaded 9_05_2016.csv (ID: 1WKkI_oGuJdI1g0N_5EjbpL2zq2X86xbM)\n",
            "✅ Successfully uploaded 41_05_2016.csv (ID: 1wgQ_5dZbS2d9YM2gEZkv3zze_x8fI8kh)\n",
            "✅ Successfully uploaded 6_05_2016.csv (ID: 15gC8I0N8h-3sSbXHB2LiHtzp0RxnWgBK)\n",
            "✅ Successfully uploaded 3_05_2016.csv (ID: 1P9BXVr74_DGXtnZ-uhWt9bdbIF31E3Pv)\n",
            "✅ Successfully uploaded 27_05_2016.csv (ID: 1OyWy1i8NAqjrjHmB-yHLZgp8KKntyuS-)\n",
            "✅ Successfully uploaded 19_05_2016.csv (ID: 1P0-sa_dCyoWI7AhYr_mgvYbJNRXOK2_W)\n",
            "✅ Successfully uploaded 47_05_2016.csv (ID: 1_-mONd0aroyInciGuJoyoljYTg9bHCWz)\n",
            "✅ Successfully uploaded 61_05_2016.csv (ID: 1mo5Ggta2Kdb4yrwlC0s4zcWl34YxiVuo)\n",
            "✅ Successfully uploaded 16_05_2016.csv (ID: 1UTYNHGWsqLDvVjJRIGWD4YeRFd8uWC9d)\n",
            "✅ Successfully uploaded 4_05_2016.csv (ID: 1Sqq0o9lXldjTw6V979hQWeCVac_TnmNE)\n",
            "✅ Successfully uploaded 52_05_2016.csv (ID: 1KyXANCOvU2rlOGUWXeInVFUJKIRLFVOK)\n",
            "✅ Successfully uploaded 56_05_2016.csv (ID: 1thRybxIt0eYCyqt-HOBYVW7l0zg_5ImF)\n",
            "✅ Successfully uploaded 62_05_2016.csv (ID: 106miHP43sk6v6sRGFkpwx-SfGj5yDJ_r)\n",
            "✅ Successfully uploaded 45_05_2016.csv (ID: 10ZoWS16i7HPqBdlxbdA4VnrM3uE99Ome)\n",
            "✅ Successfully uploaded 51_05_2016.csv (ID: 1JEvzdVkAb-F9gD9Ae5xc8TQM1jbix-2g)\n",
            "✅ Successfully uploaded 66_05_2016.csv (ID: 1r_nJptRyjuqa9QevMP_Ak109TUX83qDU)\n",
            "✅ Successfully uploaded 65_05_2016.csv (ID: 1laV2GdfbCfrRylQpx-ObgtqiH4oamhp8)\n",
            "✅ Successfully uploaded 46_05_2016.csv (ID: 1D-Dl0obkdyCGE1ZepdljD-5mbiL2N1es)\n",
            "✅ Successfully uploaded 12_05_2016.csv (ID: 1mryHWAb-EeEZBkVF3m_aLxFAxU9l8XMg)\n",
            "✅ Successfully uploaded 55_05_2016.csv (ID: 1s-rh572dVA_fN8w7cnuaU-6XBszgKLpX)\n",
            "✅ Successfully uploaded 23_05_2016.csv (ID: 1qUgdHMElgZ99NY5cRp1BbV86mNZyvOhW)\n",
            "✅ Successfully uploaded 30_05_2016.csv (ID: 1Cv4ylpZRBnFhT9eeXuXiVD-QKcBViKaV)\n",
            "✅ Successfully uploaded 42_05_2016.csv (ID: 1VcyaqY7LbQ11Ip4BrDnvKyIaRac3M3yp)\n",
            "✅ Successfully uploaded 21_05_2016.csv (ID: 1A4-Slh6QdvIHIaCJyyOGZVbrvXUfiqG7)\n",
            "✅ Successfully uploaded 34_12_2016.csv (ID: 1GHAE92tGBdnADlg0fz8uk_1H0INJXjWq)\n",
            "✅ Successfully uploaded 17_12_2016.csv (ID: 1_jO9FtBOSAuPhpl-iQpS1xOdJLOAK4Yc)\n",
            "✅ Successfully uploaded 37_12_2016.csv (ID: 1EcHqKluPiAjoW59mgcvbRvAf28GCeF_X)\n",
            "✅ Successfully uploaded 20_12_2016.csv (ID: 1GfjR8mJx8Jg42rIWwHBr6fXzLXq6_bnv)\n",
            "✅ Successfully uploaded 45_12_2016.csv (ID: 1vKGUQgVy4im7D3KhOW0pn3o0f-FE73TD)\n",
            "✅ Successfully uploaded 18_12_2016.csv (ID: 1N884A-lYhwIiY8BHHp3VN_5OVEfexxQD)\n",
            "✅ Successfully uploaded 46_12_2016.csv (ID: 1ARvn7f5416ojq5WoRx9h-8d6hxH0fr4f)\n",
            "✅ Successfully uploaded 15_12_2016.csv (ID: 1G8QjyjM8AOeA7nGip-3TTnPpxtd0v6zs)\n",
            "✅ Successfully uploaded 11_12_2016.csv (ID: 1dS7beitoUcrtoSElBJ2drGWGah0d8ddM)\n",
            "✅ Successfully uploaded 30_12_2016.csv (ID: 1mzo7_aaCtcOiJwTMX06GhNxJMzhTtfy1)\n",
            "✅ Successfully uploaded 51_12_2016.csv (ID: 17FoBcxgnTktI_buKXOj6FMErcGhkZlLT)\n",
            "✅ Successfully uploaded 2_12_2016.csv (ID: 1cyJ6IRbaGaC0rH00w0U3sSM1n7MYwY7l)\n",
            "✅ Successfully uploaded 33_12_2016.csv (ID: 1V6S-9vvC7ZbN5eooG_quqtJIEHqoGDob)\n",
            "✅ Successfully uploaded 36_12_2016.csv (ID: 116i7De910cm6M0h8mJT_074JilYINCK9)\n",
            "✅ Successfully uploaded 26_12_2016.csv (ID: 1Flt5G1JbdiGKd_2EeUrKNHhSjzn8kNbC)\n",
            "✅ Successfully uploaded 6_12_2016.csv (ID: 1me39uC06Hwdie9p7mDcXbNhyP6GNvhx8)\n",
            "✅ Successfully uploaded 24_12_2016.csv (ID: 1ll7CJweDbOkDvQZBwu-DXILnBJQ9Ud4h)\n",
            "✅ Successfully uploaded 10_12_2016.csv (ID: 13INLvGtTsgug9z5gI5bFXc8YVS9Ik2K_)\n",
            "✅ Successfully uploaded 48_12_2016.csv (ID: 1b7lhbFxrbLXdRTHdQ0A90ql9vCYDxw6y)\n",
            "✅ Successfully uploaded 19_12_2016.csv (ID: 1DvO8XM1iIMO_OEGVlpDQ3ZvWufUSK3TN)\n",
            "✅ Successfully uploaded 28_12_2016.csv (ID: 11JXAafMYV_CDngWTBcM_4i1W2ju-4UZs)\n",
            "✅ Successfully uploaded 43_12_2016.csv (ID: 1UmzxP-BOHgZdEQwCg4aB5Wce_mMAidO6)\n",
            "✅ Successfully uploaded 16_12_2016.csv (ID: 1R4XDejmj2NG-NS9wqBFIBwuovfyh0XdA)\n",
            "✅ Successfully uploaded 38_12_2016.csv (ID: 1oDgC-r0FOHtOHuHOBFIh54vtIrVIs3Ga)\n",
            "✅ Successfully uploaded 22_12_2016.csv (ID: 1OMkMATYAFraghYnCJmw9fuh0hi0mYuv5)\n",
            "✅ Successfully uploaded 47_12_2016.csv (ID: 17M9-ArY9CnY0R93e3cXmVbQrSQ5IxdrZ)\n",
            "✅ Successfully uploaded 1_12_2016.csv (ID: 1kNKpbbJCbxYx1IYMIoICf-eZUQey2srE)\n",
            "✅ Successfully uploaded 52_12_2016.csv (ID: 14h2vkOkLQPxFirg2k7YMPzKbM5R6pCSs)\n",
            "✅ Successfully uploaded 32_12_2016.csv (ID: 15MA0lzACr0L4ICHC4YVqbPQUtReoQWn_)\n",
            "✅ Successfully uploaded 5_12_2016.csv (ID: 1SytYZAxIEsebCULeRVVezRN8FyMnPQI_)\n",
            "✅ Successfully uploaded 13_12_2016.csv (ID: 1oZdufg3zdKRMYgFN261xuci3iBeQv1NP)\n",
            "✅ Successfully uploaded 40_12_2016.csv (ID: 1Xqqh-egDqN-97sLary52yGQXH0WkWNEN)\n",
            "✅ Successfully uploaded 49_12_2016.csv (ID: 1ExNzlTz696z5fy4JerihcsbP6SrtkKRW)\n",
            "✅ Successfully uploaded 31_12_2016.csv (ID: 11LyI6Taih6m1OcwtT2ABvpnPOc_oEwuK)\n",
            "✅ Successfully uploaded 8_12_2016.csv (ID: 1jIVNyJDAO2BaM-bCsOCVfmwtjEkJLcvD)\n",
            "✅ Successfully uploaded 42_12_2016.csv (ID: 1onivo9M0sw7xwavGto6qEJ6JkrFHP7DK)\n",
            "✅ Successfully uploaded 39_12_2016.csv (ID: 1cdjSm4fglvKYEh8svFuwkx61150roDHn)\n",
            "✅ Successfully uploaded 14_12_2016.csv (ID: 1lyMebPC1yXlbzfWyMDH_wp_yt8W7_dTL)\n",
            "✅ Successfully uploaded 9_12_2016.csv (ID: 1NuLSMeHr4V1e2UtnKGLsY3q_wMhLerLF)\n",
            "✅ Successfully uploaded 25_12_2016.csv (ID: 1Wr5o6pCH5YfKjxkW8mD-qZqVW3BcXh9c)\n",
            "✅ Successfully uploaded 29_12_2016.csv (ID: 1DPnzsADLMXJojF-r8xl9_T9JfJJlvljo)\n",
            "✅ Successfully uploaded 44_12_2016.csv (ID: 17ZXFe-RJW1muTGhVjwmVVYWNzYKSdsMA)\n",
            "✅ Successfully uploaded 27_12_2016.csv (ID: 1KatbJyqx9tmLEEpwpE1E7DL73svVQ_mb)\n",
            "✅ Successfully uploaded 35_12_2016.csv (ID: 1VheKnN-3RUJZSM2hdG7DG8OLskRMFXtN)\n",
            "✅ Successfully uploaded 50_12_2016.csv (ID: 1YpdEQ_F66Tq58JDPl__0MVHh0zkqu0eC)\n",
            "✅ Successfully uploaded 23_12_2016.csv (ID: 11n1-TyLATmc_Mw0-w3n7Et72yg0DWIS6)\n",
            "✅ Successfully uploaded 3_12_2016.csv (ID: 1C_Cx5qeY97K0S-cGduj-F7tAQ24jMvsN)\n",
            "✅ Successfully uploaded 4_12_2016.csv (ID: 1T6w37FOa1PoqwilZGpLyMEYFkNtQg-ol)\n",
            "✅ Successfully uploaded 12_12_2016.csv (ID: 1UD7MHt3hslVD0rda4av4quZ6py1rvUQ0)\n",
            "✅ Successfully uploaded 7_12_2016.csv (ID: 1zwzEl_wm1fM2XBGjR4PXDMRcHSTafbdf)\n",
            "✅ Successfully uploaded 21_12_2016.csv (ID: 1QCv9PcBDfQKsLNp3lGiwOoK6wvZWvy3o)\n",
            "✅ Successfully uploaded 41_12_2016.csv (ID: 1lYo3Fx4ow-nEnbh9bo-O_EErCr-5KVez)\n",
            "✅ Successfully uploaded 10_08_2016.csv (ID: 1lRMTRvoL2xv5ouhgfZba_H2Y1tlUMbWK)\n",
            "✅ Successfully uploaded 27_08_2016.csv (ID: 14Wl-ZLQrHuyxKJPrlmjCbUaPrj4-AtWB)\n",
            "✅ Successfully uploaded 43_08_2016.csv (ID: 1RB_xStJXY-yeSoBe87U0jtTZivkTnlCt)\n",
            "✅ Successfully uploaded 28_08_2016.csv (ID: 1NLyc-aHx028amsM8HSWi9WdjoLCwbXoM)\n",
            "✅ Successfully uploaded 20_08_2016.csv (ID: 1B6txftuHRtNRoKD4tAD9TQP18vRPVNhp)\n",
            "✅ Successfully uploaded 13_08_2016.csv (ID: 1LEN44LkfGcGr2OdAs23gnswvfrtkh7Zl)\n",
            "✅ Successfully uploaded 40_08_2016.csv (ID: 11NwTgT7VsswufereHxMUFrRLFNwy4rnf)\n",
            "✅ Successfully uploaded 38_08_2016.csv (ID: 1kLre848A9CGIh-riTfDa5rN-ECkIjFLr)\n",
            "✅ Successfully uploaded 21_08_2016.csv (ID: 1oRY8-YRiVnKd_YC29meLk-k9FInGQTjs)\n",
            "✅ Successfully uploaded 17_08_2016.csv (ID: 1NrH0PPBTztnc1e-EQiYrkbm8vJBGImIj)\n",
            "✅ Successfully uploaded 26_08_2016.csv (ID: 190NP4MqSPdU_oH5JQRhpgCqoMOtd6hk-)\n",
            "✅ Successfully uploaded 9_08_2016.csv (ID: 1jiXc-9boGHcc5pffhIpv-nv1kkipE86u)\n",
            "✅ Successfully uploaded 30_08_2016.csv (ID: 1vW9llNHSCZFcJQOqqwHeh4Rl4okpNQMX)\n",
            "✅ Successfully uploaded 37_08_2016.csv (ID: 14HTzGRVpBWhRNFksAC2u6qkfi7Ctg8qQ)\n",
            "✅ Successfully uploaded 32_08_2016.csv (ID: 124NzZSZosUg4jWZTSMiNCOCePP_O9PLe)\n",
            "✅ Successfully uploaded 25_08_2016.csv (ID: 1LbKMcXIejiFQhhlEXaIOJoASfLl0ffPf)\n",
            "✅ Successfully uploaded 42_08_2016.csv (ID: 1uffWl5PwVWCv83AX0d0IE3y31U7hV_0-)\n",
            "✅ Successfully uploaded 31_08_2016.csv (ID: 1LtVVRLqwZNs1KAjY2pOidKG1xiCWRjxk)\n",
            "✅ Successfully uploaded 3_08_2016.csv (ID: 1Nc3EP7oOvwFvL9KKZeoswKvdCfGc5WyR)\n",
            "✅ Successfully uploaded 18_08_2016.csv (ID: 1XlNyX-oDi97UAfRKw2oH_PAZuLz4Cq6u)\n",
            "✅ Successfully uploaded 36_08_2016.csv (ID: 1YsiMwnHjpTL42cM97BDXOR5c5HhWrGU3)\n",
            "✅ Successfully uploaded 22_08_2016.csv (ID: 1eQnRjfFK8oD1YRlNJ3y0Uw89w5SjwJEO)\n",
            "✅ Successfully uploaded 15_08_2016.csv (ID: 1HUIDU5YrOafeyF9kwBuNNSNH2aCeoLlD)\n",
            "✅ Successfully uploaded 19_08_2016.csv (ID: 1WXMmCsaKfOTWca7Nl6-sc2QhvZgXyc_4)\n",
            "✅ Successfully uploaded 33_08_2016.csv (ID: 1MOhVMCj6Q1LHcdEWsQWyRcd3anKp36XF)\n",
            "✅ Successfully uploaded 14_08_2016.csv (ID: 1DSxrsuuJ5TCiepDASBvCNxpf0AJkLIYM)\n",
            "✅ Successfully uploaded 44_08_2016.csv (ID: 1tJxVj6mdt67PEPd_q9uYnIP4gXIJlI6y)\n",
            "✅ Successfully uploaded 39_08_2016.csv (ID: 1kVfOfJWFiWqKdEn4Xs3oFOsaELniYyp5)\n",
            "✅ Successfully uploaded 2_08_2016.csv (ID: 1OUOYWQtSvonD72mVqq8WFTz_-RSMzdcH)\n",
            "✅ Successfully uploaded 34_08_2016.csv (ID: 16M8WU07D2E2mOwIWhiha7qAwLnBcsAeA)\n",
            "✅ Successfully uploaded 24_08_2016.csv (ID: 1uNt_uP91Oe3II4HX4sroKWqwd0yZYtjU)\n",
            "✅ Successfully uploaded 23_08_2016.csv (ID: 1WAccTeI5AlK-WQcG5MS-Rn7mWVsTedWU)\n",
            "✅ Successfully uploaded 6_08_2016.csv (ID: 1lYBtpVKx_0lNTxAjBHlPX13YQMBo9I1g)\n",
            "✅ Successfully uploaded 8_08_2016.csv (ID: 1HUPCjKZZRprd1098aWhKqJUyycZbzYuH)\n",
            "✅ Successfully uploaded 41_08_2016.csv (ID: 1eckt5r_qSaUCIuxI9r9l3Y_NwEUG73P8)\n",
            "✅ Successfully uploaded 11_08_2016.csv (ID: 1ZA0yWBhoCoZtPO0WktljdQ6s3G9grVmr)\n",
            "✅ Successfully uploaded 5_08_2016.csv (ID: 1jnm16ypEzBZQ_v4BSED7lDOd6JdvKGVW)\n",
            "✅ Successfully uploaded 12_08_2016.csv (ID: 11Lpt9ROn2QXRyfSD-Myh6Tdqo6Oga8lo)\n",
            "✅ Successfully uploaded 29_08_2016.csv (ID: 1KW4BQ2_FuFW6IJc_zrEaJYq2jy-r3id2)\n",
            "✅ Successfully uploaded 4_08_2016.csv (ID: 1je_77nmHsUOeWNtAI9-EA-HZPfmwSF9W)\n",
            "✅ Successfully uploaded 35_08_2016.csv (ID: 16JAhTMbthjXbcqKnMoiNRV1GjHZHDtkQ)\n",
            "✅ Successfully uploaded 16_08_2016.csv (ID: 1Yz7dSlzqYqooy9Iwh1jj6Zz9tZSkMFKA)\n",
            "✅ Successfully uploaded 1_08_2016.csv (ID: 18wdNup7Fc131M_vCyFEheuS3lBQF1imk)\n",
            "✅ Successfully uploaded 7_08_2016.csv (ID: 1PCGy-vsqb9SxdqGa4L5M-F8IzpfPu8c4)\n",
            "✅ Successfully uploaded 18_09_2016.csv (ID: 1FytjpAmVmuap9elDdVULriY5yV7yvo-m)\n",
            "✅ Successfully uploaded 12_09_2016.csv (ID: 1QgQnyz_wvsDeIxnPnnCqRuwVP4I-td0d)\n",
            "✅ Successfully uploaded 16_09_2016.csv (ID: 1-eePAluVpe2R-DTs4ByrI1cJad08EeWr)\n",
            "✅ Successfully uploaded 14_09_2016.csv (ID: 1NxTGW53gD0NaYzwiajsKnROhxo-8qAYM)\n",
            "✅ Successfully uploaded 17_09_2016.csv (ID: 1EHwiL2GFzYAeW46lQo11T4P454Dn7B1Y)\n",
            "✅ Successfully uploaded 5_09_2016.csv (ID: 1qDRLS9URgXxo6yIjqr4qgiIGk-qc3HHI)\n",
            "✅ Successfully uploaded 4_09_2016.csv (ID: 1D8gpcTyGk6DJJJg-6WOiHN4bCyiPF9rd)\n",
            "✅ Successfully uploaded 10_09_2016.csv (ID: 1YRpk_zvuLOCUNyMVJJabwiTFzpWUxk77)\n",
            "✅ Successfully uploaded 2_09_2016.csv (ID: 1RlJMEwMUkq9qmkymf8sL2NcFSQWVB2Y-)\n",
            "✅ Successfully uploaded 13_09_2016.csv (ID: 1ElVjzL4HoSst2iDwfkTkcOuk3LOulC5p)\n",
            "✅ Successfully uploaded 20_09_2016.csv (ID: 1DLHDSS8zDB6_p_kH4wPNjU0hEjbuGwId)\n",
            "✅ Successfully uploaded 7_09_2016.csv (ID: 1uDaN1BYh3z5mZ7qszCtp6gU_YqQGJByj)\n",
            "✅ Successfully uploaded 6_09_2016.csv (ID: 11FcDkLo3dfi-vzP-_S6H0y48Xibn01MI)\n",
            "✅ Successfully uploaded 9_09_2016.csv (ID: 1FaWv0BP0WWRFjgn0KCYqIXv8SeN0BvUL)\n",
            "✅ Successfully uploaded 3_09_2016.csv (ID: 1ZpuUw6A7Nb2Q0sdjl_Iz7N1bzBaNb28t)\n",
            "✅ Successfully uploaded 11_09_2016.csv (ID: 1Qhyysvouhhar25Kuyi9eNk0ZtBdWr_pL)\n",
            "✅ Successfully uploaded 15_09_2016.csv (ID: 1TZVDHMnRZNVArk7neeUXfuYH3MmZV4aj)\n",
            "✅ Successfully uploaded 1_09_2016.csv (ID: 1BH27dmYHsuwhDgGEsvZFhvQJuIy9Zj98)\n",
            "✅ Successfully uploaded 21_09_2016.csv (ID: 1FKcjysreLEO4CoxIo0cEshoIBHBZV7U7)\n",
            "✅ Successfully uploaded 22_09_2016.csv (ID: 1h7p_jrTXEAH81h9f43TCP9v3irAk-dE9)\n",
            "✅ Successfully uploaded 19_09_2016.csv (ID: 1ySxndLQKxi7URuXW3W3TMOg6G9X4ftat)\n",
            "✅ Successfully uploaded 8_09_2016.csv (ID: 1U66SDTdyOas_kZGPqPwlWpPvMU-e-Vgq)\n",
            "✅ Successfully uploaded 29_13_2016.csv (ID: 1Jiiu5FjlMc7fzzm4aQFCA1-PoKvZJ8xP)\n",
            "✅ Successfully uploaded 8_13_2016.csv (ID: 1ew0SNWLpPbeuInxBcP_6Cg2oKFzLt7Fu)\n",
            "✅ Successfully uploaded 21_13_2016.csv (ID: 1wWXfe3-VnYfxeo0pg-0QJUwV3YNdbEOE)\n",
            "✅ Successfully uploaded 32_13_2016.csv (ID: 1WqFU106bHnYtPBATS7bZ9X4vuOTGLRjb)\n",
            "✅ Successfully uploaded 20_13_2016.csv (ID: 1qJi6jGBOEdGAhP4Fhz2sTZAZnKBu9kLv)\n",
            "✅ Successfully uploaded 25_13_2016.csv (ID: 1Bz1TkLOtXObHpdGZNsh6jzbeM0dMLMVh)\n",
            "✅ Successfully uploaded 19_13_2016.csv (ID: 12BmzZApYJKKEIz3fGGnOxQUJipi7r0Os)\n",
            "✅ Successfully uploaded 36_13_2016.csv (ID: 11CA7X-a050PCE4deD1-m3AB2Af9CVnEP)\n",
            "✅ Successfully uploaded 2_13_2016.csv (ID: 1w2k8LJOWxA5l2crVlzbwnKuBd1Rno_pc)\n",
            "✅ Successfully uploaded 17_13_2016.csv (ID: 1IauqSDvh7WUQxCu1grtix5wjKxWNIXyi)\n",
            "✅ Successfully uploaded 11_13_2016.csv (ID: 1oVXUsineDfOdfcKKx_6hTBOP6LGltSh-)\n",
            "✅ Successfully uploaded 9_13_2016.csv (ID: 1YMx_qha4pKzkVpmKi6xNphBhhDs7pFcz)\n",
            "✅ Successfully uploaded 18_13_2016.csv (ID: 1zZALxAfo3jRNLdCzexdEI4SUl1GnVS-2)\n",
            "✅ Successfully uploaded 40_13_2016.csv (ID: 1SHW9aCLqoaRmMNvEQxAtYZjtABnpxe4Y)\n",
            "✅ Successfully uploaded 13_13_2016.csv (ID: 1zxjbwsK29w3iqe-AJUSInprvLhWF22Kl)\n",
            "✅ Successfully uploaded 27_13_2016.csv (ID: 1SuicVQw0J--V88puO8RWNrJqtbkyKI1X)\n",
            "✅ Successfully uploaded 34_13_2016.csv (ID: 1aPP0N--ykqQCNRcxltdeBVY1tFu0TqIt)\n",
            "✅ Successfully uploaded 3_13_2016.csv (ID: 1r3h_Um9ppirZZacJ_g3ac37ane5TqffP)\n",
            "✅ Successfully uploaded 35_13_2016.csv (ID: 1Ol_9UWgZNdu_xAOcNNm5fTU28BPM7KMf)\n",
            "✅ Successfully uploaded 16_13_2016.csv (ID: 1z0DA1NToYQMp5uhJFd8NOFFDtMDdCNGu)\n",
            "✅ Successfully uploaded 23_13_2016.csv (ID: 1yghFnG4GFatSxSgIXnZzdVDNjfhzb9dd)\n",
            "✅ Successfully uploaded 15_13_2016.csv (ID: 1x2MY0HSBvztfSOWcxqcnHEvJoX4YTHPq)\n",
            "✅ Successfully uploaded 14_13_2016.csv (ID: 1VYbZzABNrYMdV2yJRi3jdbvbEDfMSCV3)\n",
            "✅ Successfully uploaded 22_13_2016.csv (ID: 1_Cua-SzD5RmiswjKkEh0nqv2JBCEpBaP)\n",
            "✅ Successfully uploaded 4_13_2016.csv (ID: 1vth0lawRuSruWt0Ku7vcYD-TLu5sUh2Y)\n",
            "✅ Successfully uploaded 33_13_2016.csv (ID: 13SM1frCX6f5kS2JwDK9t5bHx6J08x8Lq)\n",
            "✅ Successfully uploaded 39_13_2016.csv (ID: 1ZFRmbgEhbztJohQdbA4RUd3yYgP6sbiU)\n",
            "✅ Successfully uploaded 7_13_2016.csv (ID: 1EddBEq1I_u2AlFnV03oUw3NR0aaCSrKl)\n",
            "✅ Successfully uploaded 30_13_2016.csv (ID: 1gLObOTKaPdmm9LKefL6EsPo3aHdqKjsI)\n",
            "✅ Successfully uploaded 24_13_2016.csv (ID: 1BzkcId8i27D2k1uaOt1RjrpcUAQl5cCb)\n",
            "✅ Successfully uploaded 1_13_2016.csv (ID: 1CiJie0pRwW2DSwelj7AUiaDSv336yZ4N)\n",
            "✅ Successfully uploaded 37_13_2016.csv (ID: 1OhX62ele_h_KhTugNXBVeocDT5gcjQIO)\n",
            "✅ Successfully uploaded 28_13_2016.csv (ID: 1S2XXnQWf3YPfBn1UwminljGjIM6ciU0f)\n",
            "✅ Successfully uploaded 26_13_2016.csv (ID: 1zo9ZtyAlyNaSsdoSui8Xz3vVeBQ1aAfL)\n",
            "✅ Successfully uploaded 6_13_2016.csv (ID: 1MGY59AW-SG4Yr170u_cR5ymZPIvvE1I4)\n",
            "✅ Successfully uploaded 5_13_2016.csv (ID: 1wYz05yla4orPoe62zawIrIAMmanTFlLR)\n",
            "✅ Successfully uploaded 12_13_2016.csv (ID: 1UoJ67Zp_IvPHq3v3zXq3rdecg82rrvl2)\n",
            "✅ Successfully uploaded 38_13_2016.csv (ID: 1UpaTHjpdbi-s8rGlQ3KrS6jfsIYalFy0)\n",
            "✅ Successfully uploaded 10_13_2016.csv (ID: 1N1hXSaJ01Frnb0JKKtyb9RVtvr5od4oO)\n",
            "✅ Successfully uploaded 31_13_2016.csv (ID: 16B3nFMh3OisZKDdE-1gdg1wQCVE7oDtl)\n",
            "✅ Successfully uploaded 31_07_2016.csv (ID: 1dGND6lnSfLEyWFakawFX94wpwBu0gFdU)\n",
            "✅ Successfully uploaded 18_07_2016.csv (ID: 1hJnhnfWlV7Wu6Xfg99O5jmxoQBvdIL6T)\n",
            "✅ Successfully uploaded 16_07_2016.csv (ID: 1bl0vX74fXX2MIslohyXKD4s3ohudksVU)\n",
            "✅ Successfully uploaded 12_07_2016.csv (ID: 1wzTfJ58FV98e4nb4uT6Ijzi2c_6Uf0n7)\n",
            "✅ Successfully uploaded 1_07_2016.csv (ID: 1aHVADh_AhTMRJOel63lIQ7yzsmnI2RNN)\n",
            "✅ Successfully uploaded 29_07_2016.csv (ID: 1jIqatJdA5J8KemMvmjsrdn1FJleOnuEy)\n",
            "✅ Successfully uploaded 10_07_2016.csv (ID: 1zXaBwzOylPthBfPHzvtr1AauMQs_OQkJ)\n",
            "✅ Successfully uploaded 6_07_2016.csv (ID: 1sLqnDS9BQuBZnEo_zhrHPLhhivVXzvCO)\n",
            "✅ Successfully uploaded 23_07_2016.csv (ID: 1O6mFRF5Eo7nPFUxvwp6ezqeOmmGcMHGc)\n",
            "✅ Successfully uploaded 17_07_2016.csv (ID: 1KGvStkNXOy_Lwwm7Ksnk6TBNggQW13jk)\n",
            "✅ Successfully uploaded 22_07_2016.csv (ID: 15hdD5n2s-qom_wxJdkd2xjdNFkbynoJZ)\n",
            "✅ Successfully uploaded 3_07_2016.csv (ID: 1tfqzPEVkx7MMoxeD_qi20-Q6R4UcgdxU)\n",
            "✅ Successfully uploaded 2_07_2016.csv (ID: 19yV4L9hL7PWza-B66_zF3qzzxIGqGSGp)\n",
            "✅ Successfully uploaded 33_07_2016.csv (ID: 15nNFsLQxLGRDUcpyFLKfNrJacK4groJs)\n",
            "✅ Successfully uploaded 21_07_2016.csv (ID: 1nuNnTSG4XEKm4teNq-XzjFUGIwItSDCQ)\n",
            "✅ Successfully uploaded 8_07_2016.csv (ID: 1_aIGdAQeGDXgthS829ZeZMVs_1rUIGeC)\n",
            "✅ Successfully uploaded 24_07_2016.csv (ID: 11XZdBUAN3ZrNUYOn0a-5ELXsp4F4xBlA)\n",
            "✅ Successfully uploaded 11_07_2016.csv (ID: 1s4uchVcuYde0dWw7gz4h26r16smVcqaU)\n",
            "✅ Successfully uploaded 4_07_2016.csv (ID: 1bgfx8mWYIuRvBwuVwUi5gvLFvLr5PTF5)\n",
            "✅ Successfully uploaded 25_07_2016.csv (ID: 1AFYbsFvMHYkNPMgHigiYSdbGlU2gYNda)\n",
            "✅ Successfully uploaded 32_07_2016.csv (ID: 1lsfNU1DdtoAUNVdXucKYkv1BcboUmObU)\n",
            "✅ Successfully uploaded 19_07_2016.csv (ID: 1CmjfZ29cfKSds078iL1rSXRpmHdGLVyD)\n",
            "✅ Successfully uploaded 30_07_2016.csv (ID: 1KBOoconfaGJMvx-yxesGs97iHYDmgzHn)\n",
            "✅ Successfully uploaded 9_07_2016.csv (ID: 1VelBNO8mbPMkIU09e534HDyqXOKGmgNH)\n",
            "✅ Successfully uploaded 15_07_2016.csv (ID: 1BBO7qJRHPJe9w_DMAW8l9PsmEUXMCErv)\n",
            "✅ Successfully uploaded 26_07_2016.csv (ID: 11gvWThbLQvk5qArV2r5O_jPMd8_IZvxX)\n",
            "✅ Successfully uploaded 28_07_2016.csv (ID: 1KijWBL6u7cP4Mh2IItzK3e1SYcQKZ4vJ)\n",
            "✅ Successfully uploaded 14_07_2016.csv (ID: 1NB5KRC9Zvq4goMBPen7n5ENhTGW5yOyG)\n",
            "✅ Successfully uploaded 20_07_2016.csv (ID: 1yztjptqL82of9X3naRfSDXsde8cAMdO6)\n",
            "✅ Successfully uploaded 7_07_2016.csv (ID: 1PnIYquWomg3lHUb5B759M_JwyuhhBd-j)\n",
            "✅ Successfully uploaded 13_07_2016.csv (ID: 1JFnUh0MsQpZO93BbgadLjDtL3EWgQRtx)\n",
            "✅ Successfully uploaded 5_07_2016.csv (ID: 1KDWnJbiBzgwZ-AnXGRHX_BZHXDIGJSe1)\n",
            "✅ Successfully uploaded 27_07_2016.csv (ID: 1UbRYx0wbDcXDXO5OZV4TebM0zaDwhkGr)\n",
            "✅ Successfully uploaded 19_03_2016.csv (ID: 1AU6toI6OljKSKh6R8bWqlwz1TxvIioR6)\n",
            "✅ Successfully uploaded 22_03_2016.csv (ID: 1YukHcSQ_z8ctfvTybOFNlsoiDadnxzxf)\n",
            "✅ Successfully uploaded 18_03_2016.csv (ID: 1OErFNfxArgFKp2WEr5fdO3jKNg1XZluV)\n",
            "✅ Successfully uploaded 1_03_2016.csv (ID: 1U0JUtoZ9ubnk9UnZfC2eRnHCacS2XNpm)\n",
            "✅ Successfully uploaded 6_03_2016.csv (ID: 1KROlzVZmC8HaqVAcv2uGyz0BlIjBnuje)\n",
            "✅ Successfully uploaded 11_03_2016.csv (ID: 13fRx2JVoo5Av46CcVuUogfITWyVYqnKl)\n",
            "✅ Successfully uploaded 21_03_2016.csv (ID: 11Naw4xQDr-ADT9dRqUTBxQ1ukaryhNIy)\n",
            "✅ Successfully uploaded 8_03_2016.csv (ID: 1sfSWR6_uCieV2NOI36C3N7qf3IWGD_UF)\n",
            "✅ Successfully uploaded 14_03_2016.csv (ID: 1eAY53VcYkIUNJldzkvbF4-MsVEkeWnUd)\n",
            "✅ Successfully uploaded 9_03_2016.csv (ID: 1D8dJXUdLZchcVTLdh0dF35BgactgANum)\n",
            "✅ Successfully uploaded 2_03_2016.csv (ID: 1XYPTTEC4Ko0p2py9gws_oMiw5smYKNx9)\n",
            "✅ Successfully uploaded 7_03_2016.csv (ID: 1HK4KU7y2Jjg_6NwMFXwt-lPebBa3SH2m)\n",
            "✅ Successfully uploaded 20_03_2016.csv (ID: 1dgjNHxOx5k6idHEjJwLS4srwjnyTqUJw)\n",
            "✅ Successfully uploaded 3_03_2016.csv (ID: 1CibOMMLp8uZmZYMcfsDdGZ5gk4EOT9fn)\n",
            "✅ Successfully uploaded 16_03_2016.csv (ID: 1e5PUU_FnkcsiXuXAVe0jeXhu5lIHRoAf)\n",
            "✅ Successfully uploaded 13_03_2016.csv (ID: 1XB4vA4xf1ipDCiBQ5tE17WsF-4dFceQu)\n",
            "✅ Successfully uploaded 17_03_2016.csv (ID: 1WRiJw5oGTvLgib4ItJ3f4zGvsjurOufn)\n",
            "✅ Successfully uploaded 4_03_2016.csv (ID: 1YOCmpxkko2qKCJwUDhAf7LMKBZSWKJl3)\n",
            "✅ Successfully uploaded 10_03_2016.csv (ID: 1FWY5adWnzjXu0vtKvJYWKzgcUHXRGRjZ)\n",
            "✅ Successfully uploaded 15_03_2016.csv (ID: 1ubro3FheomRhoSUrSSPpGz_mAIkzD1zw)\n",
            "✅ Successfully uploaded 5_03_2016.csv (ID: 1GA38Mb7S50Q-p2N4Oz95LOXGpUiTGL05)\n",
            "✅ Successfully uploaded 12_03_2016.csv (ID: 1TemhloAqiZ4WkDn1pbQ1jIhYlDdpoj9E)\n",
            "✅ Successfully uploaded 11_14_2016.csv (ID: 1ux4sYwE-qJaljkd45NIATNtvrzxB32Yu)\n",
            "✅ Successfully uploaded 41_14_2016.csv (ID: 1YGcddMVNkCuGpvRRP494KpUXWcIgdeOU)\n",
            "✅ Successfully uploaded 22_14_2016.csv (ID: 1uplC5y3SFtjtEQVuy8VoXC1pTuS5qTdg)\n",
            "✅ Successfully uploaded 32_14_2016.csv (ID: 18T9muo28Wm5qVStdZ1Xb0w5ny92Ell6U)\n",
            "✅ Successfully uploaded 19_14_2016.csv (ID: 16U-e04VAJVh0dxCMmMCLfAHCFlPwNQzQ)\n",
            "✅ Successfully uploaded 26_14_2016.csv (ID: 1px64U4-SvSAdBO-cXLt0Z8-t5CBVGO0T)\n",
            "✅ Successfully uploaded 24_14_2016.csv (ID: 18pnsebs-wuWwX998h43g8_zzkwOOIbmv)\n",
            "✅ Successfully uploaded 10_14_2016.csv (ID: 1fYWMTNTpi5JBGGCxX60Fp5Ltc5x9FJhw)\n",
            "✅ Successfully uploaded 12_14_2016.csv (ID: 1-44zB6BvgrZEyrZbPJlMZAAys2gMx9m5)\n",
            "✅ Successfully uploaded 5_14_2016.csv (ID: 1ygmaPMe0CN2k_SKi8xu-sOlhICVL2Jj6)\n",
            "✅ Successfully uploaded 38_14_2016.csv (ID: 1kns5RIn_R8sbBAk9qkHNhNiVU4UThuMd)\n",
            "✅ Successfully uploaded 37_14_2016.csv (ID: 1RBZQy0eDUEodgmahPHqlunNww-owlZFY)\n",
            "✅ Successfully uploaded 23_14_2016.csv (ID: 1Tx77SXf9XkAZfQXtcEvaQfV52-uSH0--)\n",
            "✅ Successfully uploaded 7_14_2016.csv (ID: 15R91mtVnOsjQ5S8WmSI0qf-FB-Z0Rz1L)\n",
            "✅ Successfully uploaded 30_14_2016.csv (ID: 1GVBRyfVOenVLsolT4whokhZuoFYtIbUt)\n",
            "✅ Successfully uploaded 39_14_2016.csv (ID: 1_vVO8t2DeXMqu2e9VNlA-9-mRdqyuEMY)\n",
            "✅ Successfully uploaded 36_14_2016.csv (ID: 1uYa8F1_h2KL3q2C_YVAj1OTyfxNG6Twy)\n",
            "✅ Successfully uploaded 33_14_2016.csv (ID: 1F_VyN-UInQHq8ndm5GtCTxkNrPyG5lld)\n",
            "✅ Successfully uploaded 43_14_2016.csv (ID: 1yX6rfEwzlN7S2azK0xaIQ3PsLQPD6c3d)\n",
            "✅ Successfully uploaded 6_14_2016.csv (ID: 1AcgqSsUCjUuUUOB1PhR688h4tJxIMFlF)\n",
            "✅ Successfully uploaded 3_14_2016.csv (ID: 1De3DMUZCvV65v9A8-ZPkpSWJv1Uad6gc)\n",
            "✅ Successfully uploaded 35_14_2016.csv (ID: 1XRE-TS1Vu3up1GtgGUEHY4DQv_8SupZq)\n",
            "✅ Successfully uploaded 44_14_2016.csv (ID: 1Q5k8fYR84kAxbc7lwRhbIf4D0ekT0K18)\n",
            "✅ Successfully uploaded 42_14_2016.csv (ID: 1ejLZNCaHG-sRp5A1v4Xv5tETrxymeVVD)\n",
            "✅ Successfully uploaded 21_14_2016.csv (ID: 1pXuUBwrGdhGZSYjBBfiiDv9t8L6U1W2p)\n",
            "✅ Successfully uploaded 17_14_2016.csv (ID: 1ys4J16GTaEHqTptF5etQKB0L6WbVHHzX)\n",
            "✅ Successfully uploaded 31_14_2016.csv (ID: 1CuODrfl6sYUUI73kHzbFR5XZ2V5GjYm8)\n",
            "✅ Successfully uploaded 25_14_2016.csv (ID: 1zZy0xCdR3b6DBr3M_5tvqqQPRQG3U5Bb)\n",
            "✅ Successfully uploaded 9_14_2016.csv (ID: 14-moxjzrEzhdnAWNcBnNKFSrmsk6-i92)\n",
            "✅ Successfully uploaded 18_14_2016.csv (ID: 12PocbMK6nz_AlHkf936ylGEX85TERZzQ)\n",
            "✅ Successfully uploaded 13_14_2016.csv (ID: 1xhwUeNZzR9jSDceBq2k37ka7n4huE1d5)\n",
            "✅ Successfully uploaded 40_14_2016.csv (ID: 1QFpS9yJJVV3fhcFEAfqOKibf4AQKM8kL)\n",
            "✅ Successfully uploaded 20_14_2016.csv (ID: 1qs8cfHh0mQbeCGsx8QR8pnHVK2HTqXMU)\n",
            "✅ Successfully uploaded 4_14_2016.csv (ID: 1tWlogG4g-ffU__-KFRYwebLoCt26X3xn)\n",
            "✅ Successfully uploaded 15_14_2016.csv (ID: 1Pm4M8jCbRnMjAE2VqjrzlXp-G-Gah_eq)\n",
            "✅ Successfully uploaded 8_14_2016.csv (ID: 1ayIuYaApJtgZ9D5br6lm7T8kuGC-PMXC)\n",
            "✅ Successfully uploaded 14_14_2016.csv (ID: 1akBi-ivigNFrdh7bNSPWecvw7E13TNlS)\n",
            "✅ Successfully uploaded 45_14_2016.csv (ID: 1W4E7wpFN5IyylhD_06kwOrHrc1QkbnOY)\n",
            "✅ Successfully uploaded 1_14_2016.csv (ID: 1CUHel8r4Z6-7P1gTuFyOYv-u-umVQP1_)\n",
            "✅ Successfully uploaded 2_14_2016.csv (ID: 11LkjpCsOfzmimWVAEsgaetNAmNopF2Tr)\n",
            "✅ Successfully uploaded 28_14_2016.csv (ID: 1EKfsl-W8QZbKwYReHi7l6uvUyZGeMZmT)\n",
            "✅ Successfully uploaded 16_14_2016.csv (ID: 1nmbKIvKdcqlW-Eh8P-DDHhCS2dWalYth)\n",
            "✅ Successfully uploaded 27_14_2016.csv (ID: 1lzxvom1zGGf377a3nd3X4mqkRO6yfVXz)\n",
            "✅ Successfully uploaded 29_14_2016.csv (ID: 1q2CyMjv-mOk95UiGk8PUhovb6AI8eVwD)\n",
            "✅ Successfully uploaded 34_14_2016.csv (ID: 1uRy9YxQ-Zo4T88KRrq9L-6BJE8KdN8eb)\n",
            "✅ Successfully uploaded 8_06_2016.csv (ID: 1jVAye6Ro1f98KcKmZKWHNWEP4RWaUi3g)\n",
            "✅ Successfully uploaded 9_06_2016.csv (ID: 1_lPfPw2cNW2p9b00UlNLwXMAgvmn1aEx)\n",
            "✅ Successfully uploaded 10_06_2016.csv (ID: 1InFNgXvD4XJ4VwN5x7xhzNRwkcSLTeQo)\n",
            "✅ Successfully uploaded 4_06_2016.csv (ID: 1vUQU2IexHATecJcXS6MS9DNLLiaMrrav)\n",
            "✅ Successfully uploaded 17_06_2016.csv (ID: 1sTcCXkW1ZrYZFRN4WrzaGKDBu1WBMrir)\n",
            "✅ Successfully uploaded 13_06_2016.csv (ID: 1LTDked8I8pThsh4X2Nc24AFs8dZKRfnl)\n",
            "✅ Successfully uploaded 11_06_2016.csv (ID: 1Kgu8g1iL--e1nk-ZOjJkW6z0G9BnnbpS)\n",
            "✅ Successfully uploaded 1_06_2016.csv (ID: 1E-_bni3XhksLnvh98VFdXFhVKLVM3RS8)\n",
            "✅ Successfully uploaded 14_06_2016.csv (ID: 1oFMkcQW3X2xQLtR1lmsHsOYYQGyXxeVd)\n",
            "✅ Successfully uploaded 2_06_2016.csv (ID: 16e50Uc-x0ZDD61HCPV-fempdPnim1yFg)\n",
            "✅ Successfully uploaded 5_06_2016.csv (ID: 19309titjSPx8_FCE7NZIoQkaPCPlj6PM)\n",
            "✅ Successfully uploaded 15_06_2016.csv (ID: 14e8MncZ5cdSUzWuLFEN6eafeRodF-SZn)\n",
            "✅ Successfully uploaded 12_06_2016.csv (ID: 1dZwVhGsUfgo6pkSOFcMUyw60qelafR3d)\n",
            "✅ Successfully uploaded 16_06_2016.csv (ID: 1TqrYS4fZ6bOwTnOQ054e9Q4s_4FWOF3E)\n",
            "✅ Successfully uploaded 6_06_2016.csv (ID: 1zlq3sHzSsLA3yg8NH1z3ernz-iD55315)\n",
            "✅ Successfully uploaded 3_06_2016.csv (ID: 1SF11NSmbgnE0JOPqNFiz-e8DmgoepRtb)\n",
            "✅ Successfully uploaded 18_06_2016.csv (ID: 149l06n_6BEwRlqFgwrGvgPjburPhruAp)\n",
            "✅ Successfully uploaded 7_06_2016.csv (ID: 1jgIGcaxWNqL7q0o7N06PlpXHLfG06Cc7)\n",
            "✅ Successfully uploaded 4_02_2016.csv (ID: 1rrZmXQ0yl6H77YaFaRyPRpxUpnqgndao)\n",
            "✅ Successfully uploaded 11_02_2016.csv (ID: 1_UBfbIMo-J8qiOwNouDMBFnbeiw2uItK)\n",
            "✅ Successfully uploaded 16_02_2016.csv (ID: 1uxKoGZJal3_Q54B4_-Xg3C3VER6cmLBX)\n",
            "✅ Successfully uploaded 13_02_2016.csv (ID: 1vWwEfJrA0tbR8InWB2jt1l5JulupDSRf)\n",
            "✅ Successfully uploaded 2_02_2016.csv (ID: 1vjXVgAZugz79e_nyZm5SX6KdAK_xxsiU)\n",
            "✅ Successfully uploaded 5_02_2016.csv (ID: 1Y26PLE4LIqSIH3mjcvEyyb-Wt4NZaqSX)\n",
            "✅ Successfully uploaded 10_02_2016.csv (ID: 1p9yQgayCSxOPFDdzO8PsoGPnKVepVdkB)\n",
            "✅ Successfully uploaded 7_02_2016.csv (ID: 145dSQadcxWe3sjxfbZmyoEDdIySt5mJ-)\n",
            "✅ Successfully uploaded 15_02_2016.csv (ID: 19eBT65U9I2PnR73ZIfTMPQ7oo3iMW1ME)\n",
            "✅ Successfully uploaded 17_02_2016.csv (ID: 1dME7OGOkCk3jXtpfHUswJLzPZ4jbXhqB)\n",
            "✅ Successfully uploaded 6_02_2016.csv (ID: 1fUiD3yGIx9ZF2ClkS-zuo-pibmBP0rj_)\n",
            "✅ Successfully uploaded 14_02_2016.csv (ID: 14d8M3QOSv2mWCreUIyaOWLVOCqVP65WW)\n",
            "✅ Successfully uploaded 3_02_2016.csv (ID: 11tw-vd8XL2OUCKzDJnn2mg2ZrdpNl5gj)\n",
            "✅ Successfully uploaded 9_02_2016.csv (ID: 1WI4fvNzo6y1vK2h_bGEZShC7oGTHZkGK)\n",
            "✅ Successfully uploaded 12_02_2016.csv (ID: 1DxY3myWaY5FSqrN_qCw7UXDFJjLXn9f0)\n",
            "✅ Successfully uploaded 1_02_2016.csv (ID: 1NG5AH2XML8n6j-B4RxucaGZVzUD-pjX9)\n",
            "✅ Successfully uploaded 8_02_2016.csv (ID: 1K5mGP77Z9-Ba8FNa9l91ajzaik4Aeqeo)\n",
            "📁 Created folder: 11 (ID: 1EUxcS0v_pKXYRx4fohUWRhs0WRcRTHjp)\n",
            "📁 Created folder: 04 (ID: 1sik0ZCRyQGtiDQJMUprgSUz4rNHwZNj9)\n",
            "📁 Created folder: 05 (ID: 1_KOniMobc_HjyR1ZO6mkwGZf2nkYeIU0)\n",
            "📁 Created folder: 12 (ID: 1GWAjuKhG1GngTaXjqmhxoAmSDKkdwOTJ)\n",
            "📁 Created folder: 01 (ID: 1EW82UB84lxQvqll3LqKg92JMAi9HxpCA)\n",
            "📁 Created folder: 10 (ID: 1iybwoBsdCiGJWQlArZt_zUoB9VB__qZP)\n",
            "📁 Created folder: 08 (ID: 1iMwWA0dfl4V4Yov_8Z1fi8-QH7gMHNO8)\n",
            "📁 Created folder: 09 (ID: 1IVVIFuKTcphAcMl4iatlt-Ivp2IbdCWs)\n",
            "📁 Created folder: 15 (ID: 1-E-I--bKXPwojXRXVTO9dGV-FI1JLd7Y)\n",
            "📁 Created folder: 13 (ID: 1_O_KLpVTCc-eNycI0c3hTWKOoxAyoiqr)\n",
            "📁 Created folder: 07 (ID: 1xp0re45Fe9O2PLYAogxeMBqJHQURzqjL)\n",
            "📁 Created folder: 03 (ID: 1fSTQpgxNDU92fdshCjZj4k7N5pDRcffT)\n",
            "📁 Created folder: 14 (ID: 1EuogjZ3zjoapseDK6ZCZi5OfFeDao2fV)\n",
            "📁 Created folder: 06 (ID: 1sbcWomhzPFk0D8Y8dpDYkcbqK1lUwSxv)\n",
            "📁 Created folder: 02 (ID: 19IxbLW_CvEI2BlB9O9wTXjE-r1GGV0rK)\n",
            "✅ Successfully uploaded 3_11_2011.csv (ID: 1heXjSrh3B-Qgvt6i9NmCfW5wAiTUJuJV)\n",
            "✅ Successfully uploaded 2_11_2011.csv (ID: 1sUjcBxTaSe6eH598NDL7uCwCzjjYRTOw)\n",
            "✅ Successfully uploaded 1_11_2011.csv (ID: 1QQ3Oxcj-_1S-0MyDksleEQmqrU42SPue)\n",
            "✅ Successfully uploaded 5_11_2011.csv (ID: 1Ojn0RBm4BxnD07d6F_bh1XFdZ-tCKr2o)\n",
            "✅ Successfully uploaded 4_11_2011.csv (ID: 1WYM8IB5ksJyB8F8F332BxSc9leZbWXhe)\n",
            "✅ Successfully uploaded 1_04_2011.csv (ID: 1XNcZv-skM68MhVNAimMWXTMiT1iRGPE5)\n",
            "✅ Successfully uploaded 5_04_2011.csv (ID: 1MUgORyccsbhjjX1cPYJ-V67aHqfgbC0g)\n",
            "✅ Successfully uploaded 9_04_2011.csv (ID: 1LLqFBMWFFegM_z42k_vEtdt5hdiIGQjk)\n",
            "✅ Successfully uploaded 4_04_2011.csv (ID: 1HyVqUz8yjc2oGC4WS9lGsOARGHWfcp00)\n",
            "✅ Successfully uploaded 8_04_2011.csv (ID: 1g49zqOwF4Qos2yD7tT19tPyOle20wIEC)\n",
            "✅ Successfully uploaded 6_04_2011.csv (ID: 15nLrxp_fSRQNP7OMGxuelomXaai2x4hH)\n",
            "✅ Successfully uploaded 12_04_2011.csv (ID: 1v80IiATVlNwKK07iebF7S3Wtqxre_MoV)\n",
            "✅ Successfully uploaded 2_04_2011.csv (ID: 1iEMe2U-2n94a7oy-H774lUGKsEXKlgqI)\n",
            "✅ Successfully uploaded 13_04_2011.csv (ID: 1kyN6zk31lpwEjkzjhQHpTTduEzYfG47N)\n",
            "✅ Successfully uploaded 7_04_2011.csv (ID: 1pe0XccZG-BmcivSObsuKGjE-ooaOKkrl)\n",
            "✅ Successfully uploaded 3_04_2011.csv (ID: 1yGie3ZztzgkKCZSX0cSop4uqbASUiuWW)\n",
            "✅ Successfully uploaded 11_04_2011.csv (ID: 1r8g_ODqN_EYXdEX6IQ00q_V5zE_qZJZW)\n",
            "✅ Successfully uploaded 10_04_2011.csv (ID: 1VsghD3sWRKFtlct-KbFR_H5zUMAf59vT)\n",
            "✅ Successfully uploaded 33_05_2011.csv (ID: 1LMVBScHj4j_TCD8V8KXH3Lrqyo5D0_Uu)\n",
            "✅ Successfully uploaded 9_05_2011.csv (ID: 1z_WpZFmVto4d5ziNVjSxE9xvl7IL96CA)\n",
            "✅ Successfully uploaded 23_05_2011.csv (ID: 12AE_8vosESJ7Jp0NdkXDs8hwsX80JWmZ)\n",
            "✅ Successfully uploaded 43_05_2011.csv (ID: 1ks1zjzJ1bjQIYD_WM2ue94GeRpF34DAF)\n",
            "✅ Successfully uploaded 24_05_2011.csv (ID: 1jtwWUvpOpWAmF_WF5f8p_CBVo94h8LfM)\n",
            "✅ Successfully uploaded 2_05_2011.csv (ID: 1yF5AfTrwvB39Y7Mq6zFC4-kH-_B8IAQp)\n",
            "✅ Successfully uploaded 5_05_2011.csv (ID: 1qqPsbGJJejlS2P1IaWLZPuKMwIj_V4Mk)\n",
            "✅ Successfully uploaded 20_05_2011.csv (ID: 1SQo_8_Oo3qRhRiFwsLiK2xuIIVrzA1Zs)\n",
            "✅ Successfully uploaded 41_05_2011.csv (ID: 1WO15HZmaDY58YA0Kwrpeu45T-eyRNJlu)\n",
            "✅ Successfully uploaded 10_05_2011.csv (ID: 1TGH__s8VvQd23LJS9_At_iX0csOh9bvR)\n",
            "✅ Successfully uploaded 25_05_2011.csv (ID: 1znCpTxYEyo0NVhZPNnzeuAmWyAJVrK64)\n",
            "✅ Successfully uploaded 1_05_2011.csv (ID: 1GBgbFym-Pp33hwjkaqMIOQKl8L8yD2hB)\n",
            "✅ Successfully uploaded 40_05_2011.csv (ID: 16jkh1a9QdTLt97LQUsbkdfJculHGn4Us)\n",
            "✅ Successfully uploaded 26_05_2011.csv (ID: 1Y2ZhDj9o0dMwc1MRWn7x6oqdUZ-Sx6-U)\n",
            "✅ Successfully uploaded 13_05_2011.csv (ID: 1QGyv9QEOFnusX8mt9auTjpY68udWUliv)\n",
            "✅ Successfully uploaded 7_05_2011.csv (ID: 1icipzcmQF4vc8KHMNg-aaHI223jrnByO)\n",
            "✅ Successfully uploaded 32_05_2011.csv (ID: 1YidWbDEz5cZ3vmzrZ95ivaMtpTqUIc5L)\n",
            "✅ Successfully uploaded 16_05_2011.csv (ID: 10Z8IFutMmZyw5LpOV5RICZPQ_Z6V2Lby)\n",
            "✅ Successfully uploaded 8_05_2011.csv (ID: 1DloFx7nj-IhJt_WXFoic-8s1LEStyWa7)\n",
            "✅ Successfully uploaded 28_05_2011.csv (ID: 1yYrTIW6pMYfaYFJ8-QCb9hkpY7l3Lo8e)\n",
            "✅ Successfully uploaded 35_05_2011.csv (ID: 10qkWOOIJ_J4mwpacVJNR66PVKS5k4KXk)\n",
            "✅ Successfully uploaded 6_05_2011.csv (ID: 1hqS8yCLWhJk6An5dSMqNBKj6HohLPCS5)\n",
            "✅ Successfully uploaded 12_05_2011.csv (ID: 1ewnU3NkNZynL37x9Q2P7G5MGEpij5st3)\n",
            "✅ Successfully uploaded 18_05_2011.csv (ID: 1-rQGZsdLVcNulgO_zEyTI3ZCWqh5WTUo)\n",
            "✅ Successfully uploaded 31_05_2011.csv (ID: 1ukb3KKbQAAOJ8Wx7X4AWf2FyUkrbEUUY)\n",
            "✅ Successfully uploaded 15_05_2011.csv (ID: 1NE2dU80d8gkGSILTFaQsjWBYXvPRUnA-)\n",
            "✅ Successfully uploaded 37_05_2011.csv (ID: 12SIH2k6onYjf_q1A_ERpd13O2HozB-Ga)\n",
            "✅ Successfully uploaded 4_05_2011.csv (ID: 1-VbIohVtSKxOH5emg8c7Zsutz7StKo8v)\n",
            "✅ Successfully uploaded 21_05_2011.csv (ID: 1W564JzBObkllA-xCnW3enWVgiXWdIyoz)\n",
            "✅ Successfully uploaded 30_05_2011.csv (ID: 1j0HojbsBbjIl3Xi_vXO79FdPKqB8tFTl)\n",
            "✅ Successfully uploaded 34_05_2011.csv (ID: 1JHrLbIRI2O5N9BZGTIKAngtrCFkNaCH-)\n",
            "✅ Successfully uploaded 45_05_2011.csv (ID: 16pLAehGPGRexZrhgm8V-tsBL5KmVry7e)\n",
            "✅ Successfully uploaded 36_05_2011.csv (ID: 1Fdeilbjg1yOYZaSaynlpDzFVxi-F5z96)\n",
            "✅ Successfully uploaded 42_05_2011.csv (ID: 1ugMRaehHNsHFQFekKG8MGANy00c4nxz2)\n",
            "✅ Successfully uploaded 17_05_2011.csv (ID: 1Vizst77bpiKqPzbnbLAbB32JdbMqM-41)\n",
            "✅ Successfully uploaded 14_05_2011.csv (ID: 1UYZAMd_G2Xz7zqY6rtJKyu6fWmm6o9UX)\n",
            "✅ Successfully uploaded 11_05_2011.csv (ID: 1Jqdq95rjraiBDAnjZZs9nA-JOWVZX-yv)\n",
            "✅ Successfully uploaded 29_05_2011.csv (ID: 1TF1mSP7HJuVb1251VT2798Mmw73I7kbo)\n",
            "✅ Successfully uploaded 44_05_2011.csv (ID: 16QbBvX0biHYGsIGWPwIETsAH4bAoo0EH)\n",
            "✅ Successfully uploaded 19_05_2011.csv (ID: 1N1A-TcLQ1X_PceBt7-VSgLi-6dYPYgXU)\n",
            "✅ Successfully uploaded 27_05_2011.csv (ID: 1_tgCj0ssHLl-tCuVP9IiPaQjHMU3KRzq)\n",
            "✅ Successfully uploaded 39_05_2011.csv (ID: 1oYvFv2J5pzTBA6OeckHft3vMB1VZOses)\n",
            "✅ Successfully uploaded 38_05_2011.csv (ID: 1XmZYqmhN3JYCAvy_OLZlJdVR9hnFRgdP)\n",
            "✅ Successfully uploaded 22_05_2011.csv (ID: 1tu31CPe5J4__5rvW4mzHGG4LFcUq8_sP)\n",
            "✅ Successfully uploaded 46_05_2011.csv (ID: 1cjfpM8uxl9xiYhBRQ_QrWUCxvpPzkzJ_)\n",
            "✅ Successfully uploaded 3_05_2011.csv (ID: 132lAOhpGtLacDV4sIxWt2a-H6ntp5XZX)\n",
            "✅ Successfully uploaded 14_12_2011.csv (ID: 1eWe4F_-aqiob3V5pYnXwcnUoJH9WmAJL)\n",
            "✅ Successfully uploaded 38_12_2011.csv (ID: 1vpjvvAMGpObnxOWiKK-Appwl2D9uVFzg)\n",
            "✅ Successfully uploaded 32_12_2011.csv (ID: 10Qru40qvarAIjAx60R4ZUsl4cjfxIJRW)\n",
            "✅ Successfully uploaded 31_12_2011.csv (ID: 1u8mucMVldH12-iHx6sXcj6J9j_P9X338)\n",
            "✅ Successfully uploaded 35_12_2011.csv (ID: 11iBe2UrqtKL1aRcRXBfkHMRCyAbINKs7)\n",
            "✅ Successfully uploaded 22_12_2011.csv (ID: 1NMF21YyhLghLteKaxKMY-4U1lvD6v1X-)\n",
            "✅ Successfully uploaded 28_12_2011.csv (ID: 1U-DJ08vLf3clD2IZdMoKe1JpIqxqQXRc)\n",
            "✅ Successfully uploaded 16_12_2011.csv (ID: 1L0c6VdZsgH5LxY32lCR3f9v6I1f12wq2)\n",
            "✅ Successfully uploaded 10_12_2011.csv (ID: 1tZGVp2nEkSwmkXCoeJfsrpvOJfHD0nV6)\n",
            "✅ Successfully uploaded 25_12_2011.csv (ID: 18AoA_ZkWbDZ1jxsiOh1LlCL4e7R2jl5X)\n",
            "✅ Successfully uploaded 6_12_2011.csv (ID: 1Er7O9aWa6n8o7QGDiQO_EjuwzTKvRen_)\n",
            "✅ Successfully uploaded 9_12_2011.csv (ID: 1nKrGb1mk0UOGg8yzeElurENIwRHts2AO)\n",
            "✅ Successfully uploaded 24_12_2011.csv (ID: 1VRzS7uzEwObO5LEPaIw69pQIjZz1KSI-)\n",
            "✅ Successfully uploaded 45_12_2011.csv (ID: 1r6CcIm_6jBS1ZdKJ4smu4n-XJ8yr-Jy3)\n",
            "✅ Successfully uploaded 7_12_2011.csv (ID: 1h9P55UI6hQP-lc2c7Y7hg474a9h32-ry)\n",
            "✅ Successfully uploaded 50_12_2011.csv (ID: 1svCgXikJ3ktVlV-KkawhyyJdC999qgdD)\n",
            "✅ Successfully uploaded 15_12_2011.csv (ID: 1GHywkmOgTeLttnSMfktH3uTeakFeDgc_)\n",
            "✅ Successfully uploaded 21_12_2011.csv (ID: 1OMLHEkC6rVlRVz08_ilyCm6M_M8w5Wh7)\n",
            "✅ Successfully uploaded 37_12_2011.csv (ID: 1mLWfw6W8IOeauJYva9GW8dDV5Mn43pTb)\n",
            "✅ Successfully uploaded 43_12_2011.csv (ID: 1XvILGfk_dkyU-6XAAaQsPwAu04fj7tDM)\n",
            "✅ Successfully uploaded 49_12_2011.csv (ID: 1gvvIh22lkuxiUj1m4pZ06MOOzWCi7AHj)\n",
            "✅ Successfully uploaded 41_12_2011.csv (ID: 18FYKxnMGUmdDtQ4BEVS_470VJDuqz_0r)\n",
            "✅ Successfully uploaded 47_12_2011.csv (ID: 1JMcVxd3HiaDwkRfNU8nyd5ZpHo8hKHI-)\n",
            "✅ Successfully uploaded 5_12_2011.csv (ID: 1HI76jSrKTrtTcNPnUknjBXQvx77PSiRu)\n",
            "✅ Successfully uploaded 2_12_2011.csv (ID: 1C88FWu1ACTB7JV9aErrNz0pBebptNdTE)\n",
            "✅ Successfully uploaded 20_12_2011.csv (ID: 1Bzivbm3stLo5J6YY9SrA_gW_I7jn4Flq)\n",
            "✅ Successfully uploaded 33_12_2011.csv (ID: 1ASHmVcE8aLNq-EgG0A2JZcECDO9IH4RN)\n",
            "✅ Successfully uploaded 29_12_2011.csv (ID: 1_bFwH61KAKEWM8atrkXJAe3L3AGqCV5H)\n",
            "✅ Successfully uploaded 30_12_2011.csv (ID: 1JrypV4PR95TUv9h02wJVrjPsfg6siB27)\n",
            "✅ Successfully uploaded 26_12_2011.csv (ID: 1GRg-ZI-RZ-i1vC-yXoR12MeWy5-zi3kx)\n",
            "✅ Successfully uploaded 42_12_2011.csv (ID: 1668g-QC_GUJyF5eul7vudMU2Bo80bhlQ)\n",
            "✅ Successfully uploaded 19_12_2011.csv (ID: 1KBeyGNro2ozQj0GKN3W45TzjDgnyJ3Uh)\n",
            "✅ Successfully uploaded 23_12_2011.csv (ID: 1V7qIGz5kOn3_xCezSI7tyXtTYbqzx4Bu)\n",
            "✅ Successfully uploaded 17_12_2011.csv (ID: 19BqaOvA8a_8P2hWW15bbI6JH00x2ds3Q)\n",
            "✅ Successfully uploaded 3_12_2011.csv (ID: 1HPlkn1ezy-8MbEP7qlQjPku9cq3oxywN)\n",
            "✅ Successfully uploaded 13_12_2011.csv (ID: 1FbZ2MsWmEDF7fdWfMNcQ5kCJq2xx050-)\n",
            "✅ Successfully uploaded 27_12_2011.csv (ID: 1dhPP_oeR-rC1wS8ZnW3evBpyXSkWNxvu)\n",
            "✅ Successfully uploaded 34_12_2011.csv (ID: 1OwDFta6m5_KumxrgVBKtpWoU4dfB3BpM)\n",
            "✅ Successfully uploaded 11_12_2011.csv (ID: 1gAfV-qYn_hlVTgZUjxYzFquLkvglHE5P)\n",
            "✅ Successfully uploaded 8_12_2011.csv (ID: 14sELP7qhDpGQRmV3mIz7MIUxY37VpNJU)\n",
            "✅ Successfully uploaded 39_12_2011.csv (ID: 1bC1ZGKzHSmYGcnZ1UCV1Z32YHD80u9d2)\n",
            "✅ Successfully uploaded 1_12_2011.csv (ID: 1n7yilWxiFzQItVM02H-ONoAwDwIrnmqf)\n",
            "✅ Successfully uploaded 12_12_2011.csv (ID: 1ybmXNlPBhEX947OHDazGN6wB0vuUoj61)\n",
            "✅ Successfully uploaded 46_12_2011.csv (ID: 1nZqn9QzFP5VnINsww-czPBdi7tvYSmPb)\n",
            "✅ Successfully uploaded 36_12_2011.csv (ID: 1XEovmvqVdhT_A2LSV7ZWj8fYYOp2wG1t)\n",
            "✅ Successfully uploaded 18_12_2011.csv (ID: 15sv-JlI1datRT0bRq56HXFxFsZvKPdp5)\n",
            "✅ Successfully uploaded 4_12_2011.csv (ID: 1kxBdLFFjI8KCIjHzR2642NsDHFRsKE3a)\n",
            "✅ Successfully uploaded 48_12_2011.csv (ID: 1PGRU3rku6359nLN79SinngTb-IkWpoEI)\n",
            "✅ Successfully uploaded 40_12_2011.csv (ID: 1hRbT7-QAU6s_jnvhmBCbSDOZhgKDmxzL)\n",
            "✅ Successfully uploaded 44_12_2011.csv (ID: 1L3M1lw2QxprSDD4soZiblDRuS4_3t0W9)\n",
            "✅ Successfully uploaded 9_01_2011.csv (ID: 1p2nhj3dmIICDRek-b83H3KpeHo3G2iZp)\n",
            "✅ Successfully uploaded 4_01_2011.csv (ID: 1SiJYWaPloR-F_uwVH6RQ0XTScoFj_aKV)\n",
            "✅ Successfully uploaded 6_01_2011.csv (ID: 1SFbD0mlrNtaX9ZkqDra2Vbkep06lk-y6)\n",
            "✅ Successfully uploaded 12_01_2011.csv (ID: 1Pesi4S5ncq_vAKIuCMIGAY4N5bH_vMZH)\n",
            "✅ Successfully uploaded 1_01_2011.csv (ID: 1l-SdlSymCyeWRpuJ8s6K3ARCFU2hlZDr)\n",
            "✅ Successfully uploaded 10_01_2011.csv (ID: 1gaPacDzbc7KWbLEP8Ut56M0RJ3r4TWeF)\n",
            "✅ Successfully uploaded 14_01_2011.csv (ID: 1f7r84qQRxDyTxrG0vjQ83_jUKiel_wdR)\n",
            "✅ Successfully uploaded 11_01_2011.csv (ID: 119OiQzuqqZeVM98WgWJpZpysdVM0kJD1)\n",
            "✅ Successfully uploaded 2_01_2011.csv (ID: 1dj7RIaa0AWrrdA5pqbzyRkHK74scULAV)\n",
            "✅ Successfully uploaded 13_01_2011.csv (ID: 107GDuHEeecJ8GUzKnJZL6sLLOlQHuZ3P)\n",
            "✅ Successfully uploaded 5_01_2011.csv (ID: 1qbza8WYiyQVTCV561hP2-Qg629YlNpUE)\n",
            "✅ Successfully uploaded 7_01_2011.csv (ID: 1c7lhcfBKzB-RU1sq48P5zid3_kqiNHoz)\n",
            "✅ Successfully uploaded 3_01_2011.csv (ID: 1V9E-80cJMYNLDS6eivuj-TgrKVsJj5oI)\n",
            "✅ Successfully uploaded 8_01_2011.csv (ID: 14_4V8IuRKC-iuQtaJdhfQgyGCHUQr8VS)\n",
            "✅ Successfully uploaded 28_10_2011.csv (ID: 1JxaX22GQyegBn5SRWiktEP--RzsLJJoQ)\n",
            "✅ Successfully uploaded 14_10_2011.csv (ID: 1BsLifzsRCLYNPJg6pDcjK9YI8_c2Ukpm)\n",
            "✅ Successfully uploaded 2_10_2011.csv (ID: 1oThFH8DXoZ0fUqvc_Cbsj-AYqiTpgbOx)\n",
            "✅ Successfully uploaded 26_10_2011.csv (ID: 1jASOmCj2585nhk7SqywJt49J1AKZ1wbz)\n",
            "✅ Successfully uploaded 21_10_2011.csv (ID: 162R6ayEjB3cE5532NplGAxv1R_MyrbdA)\n",
            "✅ Successfully uploaded 9_10_2011.csv (ID: 1_JDgV9hipVisphHPDdRy5D-PLnvgG5Qh)\n",
            "✅ Successfully uploaded 1_10_2011.csv (ID: 1DNQPLTGIJ8Ob1EkdXP33giy68oJI-nDh)\n",
            "✅ Successfully uploaded 15_10_2011.csv (ID: 1iYOOVHbVzLTJ3jOUj135RqxI6sd2d493)\n",
            "✅ Successfully uploaded 13_10_2011.csv (ID: 1wQd3gu6nNaxw492KGCOLym2P4byrQOn9)\n",
            "✅ Successfully uploaded 11_10_2011.csv (ID: 1ABRuUTTVJhIIanXR9ttrawZxHB4gIZKJ)\n",
            "✅ Successfully uploaded 18_10_2011.csv (ID: 1GWrgLiSj3YwbRBuEwto5OU1qKz6ygXsD)\n",
            "✅ Successfully uploaded 22_10_2011.csv (ID: 10GO8n_OBEleqpayrscGFJaHK1UDJNJ-j)\n",
            "✅ Successfully uploaded 12_10_2011.csv (ID: 1zsKRY-pWw1Mg-ppC-jL3Jd7xbKS6pJG-)\n",
            "✅ Successfully uploaded 3_10_2011.csv (ID: 1iY64kIbrye0tcMSitWfEH4G28GxQaMRX)\n",
            "✅ Successfully uploaded 25_10_2011.csv (ID: 1wmt0JU73usdo9gCZZK8B70TavT4XjCGa)\n",
            "✅ Successfully uploaded 17_10_2011.csv (ID: 1H01nWBip2wNxokmKvNBzf3v5wiaV9-W4)\n",
            "✅ Successfully uploaded 6_10_2011.csv (ID: 18on4SHNHSNVrYh57SmuHJgUFULk2Cb5p)\n",
            "✅ Successfully uploaded 5_10_2011.csv (ID: 1Lk5meRQzRmS5gw84p2Ip9uMqiz7n51UG)\n",
            "✅ Successfully uploaded 27_10_2011.csv (ID: 1loe-of9y0LmrGNc2ZJYE2PQ21fzItpHO)\n",
            "✅ Successfully uploaded 16_10_2011.csv (ID: 1yqkeqg2MFhJ3wgffn6WfwIVAM-KMfR7z)\n",
            "✅ Successfully uploaded 8_10_2011.csv (ID: 1r_4faf0inl2LK1vaWwu7YvoVEwu7DCm3)\n",
            "✅ Successfully uploaded 10_10_2011.csv (ID: 1glOA_qzRrW_m-kekYbNDXHKuR-nqud3Z)\n",
            "✅ Successfully uploaded 24_10_2011.csv (ID: 1ADw-jYYXcCtE5OGDhqx3i_AWx-1z_W7Q)\n",
            "✅ Successfully uploaded 19_10_2011.csv (ID: 1qKiG8YY-z17HEUEzgkKDbN5IXpXHBjRt)\n",
            "✅ Successfully uploaded 23_10_2011.csv (ID: 1QcwSkHFGa370lfe-_rywBeO1L8vlz9Ti)\n",
            "✅ Successfully uploaded 4_10_2011.csv (ID: 18HFG2GnSUo5ywZY__P-C4W3NJSITg1TO)\n",
            "✅ Successfully uploaded 20_10_2011.csv (ID: 1hQwfgZ2ReGuPErKjhQoOl0m1qDpchILq)\n",
            "✅ Successfully uploaded 7_10_2011.csv (ID: 1TNs-HBuy6y-qFYTIoL7T3iepYZ7hIjkP)\n",
            "✅ Successfully uploaded 13_08_2011.csv (ID: 1uvwNWn9eNS6jQHo3OjaDBKo7KOhfpH4I)\n",
            "✅ Successfully uploaded 9_08_2011.csv (ID: 14_bRJB3a3V79Z18ynHdcVVkcp9WCfh99)\n",
            "✅ Successfully uploaded 31_08_2011.csv (ID: 1-jbZThTrYKgn_cjHr8kyhQWvR83nzzv_)\n",
            "✅ Successfully uploaded 29_08_2011.csv (ID: 1cK8Um5rbM0lES9pYBfC1ksPwRqNYwDEK)\n",
            "✅ Successfully uploaded 4_08_2011.csv (ID: 1LA38yOrmmWjSBGARfGxWannhtrcL_qE7)\n",
            "✅ Successfully uploaded 23_08_2011.csv (ID: 1eTIy4onRd6kNjMjm7JHC9b9iRyxyJ2iD)\n",
            "✅ Successfully uploaded 7_08_2011.csv (ID: 1ETGwvb5lVe7ZEoQYaXN5dZJRnsq_bwa3)\n",
            "✅ Successfully uploaded 19_08_2011.csv (ID: 1Lb7uvAzluXMUeqezU1KjQDwFERiCVNiv)\n",
            "✅ Successfully uploaded 35_08_2011.csv (ID: 1HzJGDbANT-0Hknf_IdNtTDbZUiSQMfsx)\n",
            "✅ Successfully uploaded 30_08_2011.csv (ID: 17z_I-03S3Cp1-7zKeJ2Fc7CEcPoGfSqA)\n",
            "✅ Successfully uploaded 1_08_2011.csv (ID: 1AB8G2i1uYQbc2M3CEOIRKuLQLYXL-naz)\n",
            "✅ Successfully uploaded 2_08_2011.csv (ID: 1KStdW2JnIledFkzaxSDF26yahvMRgSDq)\n",
            "✅ Successfully uploaded 18_08_2011.csv (ID: 1RLAmhW0BaE5qmZ7Bo8yfM4laNeUSiIeX)\n",
            "✅ Successfully uploaded 10_08_2011.csv (ID: 1Bhqgz3tBXpZ9qX8wuLhq6x4oLs7c_8Xf)\n",
            "✅ Successfully uploaded 6_08_2011.csv (ID: 1AZJI8bj8wjlN-r_W61tcVia_Qsd6NBiN)\n",
            "✅ Successfully uploaded 34_08_2011.csv (ID: 1fL1oCI6I0FjXlYxZ20ZBabne54twkg29)\n",
            "✅ Successfully uploaded 36_08_2011.csv (ID: 1_Ps-dT6lJI3ja6CrXU_ghTGZSM7ya8wM)\n",
            "✅ Successfully uploaded 8_08_2011.csv (ID: 1wDzYt0hlsfzB8n60YBl6wGNRnUiFm4be)\n",
            "✅ Successfully uploaded 28_08_2011.csv (ID: 1XrzABsZL81zmdFEFN98Hj83GqOtE39Hl)\n",
            "✅ Successfully uploaded 26_08_2011.csv (ID: 1ox2Oyv6E4t6W-EMQ3RvYm6a9h5AcDbrt)\n",
            "✅ Successfully uploaded 22_08_2011.csv (ID: 1FpdTdxXPtkMmnbCoS6qLyoA4joI73vJi)\n",
            "✅ Successfully uploaded 21_08_2011.csv (ID: 1mWnSuRZdUVoQtGI_eHsn1YJVBu7hB8QT)\n",
            "✅ Successfully uploaded 24_08_2011.csv (ID: 1dnUcX7MBFZHwID1ViFxJQRBudNd4Pa2T)\n",
            "✅ Successfully uploaded 32_08_2011.csv (ID: 1zWJ7jIQKmp3cSh40qONBcHHYUQif9QOH)\n",
            "✅ Successfully uploaded 25_08_2011.csv (ID: 1Atkp4ncA0O4lmxSIPf7fYKkkNNWKXBSU)\n",
            "✅ Successfully uploaded 33_08_2011.csv (ID: 18xUf0vOAa9wrZW-mAeFsvS63DvgRvCLr)\n",
            "✅ Successfully uploaded 14_08_2011.csv (ID: 13jT_6yogYp3vgVO73AWYSrwai70MqZ6r)\n",
            "✅ Successfully uploaded 15_08_2011.csv (ID: 1DN0RMMGO9leDTnI8NB0qYtuHCnVEw7rs)\n",
            "✅ Successfully uploaded 12_08_2011.csv (ID: 1INS5OEdJRBdNtJDDOXHz5Z8prF6llwso)\n",
            "✅ Successfully uploaded 16_08_2011.csv (ID: 1eUomPxLbQ_9by9qAd1TAnTp3HEs0FxDQ)\n",
            "✅ Successfully uploaded 5_08_2011.csv (ID: 1cOdREznQVhbNsHq6zDT6lOMYcCit3gSr)\n",
            "✅ Successfully uploaded 20_08_2011.csv (ID: 1FtxMO6OVVanAwPPDKhp6BJ501y9OsF-b)\n",
            "✅ Successfully uploaded 17_08_2011.csv (ID: 1JmGNrCiOetq48XDH9yKBQT8J3I60AzcY)\n",
            "✅ Successfully uploaded 3_08_2011.csv (ID: 1tPhxeEkahVJDYkoRHg7rWOBYBnbogHQN)\n",
            "✅ Successfully uploaded 11_08_2011.csv (ID: 13WqC-njxrUKN88ZM4IhU6iD1FiVDoy-1)\n",
            "✅ Successfully uploaded 27_08_2011.csv (ID: 1hvTzMU3R3N_3WINJCCi-P3c7IqIqP-ej)\n",
            "✅ Successfully uploaded 16_09_2011.csv (ID: 1YvS9UerB1VubTXXONNXGYA2z26efPg43)\n",
            "✅ Successfully uploaded 21_09_2011.csv (ID: 1JsAij3gUcprjMtYvvjxa9gaHfO8gxR4v)\n",
            "✅ Successfully uploaded 18_09_2011.csv (ID: 1j00_RvukgDrfHUXdLnlEAl6GQfwJz0eR)\n",
            "✅ Successfully uploaded 2_09_2011.csv (ID: 1ySdgxYUwO1jJLFzM1IffpGvx8tC46l1b)\n",
            "✅ Successfully uploaded 17_09_2011.csv (ID: 1X_M6jDWLtYu9ETpA4SCgy-ONVFQ5h019)\n",
            "✅ Successfully uploaded 8_09_2011.csv (ID: 13V0FWzAxRvjulmgNJI72AhK_3r0g3x2e)\n",
            "✅ Successfully uploaded 20_09_2011.csv (ID: 1Ug03GXTEFpxofkrV4tQlKWYhELy4cFQO)\n",
            "✅ Successfully uploaded 9_09_2011.csv (ID: 1b2og7ZQgqAI3X_2jv6eHZV-VObg6659I)\n",
            "✅ Successfully uploaded 1_09_2011.csv (ID: 1OY6Uv0Q_oSfd8waKDaycG3A1LQsrdX0G)\n",
            "✅ Successfully uploaded 6_09_2011.csv (ID: 14pNViwCn2vJyJG8Y43prD-_ZJifxLHLl)\n",
            "✅ Successfully uploaded 11_09_2011.csv (ID: 1IWp4om2IjogNulbU-5q68J8W0qWrOf1A)\n",
            "✅ Successfully uploaded 14_09_2011.csv (ID: 1eHFSF5hTIfkbHmh_vYDz0GHAUWnM5opm)\n",
            "✅ Successfully uploaded 13_09_2011.csv (ID: 1j4UOV04lFrEd1yeY9pryHZa4V96_kGmf)\n",
            "✅ Successfully uploaded 10_09_2011.csv (ID: 1GecSp6R5cEqmVFB-Q3D3MQv1sH-Y6LnL)\n",
            "✅ Successfully uploaded 5_09_2011.csv (ID: 10N7ne4FwJXIZ6Mbonhxv8ZxRAkGe3bEk)\n",
            "✅ Successfully uploaded 19_09_2011.csv (ID: 1NiUU21xwCzAj0P0q9zvVUvW56vO8BhXB)\n",
            "✅ Successfully uploaded 7_09_2011.csv (ID: 16Xmg8VFRm6DZHunTHjbH8O0hkw2Ji00f)\n",
            "✅ Successfully uploaded 3_09_2011.csv (ID: 14FKNGCFzp3hns872D-Vj2tThj6NYbC5k)\n",
            "✅ Successfully uploaded 12_09_2011.csv (ID: 1Eo0hhCeZuDnBds1tqEUCliet2N-gvomd)\n",
            "✅ Successfully uploaded 15_09_2011.csv (ID: 10tSqWxMRYPA2IbDK9SEa7QTNDmYSh-CC)\n",
            "✅ Successfully uploaded 4_09_2011.csv (ID: 1ndeWzd6vHRpIerghxG1ioF0xBb9Wa86C)\n",
            "✅ Successfully uploaded 5_15_2011.csv (ID: 1HX-J-l9DRvvT21qmfxqd2nGMPPvqTvjP)\n",
            "✅ Successfully uploaded 10_15_2011.csv (ID: 1tYw1wkyTiw3HnIFMXdQQ7hQiktrpTk6t)\n",
            "✅ Successfully uploaded 9_15_2011.csv (ID: 1q_LhLfgcKTuJGwzZ9VjkT08jaNJ2eXgR)\n",
            "✅ Successfully uploaded 3_15_2011.csv (ID: 1jyGj6svLMF-I59vUHlN2-gFmBCUAS8EG)\n",
            "✅ Successfully uploaded 2_15_2011.csv (ID: 1MMhhozP41X-ICidm-162D4ck9Wpgw2lh)\n",
            "✅ Successfully uploaded 6_15_2011.csv (ID: 1esVevt1SYlDFcbbs601IbjiEkZFOyXJs)\n",
            "✅ Successfully uploaded 4_15_2011.csv (ID: 1SEBMKglOdMstitXv6mUDjmoMqfK_LcU-)\n",
            "✅ Successfully uploaded 12_15_2011.csv (ID: 1NGBNgc0wlnaHTsM-fDsvBfLkVIHpoO1D)\n",
            "✅ Successfully uploaded 11_15_2011.csv (ID: 1BvbEfwQMB2johsGV1x7fa1C0jOA80C6p)\n",
            "✅ Successfully uploaded 7_15_2011.csv (ID: 1HdIX008cIzhSpzILOGNOJwWqT7Umi2xx)\n",
            "✅ Successfully uploaded 8_15_2011.csv (ID: 1WwiJ_l8nSaR75jdjwjCQFvv8kVASwgow)\n",
            "✅ Successfully uploaded 1_15_2011.csv (ID: 1wqxEnkSC7JoP0uzOhEl54dPV8uO4e2rc)\n",
            "✅ Successfully uploaded 19_13_2011.csv (ID: 1ARPQwkOhht59WAa3oaGj_FV_Hzuc-5kQ)\n",
            "✅ Successfully uploaded 13_13_2011.csv (ID: 1pnDI8foKMJINbgflP9om80UCqp36dfs9)\n",
            "✅ Successfully uploaded 18_13_2011.csv (ID: 1sroVcY2gS0rwvqyMN0yxh0tyU49IjlUE)\n",
            "✅ Successfully uploaded 17_13_2011.csv (ID: 1JGx_kT0QWP2Ipg8TWU5NaKU4CqUvCHLD)\n",
            "✅ Successfully uploaded 1_13_2011.csv (ID: 1ovOJdufZGTLGgtJij64jm-ccoWC3rNZ3)\n",
            "✅ Successfully uploaded 25_13_2011.csv (ID: 1c6UnAEq32F1rqlAxYJbCK5DlJ1P4jR3l)\n",
            "✅ Successfully uploaded 23_13_2011.csv (ID: 1oEOg3jNdza8UZccIIPSyeM62ei-YBm_4)\n",
            "✅ Successfully uploaded 2_13_2011.csv (ID: 1cPwFDyCgqGYZGZv6cHf8P7TYFdvtZ0o4)\n",
            "✅ Successfully uploaded 24_13_2011.csv (ID: 1BToNUgzecYDM4DcEyeuvHvSqwOlJpb-w)\n",
            "✅ Successfully uploaded 15_13_2011.csv (ID: 1DLlmhf1jKYulhZsf-QszHlOq8srveLsU)\n",
            "✅ Successfully uploaded 11_13_2011.csv (ID: 110KiQ3AdyS5YS2CunPSvbDTJdZqfl9zx)\n",
            "✅ Successfully uploaded 28_13_2011.csv (ID: 1PQTte2z-J0KMwR4agz7qUdrGn26acJ0T)\n",
            "✅ Successfully uploaded 3_13_2011.csv (ID: 1PqS60FLdno4bON11qUUVKQa9iic8L1nL)\n",
            "✅ Successfully uploaded 22_13_2011.csv (ID: 17fsFHHcHjq7JjoSBGnkrGeHsSItK76mt)\n",
            "✅ Successfully uploaded 10_13_2011.csv (ID: 1amU2BIot5JcEUzZQGUcalPcmC66WayaB)\n",
            "✅ Successfully uploaded 4_13_2011.csv (ID: 1-Rq5IACCUDev8x203BycU5JixYpB2e3L)\n",
            "✅ Successfully uploaded 9_13_2011.csv (ID: 1psd5vsf-l0WYVnIHXgp-PEj0Q9an61hH)\n",
            "✅ Successfully uploaded 12_13_2011.csv (ID: 1-I4nfdHDa3Lb52k_yYXX8XG95P2k1p0p)\n",
            "✅ Successfully uploaded 16_13_2011.csv (ID: 1zkcNA4r2rNJ1fRAFqoD0FmSJV-O9GdB8)\n",
            "✅ Successfully uploaded 7_13_2011.csv (ID: 1gk-DNZs__oE9zZGwXDQbEYroyJAMTfeR)\n",
            "✅ Successfully uploaded 21_13_2011.csv (ID: 1GJXc5iXcEzZWpbB2Umop0_3smm8ZklBf)\n",
            "✅ Successfully uploaded 14_13_2011.csv (ID: 13Cn25wrDsmGrNLPSybWzUbgQddGSTKeV)\n",
            "✅ Successfully uploaded 20_13_2011.csv (ID: 1_Et2Kgg8x9NF8WohKX_UnfWHNLHPesAF)\n",
            "✅ Successfully uploaded 5_13_2011.csv (ID: 1B8kthOicAm6jLo2vfnnUVtTtKNaobbHu)\n",
            "✅ Successfully uploaded 6_13_2011.csv (ID: 1MwKcrDswf-Q1PQlKqb7QvifCfqtfxynj)\n",
            "✅ Successfully uploaded 8_13_2011.csv (ID: 1mH_x5c19UHeFvvh86UQZl0dqa96pn_HR)\n",
            "✅ Successfully uploaded 27_13_2011.csv (ID: 1G2EwnabR0shO_84jl-ocw80-W2UpHenW)\n",
            "✅ Successfully uploaded 26_13_2011.csv (ID: 1Z5l-gKbeGqNuup9EXmmWdTZRBDd1JejJ)\n",
            "✅ Successfully uploaded 6_07_2011.csv (ID: 1y5KgRs6NziEGzc4u1Z1ZPYq8BDxwJNVZ)\n",
            "✅ Successfully uploaded 14_07_2011.csv (ID: 1u66U8PLfv8wYBe5iX75hbzvGjb725zHg)\n",
            "✅ Successfully uploaded 2_07_2011.csv (ID: 12W-5rSZ9zusEV1S8Yady00B1UluOYTzy)\n",
            "✅ Successfully uploaded 29_07_2011.csv (ID: 1wva6K6qFf7H30G5RA-PAlYn2ycxcFUFl)\n",
            "✅ Successfully uploaded 10_07_2011.csv (ID: 12Rgi_qL4djRHdZsud-ceChzJIBcoqZbO)\n",
            "✅ Successfully uploaded 17_07_2011.csv (ID: 1xNUyl0mmeX7WUFKI7wODSgrenuVZmSal)\n",
            "✅ Successfully uploaded 7_07_2011.csv (ID: 1pqkS1eHVQfcU3RsJK_skDhVeOWM8-f-S)\n",
            "✅ Successfully uploaded 31_07_2011.csv (ID: 1lFbFXh4cqrQGVi_F1cKAZTRv7WlsUHtM)\n",
            "✅ Successfully uploaded 23_07_2011.csv (ID: 1ReUpPhnjA2Ua6WTIcv3AXu3m6AsHSL7c)\n",
            "✅ Successfully uploaded 21_07_2011.csv (ID: 16mJJ0jVJa0XLELFNHXlm-UTDNo2WTeD_)\n",
            "✅ Successfully uploaded 3_07_2011.csv (ID: 1RHvGdTAAfgSnc1MBrJD5W5S25BlLkkkn)\n",
            "✅ Successfully uploaded 16_07_2011.csv (ID: 1CCPrFeO_AleNbKgJpEYj5yiBRoVynCVg)\n",
            "✅ Successfully uploaded 22_07_2011.csv (ID: 1JPYVCcoriL3MmmbTyRGkMjdEHYS35Gt5)\n",
            "✅ Successfully uploaded 33_07_2011.csv (ID: 1q5EBBIu9LRTB_UcfWkf1kF2fGKIfZHDk)\n",
            "✅ Successfully uploaded 19_07_2011.csv (ID: 1T3lT0ckF4PnPWej8KUiUBKZBPJobAcHY)\n",
            "✅ Successfully uploaded 12_07_2011.csv (ID: 1SrsFzu0fnAaF0maWKmI3yMsXK9kiMyC1)\n",
            "✅ Successfully uploaded 15_07_2011.csv (ID: 1Xu2Zpc9eES9kRcxrVli6PjdA7vZmPDUK)\n",
            "✅ Successfully uploaded 1_07_2011.csv (ID: 1zwEljScDygdcr8b-P9nbNf8SrlXpDDsW)\n",
            "✅ Successfully uploaded 35_07_2011.csv (ID: 1dBBOEA3MXDAqaQZte2n3lm64wo7ENocw)\n",
            "✅ Successfully uploaded 28_07_2011.csv (ID: 19KQtAk9JEJGvFsNCc_ER5gexO31yi7mC)\n",
            "✅ Successfully uploaded 30_07_2011.csv (ID: 1KWjzKhRCVCqEsqErQLQkdxBlDU1J8ogQ)\n",
            "✅ Successfully uploaded 13_07_2011.csv (ID: 1Wzg1USnIEQbqYFvggHB29BT8QxVOW3bW)\n",
            "✅ Successfully uploaded 27_07_2011.csv (ID: 1WG8z4JB4fl9ZRU4K7sDDOOhSPmFOawlN)\n",
            "✅ Successfully uploaded 20_07_2011.csv (ID: 18fJjBRv8ZQmwWl2MVOoeAQk6OL_9f03z)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4040478747.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m \u001b[0mupload_file_to_drive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_folder_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4040478747.py\u001b[0m in \u001b[0;36mupload_file_to_drive\u001b[0;34m(local_path, target_folder_id)\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mmedia_body\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmedia\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                             \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                         ).execute()\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0mfile_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muploaded_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;31m# Handle retries for server-side errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         resp, content = _retry_request(\n\u001b[0m\u001b[1;32m    924\u001b[0m             \u001b[0mhttp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0mnum_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;31m# Retry on SSL errors and socket timeout errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_ssl_SSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mssl_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google_auth_httplib2.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, uri, method, body, headers, redirections, connection_type, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# Make the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         response, content = self.http.request(\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httplib2/__init__.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, uri, method, body, headers, redirections, connection_type)\u001b[0m\n\u001b[1;32m   1720\u001b[0m                     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1722\u001b[0;31m                     (response, content) = self._request(\n\u001b[0m\u001b[1;32m   1723\u001b[0m                         \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthority\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mredirections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcachekey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httplib2/__init__.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, conn, host, absolute_uri, request_uri, method, body, headers, redirections, cachekey)\u001b[0m\n\u001b[1;32m   1440\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conn_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httplib2/__init__.py\u001b[0m in \u001b[0;36m_conn_request\u001b[0;34m(self, conn, request_uri, method, body, headers)\u001b[0m\n\u001b[1;32m   1392\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1393\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1394\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1395\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadStatusLine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponseNotReady\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m                 \u001b[0;31m# If we get a BadStatusLine on the first try then that means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1431\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"tables\", 'zip', \"tables\")\n",
        "files.download(\"tables.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JTNAdFBHf88h",
        "outputId": "f6a0045d-e9de-4006-99a1-aa8b5dff3b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f2ebceb1-13df-45f8-8817-50f4d65a6c32\", \"tables.zip\", 6689831)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}