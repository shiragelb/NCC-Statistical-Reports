{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiragelb/NCC-Statistical-Reports/blob/main/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install camelot-py[cv]\n",
        "!pip install tabula-py\n",
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Md13rOWPyd0",
        "outputId": "f50786e6-1566-4e47-b8b1-05121cd606cb"
      },
      "id": "9Md13rOWPyd0",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m678.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0\n",
            "Collecting camelot-py[cv]\n",
            "  Downloading camelot_py-1.0.9-py3-none-any.whl.metadata (9.8 kB)\n",
            "\u001b[33mWARNING: camelot-py 1.0.9 does not provide the extra 'cv'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (8.2.1)\n",
            "Requirement already satisfied: chardet>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (5.2.0)\n",
            "Requirement already satisfied: numpy>=1.26.1 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (2.0.2)\n",
            "Requirement already satisfied: openpyxl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (3.1.5)\n",
            "Requirement already satisfied: pdfminer-six>=20240706 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (20250506)\n",
            "Collecting pypdf<6.0,>=4.0 (from camelot-py[cv])\n",
            "  Downloading pypdf-5.9.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pandas>=2.2.2 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (2.2.2)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (0.9.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.7.0.68 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (4.12.0.88)\n",
            "Requirement already satisfied: pypdfium2>=4 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (4.30.0)\n",
            "Requirement already satisfied: pillow>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from camelot-py[cv]) (11.3.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl>=3.1.0->camelot-py[cv]) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.2->camelot-py[cv]) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer-six>=20240706->camelot-py[cv]) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.2->camelot-py[cv]) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer-six>=20240706->camelot-py[cv]) (2.22)\n",
            "Downloading pypdf-5.9.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading camelot_py-1.0.9-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, camelot-py\n",
            "Successfully installed camelot-py-1.0.9 pypdf-5.9.0\n",
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.12/dist-packages (from tabula-py) (2.2.2)\n",
            "Requirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.12/dist-packages (from tabula-py) (2.0.2)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.12/dist-packages (from tabula-py) (1.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.17.0)\n",
            "Downloading tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tabula-py\n",
            "Successfully installed tabula-py-2.10.0\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "6QwA0xOvTSZ3"
      },
      "id": "6QwA0xOvTSZ3"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from docx import Document\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import camelot\n",
        "import tabula\n",
        "import pdfplumber\n",
        "from docx.shared import Inches # Import Inches for setting image size\n",
        "import json"
      ],
      "metadata": {
        "id": "7lAXSHIxTVA7"
      },
      "id": "7lAXSHIxTVA7",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Tables"
      ],
      "metadata": {
        "id": "u4nJO-WJTYgD"
      },
      "id": "u4nJO-WJTYgD"
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_directory():\n",
        "    \"\"\"Create tables directory if it doesn't exist\"\"\"\n",
        "    if not os.path.exists('tables'):\n",
        "        os.makedirs('tables')\n",
        "        print(\"Created 'tables/' directory\")\n",
        "    else:\n",
        "        print(\"'tables/' directory already exists\")\n",
        "\n",
        "def extract_tables_with_names(docx_path):\n",
        "    \"\"\"Extract tables with their names from DOCX\"\"\"\n",
        "    doc = Document(docx_path)\n",
        "    tables = []\n",
        "\n",
        "    for i, table in enumerate(doc.tables):\n",
        "        # Extract table data\n",
        "        data = []\n",
        "        for row in table.rows:\n",
        "            data.append([cell.text.strip() for cell in row.cells])\n",
        "\n",
        "        if data:\n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "            # Try to find table name from first row or use default\n",
        "            # Assuming first row might contain the table name\n",
        "            table_name = f\"Table_{i+1}\"  # Default name\n",
        "            if len(data[0]) > 0 and len(data) >= 1:  # Single cell in first row might be title\n",
        "                table_name = data[0][0] if data[0][0] else table_name\n",
        "                df = pd.DataFrame(data[1:])  # Skip title row\n",
        "\n",
        "            tables.append((table_name, df))\n",
        "\n",
        "    return tables\n",
        "\n",
        "def save_tables_to_csv(tables, chapter, year):\n",
        "    \"\"\"Save tables to CSV files and return reference dictionary\"\"\"\n",
        "    reference_dict = {}\n",
        "\n",
        "    for i, (name, df) in enumerate(tables, 1):\n",
        "        # Create filename: table{i}{j}{k}.csv\n",
        "        filename = f\"table{i}{chapter}{year}.csv\"\n",
        "        filepath = os.path.join('tables', filename)\n",
        "\n",
        "        # Save dataframe to CSV\n",
        "        df.to_csv(filepath, index=False, header=False)\n",
        "\n",
        "        # Add to reference dictionary\n",
        "        reference_dict[name] = filepath\n",
        "        print(f\"Saved: {filepath}\")\n",
        "\n",
        "    return reference_dict\n",
        "\n",
        "def save_dictionary_to_json(reference_dict, filename='table_references.json'):\n",
        "    \"\"\"Save reference dictionary to JSON file with proper Unicode support\"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(reference_dict, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"Reference dictionary saved to {filename}\")\n",
        "\n",
        "def process_documents(doc1_path, chapter1, year1, doc2_path, chapter2, year2):\n",
        "    \"\"\"Main function to process both documents\"\"\"\n",
        "    # Setup directory\n",
        "    setup_directory()\n",
        "\n",
        "    # Combined dictionary for all tables\n",
        "    all_references = {}\n",
        "\n",
        "    # Process first document\n",
        "    print(f\"\\nProcessing: {doc1_path}\")\n",
        "    tables1 = extract_tables_with_names(doc1_path)\n",
        "    ref_dict1 = save_tables_to_csv(tables1, chapter1, year1)\n",
        "    all_references.update(ref_dict1)\n",
        "\n",
        "    # Process second document\n",
        "    print(f\"\\nProcessing: {doc2_path}\")\n",
        "    tables2 = extract_tables_with_names(doc2_path)\n",
        "    ref_dict2 = save_tables_to_csv(tables2, chapter2, year2)\n",
        "    all_references.update(ref_dict2)\n",
        "\n",
        "    # Save combined dictionary\n",
        "    save_dictionary_to_json(all_references)\n",
        "\n",
        "    print(f\"\\nTotal tables processed: {len(all_references)}\")\n",
        "    return all_references"
      ],
      "metadata": {
        "id": "TwdQ-81kdkoH"
      },
      "id": "TwdQ-81kdkoH",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload\n",
        "chp1_2001_raw = files.upload()\n",
        "chp1_2002_raw = files.upload()\n",
        "\n",
        "# Extract file names\n",
        "chp1_2001 = list(chp1_2001_raw.keys())[0]\n",
        "chp1_2002 = list(chp1_2002_raw.keys())[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "Q13mkGDjTbLx",
        "outputId": "66228547-db4a-4f27-9786-eb230bf3f1a5"
      },
      "id": "Q13mkGDjTbLx",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7f5fd9bb-2f0e-42a5-bc99-33a019ca5a9d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7f5fd9bb-2f0e-42a5-bc99-33a019ca5a9d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving chap 01.docx to chap 01.docx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4eb3514e-9ab4-44f3-98d7-b392303f5e43\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4eb3514e-9ab4-44f3-98d7-b392303f5e43\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving chap 01.docx to chap 01 (1).docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_documents(chp1_2001, 1, 2001, chp1_2002, 1, 2002)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWqslKxIkIlh",
        "outputId": "9d04a8bf-aaa2-4371-e3df-1633ac8ae3c6"
      },
      "id": "UWqslKxIkIlh",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 'tables/' directory\n",
            "\n",
            "Processing: chap 01.docx\n",
            "Saved: tables/table112001.csv\n",
            "Saved: tables/table212001.csv\n",
            "Saved: tables/table312001.csv\n",
            "Saved: tables/table412001.csv\n",
            "Saved: tables/table512001.csv\n",
            "Saved: tables/table612001.csv\n",
            "Saved: tables/table712001.csv\n",
            "Saved: tables/table812001.csv\n",
            "Saved: tables/table912001.csv\n",
            "Saved: tables/table1012001.csv\n",
            "Saved: tables/table1112001.csv\n",
            "Saved: tables/table1212001.csv\n",
            "Saved: tables/table1312001.csv\n",
            "Saved: tables/table1412001.csv\n",
            "Saved: tables/table1512001.csv\n",
            "Saved: tables/table1612001.csv\n",
            "Saved: tables/table1712001.csv\n",
            "Saved: tables/table1812001.csv\n",
            "Saved: tables/table1912001.csv\n",
            "Saved: tables/table2012001.csv\n",
            "Saved: tables/table2112001.csv\n",
            "Saved: tables/table2212001.csv\n",
            "Saved: tables/table2312001.csv\n",
            "Saved: tables/table2412001.csv\n",
            "Saved: tables/table2512001.csv\n",
            "Saved: tables/table2612001.csv\n",
            "\n",
            "Processing: chap 01 (1).docx\n",
            "Saved: tables/table112002.csv\n",
            "Saved: tables/table212002.csv\n",
            "Saved: tables/table312002.csv\n",
            "Saved: tables/table412002.csv\n",
            "Saved: tables/table512002.csv\n",
            "Saved: tables/table612002.csv\n",
            "Saved: tables/table712002.csv\n",
            "Saved: tables/table812002.csv\n",
            "Saved: tables/table912002.csv\n",
            "Saved: tables/table1012002.csv\n",
            "Saved: tables/table1112002.csv\n",
            "Saved: tables/table1212002.csv\n",
            "Saved: tables/table1312002.csv\n",
            "Saved: tables/table1412002.csv\n",
            "Saved: tables/table1512002.csv\n",
            "Saved: tables/table1612002.csv\n",
            "Saved: tables/table1712002.csv\n",
            "Saved: tables/table1812002.csv\n",
            "Saved: tables/table1912002.csv\n",
            "Saved: tables/table2012002.csv\n",
            "Saved: tables/table2112002.csv\n",
            "Saved: tables/table2212002.csv\n",
            "Saved: tables/table2312002.csv\n",
            "Saved: tables/table2412002.csv\n",
            "Saved: tables/table2512002.csv\n",
            "Reference dictionary saved to table_references.json\n",
            "\n",
            "Total tables processed: 39\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Table_1': 'tables/table112002.csv',\n",
              " 'ילדים בישראל*\\nלפי דת (אלפים ושיעור גידולם)\\n2000-1970': 'tables/table212001.csv',\n",
              " 'אחוז הילדים בישראל מכלל האוכלוסייה\\nלפי דת \\n2000-1970': 'tables/table312001.csv',\n",
              " 'מספר הילדים\\nלפי גיל, דת, סוג וגודל יישוב (אלפים*)\\nממוצע 2000': 'tables/table412001.csv',\n",
              " 'מספר הילדים\\nלפי גיל, דת, סוג וגודל יישוב (אלפים*)\\nממוצע 2000 (המשך)': 'tables/table512001.csv',\n",
              " 'מספר הילדים, חלקם באוכלוסייה ודתם \\nלפי מחוז ונפה (אלפים ואחוזים)\\nממוצע 2000': 'tables/table612001.csv',\n",
              " 'מספר הילדים ביישובים מעורבים נבחרים \\nלפי דת (אלפים ואחוזים)\\nסוף דצמבר 2000': 'tables/table712001.csv',\n",
              " 'מספר הילדים לפי גיל וחלקם באוכלוסיית היישובים* \\n(אלפים ואחוזים)\\nסוף דצמבר 2000': 'tables/table812001.csv',\n",
              " 'מספר הילדים לפי גיל וחלקם באוכלוסיית היישובים* \\n(אלפים ואחוזים)\\nסוף דצמבר 2000 (המשך)': 'tables/table1212001.csv',\n",
              " 'חלקם של  הילדים באוכלוסיית היישובים שמנו 10,000 תושבים ויותר \\n(אחוזים)\\nסוף דצמבר 2000': 'tables/table1312001.csv',\n",
              " 'חלקם של  הילדים באוכלוסיית היישובים שמנו 10,000-5,000 תושבים \\n(אחוזים)\\nסוף דצמבר 2000': 'tables/table1412001.csv',\n",
              " 'אחוז  הילדים מכלל האוכלוסייה \\nביישובים נבחרים שבהם  הם מונים פחות מ-25% או יותר מ-50% \\nסוף דצמבר 2000': 'tables/table1512001.csv',\n",
              " 'ילדים בישראל לפי גיל ודת\\n(אלפים)\\nסוף שנת 2000': 'tables/table1612001.csv',\n",
              " \"תרשים 1ב'\": 'tables/table1712001.csv',\n",
              " 'ילדים אשר על פי מרשם האוכלוסין אינם קשורים להורה/הורים \\nלפי יישוב מגורים* \\n(מספרים ושיעורים ל-1,000) מרץ 2001': 'tables/table1812001.csv',\n",
              " 'ילדים אשר על פי מרשם האוכלוסין אינם קשורים להורה/הורים \\nלפי יישוב מגורים* \\n(מספרים ושיעורים ל-1,000) מרץ 2001 (המשך)': 'tables/table2212001.csv',\n",
              " 'מוצאם* של ילדים יהודיים בגיל 17-0 \\nלפי גיל (אלפים**)\\nממוצע 2000': 'tables/table2312001.csv',\n",
              " 'לידות חי\\nלפי מחוז, נפה וקבוצת אוכלוסייה (מספרים מוחלטים)\\n2000-1995': 'tables/table2412001.csv',\n",
              " 'לידות חי \\n לפי יישוב* (מספרים)\\n2000-1995': 'tables/table2512001.csv',\n",
              " 'לידות חי \\nלפי יישוב* \\n(מספרים מוחלטים) 2000-1995 (המשך)': 'tables/table2612001.csv',\n",
              " 'ילדים בישראל*\\nלפי דת (אלפים ושיעור גידולם)\\n2001-1970': 'tables/table212002.csv',\n",
              " 'אחוז הילדים בישראל מכלל האוכלוסייה\\nלפי דת \\n2001-1970': 'tables/table312002.csv',\n",
              " 'התפלגות אוכלוסיית הילדים בישראל\\nלפי דת (אחוזים)\\nסוף 2001': 'tables/table412002.csv',\n",
              " 'ילדים בישראל לפי גיל ודת\\n(אלפים)\\nסוף שנת 2001': 'tables/table512002.csv',\n",
              " 'מספר הילדים\\nלפי גיל, דת, סוג וגודל יישוב (אלפים*)\\nממוצע 2001': 'tables/table612002.csv',\n",
              " 'מספר הילדים\\nלפי גיל, דת, סוג וגודל יישוב (אלפים*)\\nממוצע 2001 (המשך)': 'tables/table812002.csv',\n",
              " 'מספר הילדים, חלקם באוכלוסייה ודתם \\nלפי מחוז ונפה (אלפים ואחוזים)\\nממוצע 2001': 'tables/table912002.csv',\n",
              " 'מספר הילדים לפי גיל וחלקם באוכלוסיית היישובים* \\n(אלפים ואחוזים)\\nסוף דצמבר 2001': 'tables/table1012002.csv',\n",
              " 'מספר הילדים לפי גיל וחלקם באוכלוסיית היישובים* \\n(אלפים ואחוזים)\\nסוף דצמבר 2001 (המשך)': 'tables/table1412002.csv',\n",
              " 'חלקם של  הילדים באוכלוסיית היישובים שמנו 10,000 תושבים ויותר \\n(אחוזים)\\nסוף דצמבר 2001': 'tables/table1512002.csv',\n",
              " 'חלקם של  הילדים באוכלוסיית היישובים שמנו 10,000-5,000 תושבים \\n(אחוזים)\\nסוף דצמבר 2001': 'tables/table1612002.csv',\n",
              " 'יישובים נבחרים שבהם  הילדים \\nמונים פחות מ-25% או יותר מ-50% מהאוכלוסייה\\nסוף דצמבר 2001': 'tables/table1712002.csv',\n",
              " 'אחוז  הילדים המתגוררים ביישובים \\nהלא מוכרים בנגב ( מכלל אוכלוסיית היישובים) \\nמאי 2002': 'tables/table1812002.csv',\n",
              " 'גיל הילדים המתגוררים ביישובים \\nהלא מוכרים בנגב (אחוזים)\\nמאי 2002': 'tables/table1912002.csv',\n",
              " 'מוצאם* של ילדים יהודיים בגיל 17-0 \\nלפי גיל (אלפים**)\\nממוצע 2001': 'tables/table2012002.csv',\n",
              " \"תרשים 1ה'\": 'tables/table2112002.csv',\n",
              " 'לידות חי\\nלפי מחוז, נפה וקבוצת אוכלוסייה (מספרים)\\n2001-1995': 'tables/table2212002.csv',\n",
              " 'לידות חי \\n לפי יישוב* (מספרים)\\n2001-1995': 'tables/table2312002.csv',\n",
              " 'לידות חי \\nלפי יישוב* \\n(מספרים מוחלטים) 2001-1995 (המשך)': 'tables/table2512002.csv'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table Extraction\n"
      ],
      "metadata": {
        "id": "cBSbt3pVnHNK"
      },
      "id": "cBSbt3pVnHNK"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "!pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKPQwn_Ima0q",
        "outputId": "20bcf245-77c8-4618-c886-fa910d67bf8f"
      },
      "id": "TKPQwn_Ima0q",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
            "Downloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Automated Table Extraction System\n",
        "# Phase 1: Environment Setup & Dependencies\n",
        "\n",
        "# Install required packages (run this in Google Colab)\n",
        "# !pip install python-docx python-docx2txt\n",
        "\n",
        "# Core imports\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import mimetypes\n",
        "\n",
        "# Document processing imports\n",
        "from docx import Document\n",
        "import docx2txt\n",
        "\n",
        "# Google Colab specific imports\n",
        "from google.colab import files\n",
        "\n",
        "# File validation and utilities\n",
        "import zipfile\n",
        "from typing import Tuple, List, Dict, Optional, Union\n",
        "\n",
        "print(\"✅ All required libraries imported successfully\")\n",
        "print(\"📋 Environment setup complete\")\n",
        "\n",
        "# Phase 1, Step 1.2: Enhanced Directory Structure Function\n",
        "def setup_directory_structure(year: int, chapter: int) -> str:\n",
        "    \"\"\"\n",
        "    Create directory structure: Tables/year/chapter_j/\n",
        "\n",
        "    Args:\n",
        "        year: Year for the directory structure\n",
        "        chapter: Chapter number for the directory\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the created chapter directory\n",
        "\n",
        "    Raises:\n",
        "        OSError: If directory creation fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create the directory path\n",
        "        chapter_dir = f\"Tables/{year}/chapter_{chapter}\"\n",
        "\n",
        "        # Create directories (including parents)\n",
        "        os.makedirs(chapter_dir, exist_ok=True)\n",
        "\n",
        "        print(f\"✅ Directory structure created: {chapter_dir}/\")\n",
        "        return chapter_dir\n",
        "\n",
        "    except OSError as e:\n",
        "        error_msg = f\"❌ Failed to create directory structure Tables/{year}/chapter_{chapter}/: {e}\"\n",
        "        print(error_msg)\n",
        "        raise OSError(error_msg)\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ Unexpected error creating directory: {e}\"\n",
        "        print(error_msg)\n",
        "        raise Exception(error_msg)\n",
        "\n",
        "# Phase 2: File Processing & Validation\n",
        "# Step 2.1: File Format Detection and Validation\n",
        "\n",
        "def validate_and_identify_file(filepath: str) -> str:\n",
        "    \"\"\"\n",
        "    Validate file exists and identify if it's DOC or DOCX format\n",
        "\n",
        "    Args:\n",
        "        filepath: Path to the file to validate\n",
        "\n",
        "    Returns:\n",
        "        str: 'docx' or 'doc' depending on file type\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If file doesn't exist\n",
        "        ValueError: If file is not DOC or DOCX format\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if file exists\n",
        "        if not os.path.exists(filepath):\n",
        "            raise FileNotFoundError(f\"❌ File not found: {filepath}\")\n",
        "\n",
        "        # Get file extension\n",
        "        _, extension = os.path.splitext(filepath.lower())\n",
        "\n",
        "        # Check if it's a supported format\n",
        "        if extension == '.docx':\n",
        "            # Additional validation: try to open as zip (DOCX is zip-based)\n",
        "            try:\n",
        "                with zipfile.ZipFile(filepath, 'r') as zip_file:\n",
        "                    # Check if it has the typical DOCX structure\n",
        "                    if 'word/document.xml' in zip_file.namelist():\n",
        "                        print(f\"✅ Valid DOCX file detected: {os.path.basename(filepath)}\")\n",
        "                        return 'docx'\n",
        "                    else:\n",
        "                        raise ValueError(f\"❌ File appears corrupted or invalid DOCX: {filepath}\")\n",
        "            except zipfile.BadZipFile:\n",
        "                raise ValueError(f\"❌ File is not a valid DOCX format: {filepath}\")\n",
        "\n",
        "        elif extension == '.doc':\n",
        "            # Basic validation for DOC files (check file size > 0)\n",
        "            file_size = os.path.getsize(filepath)\n",
        "            if file_size > 0:\n",
        "                print(f\"✅ DOC file detected: {os.path.basename(filepath)}\")\n",
        "                return 'doc'\n",
        "            else:\n",
        "                raise ValueError(f\"❌ DOC file appears empty: {filepath}\")\n",
        "\n",
        "        else:\n",
        "            supported_formats = ['.doc', '.docx']\n",
        "            raise ValueError(f\"❌ Unsupported file format: {extension}. Supported formats: {supported_formats}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        if isinstance(e, (FileNotFoundError, ValueError)):\n",
        "            raise e\n",
        "        else:\n",
        "            raise ValueError(f\"❌ Error validating file {filepath}: {e}\")\n",
        "\n",
        "# Step 2.2: Unified Document Loader with DOC to DOCX Conversion\n",
        "\n",
        "def load_document(filepath: str, file_type: str) -> Document:\n",
        "    \"\"\"\n",
        "    Load document using appropriate method, converting DOC to DOCX if needed\n",
        "\n",
        "    Args:\n",
        "        filepath: Path to the document file\n",
        "        file_type: 'doc' or 'docx' as returned by validate_and_identify_file\n",
        "\n",
        "    Returns:\n",
        "        Document: python-docx Document object (same for both DOC and DOCX)\n",
        "\n",
        "    Raises:\n",
        "        Exception: If document loading or conversion fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if file_type == 'docx':\n",
        "            # Load DOCX directly using python-docx\n",
        "            try:\n",
        "                doc = Document(filepath)\n",
        "                print(f\"✅ DOCX document loaded successfully: {os.path.basename(filepath)}\")\n",
        "                return doc\n",
        "            except Exception as e:\n",
        "                raise Exception(f\"❌ Failed to load DOCX document: {e}\")\n",
        "\n",
        "        elif file_type == 'doc':\n",
        "            # Convert DOC to DOCX first, then load\n",
        "            try:\n",
        "                print(f\"🔄 Converting DOC to DOCX: {os.path.basename(filepath)}\")\n",
        "\n",
        "                # Create temporary file for converted DOCX\n",
        "                with tempfile.NamedTemporaryFile(suffix='.docx', delete=False) as temp_file:\n",
        "                    temp_docx_path = temp_file.name\n",
        "\n",
        "                try:\n",
        "                    # Convert DOC to DOCX using pypandoc (preserves tables)\n",
        "                    pypandoc.convert_file(\n",
        "                        filepath,\n",
        "                        'docx',\n",
        "                        outputfile=temp_docx_path,\n",
        "                        extra_args=['--preserve-tabs']  # Help preserve table structure\n",
        "                    )\n",
        "\n",
        "                    # Load the converted DOCX file\n",
        "                    doc = Document(temp_docx_path)\n",
        "                    print(f\"✅ DOC converted and loaded successfully: {os.path.basename(filepath)}\")\n",
        "                    return doc\n",
        "\n",
        "                finally:\n",
        "                    # Clean up temporary file\n",
        "                    try:\n",
        "                        os.unlink(temp_docx_path)\n",
        "                    except:\n",
        "                        pass  # Ignore cleanup errors\n",
        "\n",
        "            except Exception as e:\n",
        "                raise Exception(f\"❌ Failed to convert/load DOC document: {e}\")\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"❌ Unsupported file type: {file_type}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ Document loading error for {filepath}: {e}\"\n",
        "        print(error_msg)\n",
        "        raise Exception(error_msg)\n",
        "\n",
        "# Phase 3: Table Extraction & Filtering\n",
        "# Step 3.1: Enhanced Table Extraction with Hebrew Filtering and Name Preservation\n",
        "\n",
        "def extract_filtered_tables(doc: Document) -> List[Tuple[str, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Extract tables that contain 'לוח' in the title row, preserving table names\n",
        "\n",
        "    Args:\n",
        "        doc: python-docx Document object\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, pd.DataFrame]]: List of (table_name, dataframe) tuples\n",
        "\n",
        "    Raises:\n",
        "        Exception: If table extraction fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tables = []\n",
        "        filtered_count = 0\n",
        "        total_tables = len(doc.tables)\n",
        "\n",
        "        print(f\"🔍 Found {total_tables} tables in document, filtering for 'לוח'...\")\n",
        "\n",
        "        for i, table in enumerate(doc.tables):\n",
        "            try:\n",
        "                # Extract table data\n",
        "                data = []\n",
        "                for row in table.rows:\n",
        "                    row_data = [cell.text.strip() for cell in row.cells]\n",
        "                    data.append(row_data)\n",
        "\n",
        "                # Skip empty tables\n",
        "                if not data or not any(cell for row in data for cell in row if cell.strip()):\n",
        "                    print(f\"⚠️  Table {i+1}: Empty table, skipping\")\n",
        "                    continue\n",
        "\n",
        "                # Check if first row contains 'לוח'\n",
        "                first_row = data[0]\n",
        "                contains_hebrew_table = any('לוח' in str(cell) for cell in first_row)\n",
        "\n",
        "                if contains_hebrew_table:\n",
        "                    # Extract table name from first cell or use default\n",
        "                    table_name = f\"Table_{i+1}\"  # Default name\n",
        "                    if first_row[0] and first_row[0].strip():\n",
        "                        table_name = first_row[0].strip()\n",
        "\n",
        "                    # Create DataFrame from data (skip title row)\n",
        "                    if len(data) > 1:\n",
        "                        df = pd.DataFrame(data[1:])  # Skip title row\n",
        "\n",
        "                        # Validate DataFrame has content\n",
        "                        if not df.empty and df.notna().any().any():\n",
        "                            tables.append((table_name, df))\n",
        "                            print(f\"✅ Table {i+1}: '{table_name}' - extracted ({len(df)} rows)\")\n",
        "                            filtered_count += 1\n",
        "                        else:\n",
        "                            print(f\"⚠️  Table {i+1}: '{table_name}' - no extractable data after title row\")\n",
        "                    else:\n",
        "                        print(f\"⚠️  Table {i+1}: '{table_name}' - only title row found, no data to extract\")\n",
        "                else:\n",
        "                    print(f\"❌ Table {i+1}: No 'לוח' found in title row, skipping\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  Error processing table {i+1}: {e}\")\n",
        "                continue\n",
        "\n",
        "        print(f\"📊 Filtering complete: {filtered_count}/{total_tables} tables contain 'לוח'\")\n",
        "\n",
        "        if not tables:\n",
        "            print(\"⚠️  No tables with 'לוח' found in document\")\n",
        "\n",
        "        return tables\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ Table extraction failed: {e}\"\n",
        "        print(error_msg)\n",
        "        raise Exception(error_msg)\n",
        "\n",
        "# Step 3.2: Table Naming and Numbering\n",
        "\n",
        "def generate_table_filename(table_number: int, chapter: int, year: int) -> str:\n",
        "    \"\"\"\n",
        "    Generate standardized filename for tables: Ti_chpj_year.csv\n",
        "\n",
        "    Args:\n",
        "        table_number: Sequential number of the table (i)\n",
        "        chapter: Chapter number (j)\n",
        "        year: Year\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted filename (e.g., \"T1_chp2_2023.csv\")\n",
        "    \"\"\"\n",
        "    filename = f\"T{table_number}_chp{chapter}_{year}.csv\"\n",
        "    return filename\n",
        "\n",
        "def assign_table_numbers(tables: List[Tuple[str, pd.DataFrame]]) -> List[Tuple[str, pd.DataFrame, str]]:\n",
        "    \"\"\"\n",
        "    Assign sequential numbers and generate filenames for filtered tables\n",
        "\n",
        "    Args:\n",
        "        tables: List of (table_name, dataframe) tuples from extract_filtered_tables\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, pd.DataFrame, str]]: List of (table_name, dataframe, filename) tuples\n",
        "    \"\"\"\n",
        "    numbered_tables = []\n",
        "\n",
        "    for i, (table_name, df) in enumerate(tables, 1):\n",
        "        # Note: chapter and year will be provided when saving\n",
        "        # For now, we just assign the table number\n",
        "        numbered_tables.append((table_name, df, i))\n",
        "\n",
        "    print(f\"📝 Assigned sequential numbers to {len(numbered_tables)} tables\")\n",
        "    return numbered_tables\n",
        "\n",
        "# Phase 4: File Operations & Output\n",
        "# Step 4.1: Enhanced CSV Saving with Directory Structure\n",
        "\n",
        "def save_tables_to_csv(numbered_tables: List[Tuple[str, pd.DataFrame, int]],\n",
        "                       chapter: int, year: int, chapter_dir: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Save tables to CSV files in organized directory structure\n",
        "\n",
        "    Args:\n",
        "        numbered_tables: List of (table_name, dataframe, table_number) tuples\n",
        "        chapter: Chapter number for filename generation\n",
        "        year: Year for filename generation\n",
        "        chapter_dir: Directory path where CSV files should be saved\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, str]: Reference dictionary mapping table names to file paths\n",
        "    \"\"\"\n",
        "    reference_dict = {}\n",
        "    total_tables = len(numbered_tables)\n",
        "\n",
        "    if total_tables == 0:\n",
        "        print(\"⚠️  No tables to save\")\n",
        "        return reference_dict\n",
        "\n",
        "    for table_name, df, table_number in numbered_tables:\n",
        "        try:\n",
        "            # Generate filename using the standardized format\n",
        "            filename = generate_table_filename(table_number, chapter, year)\n",
        "            filepath = os.path.join(chapter_dir, filename)\n",
        "\n",
        "            # Save DataFrame to CSV (no index, no header as per original code)\n",
        "            df.to_csv(filepath, index=False, header=False, encoding='utf-8')\n",
        "\n",
        "            # Add to reference dictionary\n",
        "            reference_dict[table_name] = filepath\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Failed to save table '{table_name}': {e}\")\n",
        "            continue\n",
        "\n",
        "    successful_saves = len(reference_dict)\n",
        "    print(f\"✅ Successfully saved {successful_saves}/{total_tables} tables\")\n",
        "    return reference_dict\n",
        "\n",
        "# Step 4.2: Enhanced JSON Reference Saving\n",
        "\n",
        "def save_file_reference_dictionary(reference_dict: Dict[str, str],\n",
        "                                 chapter: int, year: int, chapter_dir: str) -> str:\n",
        "    \"\"\"\n",
        "    Save reference dictionary to JSON file with per-file naming convention\n",
        "\n",
        "    Args:\n",
        "        reference_dict: Dictionary mapping table names to file paths\n",
        "        chapter: Chapter number for filename generation\n",
        "        year: Year for filename generation\n",
        "        chapter_dir: Directory where JSON file should be saved\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the saved JSON file\n",
        "\n",
        "    Raises:\n",
        "        Exception: If JSON saving fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate filename: references_chpj_year.json\n",
        "        json_filename = f\"references_chp{chapter}_{year}.json\"\n",
        "        json_filepath = os.path.join(chapter_dir, json_filename)\n",
        "\n",
        "        # Save reference dictionary with proper Unicode support\n",
        "        with open(json_filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(reference_dict, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"✅ Reference dictionary saved: {json_filename}\")\n",
        "        return json_filepath\n",
        "\n",
        "    except Exception as e:\n",
        "        error_msg = f\"❌ Failed to save reference dictionary: {e}\"\n",
        "        print(error_msg)\n",
        "        raise Exception(error_msg)\n",
        "\n",
        "# Phase 5: Main Automation Loop\n",
        "# Step 5.1: Single-File Processing Function\n",
        "\n",
        "def process_single_file(filepath: str, chapter: int, year: int) -> Dict[str, Union[int, str, bool]]:\n",
        "    \"\"\"\n",
        "    Complete pipeline to process a single document file\n",
        "\n",
        "    Args:\n",
        "        filepath: Path to the document file to process\n",
        "        chapter: Chapter number for organization and naming\n",
        "        year: Year for organization and naming\n",
        "\n",
        "    Returns:\n",
        "        Dict: Processing statistics and status including:\n",
        "            - success: bool\n",
        "            - tables_found: int\n",
        "            - tables_filtered: int\n",
        "            - tables_saved: int\n",
        "            - json_saved: bool\n",
        "            - error_message: str (if error occurred)\n",
        "    \"\"\"\n",
        "    stats = {\n",
        "        'success': False,\n",
        "        'tables_found': 0,\n",
        "        'tables_filtered': 0,\n",
        "        'tables_saved': 0,\n",
        "        'json_saved': False,\n",
        "        'error_message': None\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(f\"\\n🔄 Processing: {os.path.basename(filepath)} (Chapter {chapter}, Year {year})\")\n",
        "\n",
        "        # Step 1: Validate and identify file type\n",
        "        try:\n",
        "            file_type = validate_and_identify_file(filepath)\n",
        "        except Exception as e:\n",
        "            stats['error_message'] = f\"File validation failed: {e}\"\n",
        "            return stats\n",
        "\n",
        "        # Step 2: Setup directory structure\n",
        "        try:\n",
        "            chapter_dir = setup_directory_structure(year, chapter)\n",
        "        except Exception as e:\n",
        "            stats['error_message'] = f\"Directory setup failed: {e}\"\n",
        "            return stats\n",
        "\n",
        "        # Step 3: Load document\n",
        "        try:\n",
        "            doc = load_document(filepath, file_type)\n",
        "            stats['tables_found'] = len(doc.tables)\n",
        "        except Exception as e:\n",
        "            stats['error_message'] = f\"Document loading failed: {e}\"\n",
        "            return stats\n",
        "\n",
        "        # Step 4: Extract and filter tables\n",
        "        try:\n",
        "            filtered_tables = extract_filtered_tables(doc)\n",
        "            stats['tables_filtered'] = len(filtered_tables)\n",
        "\n",
        "            if not filtered_tables:\n",
        "                print(\"⚠️  No tables containing 'לוח' found in document\")\n",
        "                stats['success'] = True  # Not an error, just no matching tables\n",
        "                return stats\n",
        "\n",
        "        except Exception as e:\n",
        "            stats['error_message'] = f\"Table extraction failed: {e}\"\n",
        "            return stats\n",
        "\n",
        "        # Step 5: Assign table numbers\n",
        "        try:\n",
        "            numbered_tables = assign_table_numbers(filtered_tables)\n",
        "        except Exception as e:\n",
        "            stats['error_message'] = f\"Table numbering failed: {e}\"\n",
        "            return stats\n",
        "\n",
        "        # Step 6: Save tables to CSV\n",
        "        try:\n",
        "            reference_dict = save_tables_to_csv(numbered_tables, chapter, year, chapter_dir)\n",
        "            stats['tables_saved'] = len(reference_dict)\n",
        "        except Exception as e:\n",
        "            stats['error_message'] = f\"CSV saving failed: {e}\"\n",
        "            return stats\n",
        "\n",
        "        # Step 7: Save reference dictionary\n",
        "        try:\n",
        "            json_filepath = save_file_reference_dictionary(reference_dict, chapter, year, chapter_dir)\n",
        "            stats['json_saved'] = True\n",
        "            print(f\"📁 Files saved to: {chapter_dir}/\")\n",
        "        except Exception as e:\n",
        "            stats['error_message'] = f\"JSON saving failed: {e}\"\n",
        "            return stats\n",
        "\n",
        "        # Success!\n",
        "        stats['success'] = True\n",
        "        print(f\"✅ Processing complete: {stats['tables_saved']} tables extracted and saved\")\n",
        "\n",
        "    except Exception as e:\n",
        "        stats['error_message'] = f\"Unexpected error: {e}\"\n",
        "        print(f\"❌ Processing failed: {stats['error_message']}\")\n",
        "\n",
        "    return stats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP8iULvKo-zJ",
        "outputId": "41f7ed43-2461-40bf-bc9d-f77a732d176e"
      },
      "id": "AP8iULvKo-zJ",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All required libraries imported successfully\n",
            "📋 Environment setup complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra steps, needs more refinement."
      ],
      "metadata": {
        "id": "tsUbzz5EyrVx"
      },
      "id": "tsUbzz5EyrVx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5.2: Main Automation Interface\n",
        "\n",
        "def automated_table_extractor():\n",
        "    \"\"\"\n",
        "    Main automation interface for processing multiple files with Google Colab integration\n",
        "    Handles file upload loop and displays comprehensive summary statistics\n",
        "    \"\"\"\n",
        "    print(\"🚀 Automated Table Extractor Started\")\n",
        "    print(\"📋 This system will process DOC/DOCX files and extract tables containing 'לוח'\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize summary statistics\n",
        "    session_stats = {\n",
        "        'files_processed': 0,\n",
        "        'files_successful': 0,\n",
        "        'files_failed': 0,\n",
        "        'total_tables_found': 0,\n",
        "        'total_tables_filtered': 0,\n",
        "        'total_tables_saved': 0,\n",
        "        'processing_results': []\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        print(f\"\\n📁 Upload Document #{session_stats['files_processed'] + 1}\")\n",
        "        print(\"   (Upload your DOC or DOCX file using the file picker)\")\n",
        "\n",
        "        # Upload file\n",
        "        try:\n",
        "            uploaded = files.upload()\n",
        "            if not uploaded:\n",
        "                print(\"⚠️  No file uploaded\")\n",
        "                continue\n",
        "\n",
        "            # Get uploaded filename\n",
        "            filename = list(uploaded.keys())[0]\n",
        "            filepath = filename\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ File upload failed: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Get chapter and year from user\n",
        "        try:\n",
        "            chapter = int(input(f\"📖 Enter chapter number for '{filename}': \").strip())\n",
        "            year = int(input(f\"📅 Enter year for '{filename}': \").strip())\n",
        "        except ValueError:\n",
        "            print(\"❌ Invalid chapter or year. Please enter numbers only.\")\n",
        "            continue\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n🔴 Process interrupted by user\")\n",
        "            break\n",
        "\n",
        "        # Process the file\n",
        "        try:\n",
        "            file_stats = process_single_file(filepath, chapter, year)\n",
        "\n",
        "            # Update session statistics\n",
        "            session_stats['files_processed'] += 1\n",
        "            session_stats['total_tables_found'] += file_stats['tables_found']\n",
        "            session_stats['total_tables_filtered'] += file_stats['tables_filtered']\n",
        "            session_stats['total_tables_saved'] += file_stats['tables_saved']\n",
        "\n",
        "            # Track success/failure\n",
        "            if file_stats['success']:\n",
        "                session_stats['files_successful'] += 1\n",
        "            else:\n",
        "                session_stats['files_failed'] += 1\n",
        "\n",
        "            # Store individual file results\n",
        "            session_stats['processing_results'].append({\n",
        "                'filename': filename,\n",
        "                'chapter': chapter,\n",
        "                'year': year,\n",
        "                'stats': file_stats\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Unexpected error processing {filename}: {e}\")\n",
        "            session_stats['files_failed'] += 1\n",
        "\n",
        "        # Ask if user wants to continue\n",
        "        print(f\"\\n📊 Current Session: {session_stats['files_successful']} successful, {session_stats['files_failed']} failed\")\n",
        "\n",
        "        try:\n",
        "            continue_choice = input(\"❓ Process another file? (y/n): \").strip().lower()\n",
        "            if continue_choice not in ['y', 'yes']:\n",
        "                break\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n🔴 Process interrupted by user\")\n",
        "            break\n",
        "\n",
        "    # Display final summary\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"📋 FINAL SESSION SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Files Processed: {session_stats['files_processed']}\")\n",
        "    print(f\"✅ Successful: {session_stats['files_successful']}\")\n",
        "    print(f\"❌ Failed: {session_stats['files_failed']}\")\n",
        "    print(f\"📊 Total Tables Found: {session_stats['total_tables_found']}\")\n",
        "    print(f\"🔍 Tables Filtered (contain 'לוח'): {session_stats['total_tables_filtered']}\")\n",
        "    print(f\"💾 Tables Saved: {session_stats['total_tables_saved']}\")\n",
        "\n",
        "    # Detailed breakdown\n",
        "    if session_stats['processing_results']:\n",
        "        print(f\"\\n📄 FILE-BY-FILE BREAKDOWN:\")\n",
        "        for result in session_stats['processing_results']:\n",
        "            status = \"✅\" if result['stats']['success'] else \"❌\"\n",
        "            print(f\"  {status} {result['filename']} (Ch.{result['chapter']}, {result['year']}): \"\n",
        "                  f\"{result['stats']['tables_saved']} tables saved\")\n",
        "            if not result['stats']['success'] and result['stats']['error_message']:\n",
        "                print(f\"      Error: {result['stats']['error_message']}\")\n",
        "\n",
        "    print(f\"\\n🎉 Automation session complete!\")\n",
        "    return session_stats\n",
        "\n",
        "# Phase 6: Error Handling & Reporting\n",
        "# Step 6.1 & 6.2: Comprehensive Error Handling and System Completion\n",
        "\n",
        "def display_system_info():\n",
        "    \"\"\"Display system information and usage instructions\"\"\"\n",
        "    print(\"🔧 AUTOMATED TABLE EXTRACTION SYSTEM\")\n",
        "    print(\"=\" * 50)\n",
        "    print(\"📋 Features:\")\n",
        "    print(\"  • Supports both DOC and DOCX files\")\n",
        "    print(\"  • Filters tables containing 'לוח' in title row\")\n",
        "    print(\"  • Organized output: Tables/year/chapter_j/\")\n",
        "    print(\"  • File naming: Ti_chpj_year.csv\")\n",
        "    print(\"  • Reference dictionaries: references_chpj_year.json\")\n",
        "    print(\"  • Hebrew text support with UTF-8 encoding\")\n",
        "    print(\"\\n🚀 Usage:\")\n",
        "    print(\"  1. Run: automated_table_extractor()\")\n",
        "    print(\"  2. Upload files one by one\")\n",
        "    print(\"  3. Provide chapter and year for each file\")\n",
        "    print(\"  4. Review extraction results and summary\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "def validate_system_dependencies():\n",
        "    \"\"\"\n",
        "    Validate that all required dependencies are available\n",
        "    Returns True if all dependencies are available, False otherwise\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Test critical imports\n",
        "        from docx import Document\n",
        "        import pypandoc\n",
        "        import pandas as pd\n",
        "        from google.colab import files\n",
        "\n",
        "        print(\"✅ All system dependencies verified\")\n",
        "        return True\n",
        "\n",
        "    except ImportError as e:\n",
        "        print(f\"❌ Missing dependency: {e}\")\n",
        "        print(\"🔧 Please run the pip install commands:\")\n",
        "        print(\"   !pip install python-docx pypandoc\")\n",
        "        return False\n",
        "\n",
        "# System initialization and final setup\n",
        "print(\"✅ All required libraries imported successfully\")\n",
        "print(\"📋 Environment setup complete\")\n",
        "print(\"🔧 System ready for automated table extraction\")\n",
        "\n",
        "# Display system information\n",
        "display_system_info()\n",
        "\n",
        "# Validate dependencies\n",
        "if validate_system_dependencies():\n",
        "    print(\"\\n🎯 READY TO START\")\n",
        "    print(\"Run: automated_table_extractor()\")\n",
        "else:\n",
        "    print(\"\\n⚠️  Please install missing dependencies before starting\")"
      ],
      "metadata": {
        "id": "pB2uxS5pyp94"
      },
      "id": "pB2uxS5pyp94",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}